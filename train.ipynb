{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting kit application with the following args:  ['/home/jianheng/.local/share/ov/pkg/isaac-sim-4.0.0/exts/omni.isaac.kit/omni/isaac/kit/simulation_app.py', '/home/jianheng/.local/share/ov/pkg/isaac-sim-4.0.0/apps/omni.isaac.sim.python.kit', '--/app/tokens/exe-path=/home/jianheng/.local/share/ov/pkg/isaac-sim-4.0.0/kit', '--/persistent/app/viewport/displayOptions=3094', '--/rtx/materialDb/syncLoads=True', '--/rtx/hydra/materialSyncLoads=True', '--/omni.kit.plugin/syncUsdLoads=True', '--/app/renderer/resolution/width=1280', '--/app/renderer/resolution/height=720', '--/app/window/width=1440', '--/app/window/height=900', '--/renderer/multiGpu/enabled=True', '--/app/fastShutdown=True', '--ext-folder', '/home/jianheng/.local/share/ov/pkg/isaac-sim-4.0.0/exts', '--ext-folder', '/home/jianheng/.local/share/ov/pkg/isaac-sim-4.0.0/apps', '--/physics/cudaDevice=0', '--portable']\n",
      "Passing the following args to the base kit application:  ['exp=dreamer_v3_isaacsim']\n",
      "[Info] [carb] Logging to file: /home/jianheng/.local/share/ov/pkg/isaac-sim-4.0.0/kit/logs/Kit/Isaac-Sim/4.0/kit_20240823_091755.log\n",
      "\u001b[1m\u001b[93m2024-08-23 08:17:55 [0ms] [Warning] [omni.kit.app.plugin] No crash reporter present, dumps uploading isn't available.\n",
      "\u001b[0m[0.050s] [ext: omni.kit.async_engine-0.0.0] startup\n",
      "[0.327s] [ext: omni.stats-0.0.0] startup\n",
      "[0.328s] [ext: omni.client-1.1.0] startup\n",
      "[0.347s] [ext: omni.datastore-0.0.0] startup\n",
      "[0.347s] [ext: omni.blobkey-1.1.0] startup\n",
      "[0.347s] [ext: omni.ujitso.default-1.0.0] startup\n",
      "[0.348s] [ext: omni.hsscclient-0.0.0] startup\n",
      "[0.349s] [ext: omni.rtx.shadercache.vulkan-1.0.0] startup\n",
      "[0.349s] [ext: omni.assets.plugins-0.0.0] startup\n",
      "[0.350s] [ext: omni.gpu_foundation-0.0.0] startup\n",
      "[0.358s] [ext: carb.windowing.plugins-1.0.0] startup\n",
      "[0.370s] [ext: omni.kit.renderer.init-0.0.0] startup\n",
      "\n",
      "|---------------------------------------------------------------------------------------------|\n",
      "| Driver Version: 535.183.01    | Graphics API: Vulkan\n",
      "|=============================================================================================|\n",
      "| GPU | Name                             | Active | LDA | GPU Memory | Vendor-ID | LUID       |\n",
      "|     |                                  |        |     |            | Device-ID | UUID       |\n",
      "|     |                                  |        |     |            | Bus-ID    |            |\n",
      "|---------------------------------------------------------------------------------------------|\n",
      "| 0   | NVIDIA GeForce RTX 4090          | Yes: 0 |     | 24810   MB | 10de      | 0          |\n",
      "|     |                                  |        |     |            | 2684      | 25c41863.. |\n",
      "|     |                                  |        |     |            | 3e        |            |\n",
      "|=============================================================================================|\n",
      "| OS: 20.04.6 LTS (Focal Fossa) ubuntu, Version: 20.04.6, Kernel: 5.15.0-113-generic\n",
      "| XServer Vendor: The X.Org Foundation, XServer Version: 12013000 (1.20.13.0)\n",
      "| Processor: AMD Ryzen 9 5950X 16-Core Processor | Cores: 16 | Logical: 32\n",
      "|---------------------------------------------------------------------------------------------|\n",
      "| Total Memory (MB): 64204 | Free Memory: 48502\n",
      "| Total Page/Swap (MB): 2047 | Free Page/Swap: 0\n",
      "|---------------------------------------------------------------------------------------------|\n",
      "\u001b[1m\u001b[93m2024-08-23 08:17:56 [955ms] [Warning] [gpu.foundation.plugin] IOMMU is enabled.\n",
      "\u001b[0m[1.224s] [ext: omni.kit.pipapi-0.0.0] startup\n",
      "[1.225s] [ext: omni.kit.pip_archive-0.0.0] startup\n",
      "[1.226s] [ext: omni.pip.compute-1.3.1] startup\n",
      "[1.226s] [ext: omni.pip.cloud-1.1.3] startup\n",
      "[1.228s] [ext: omni.isaac.core_archive-2.3.0] startup\n",
      "[1.228s] [ext: omni.isaac.ml_archive-2.0.1] startup\n",
      "[1.228s] [ext: omni.mtlx-0.1.0] startup\n",
      "[1.228s] [ext: omni.usd.config-1.0.3] startup\n",
      "[1.231s] [ext: omni.gpucompute.plugins-0.0.0] startup\n",
      "[1.231s] [ext: omni.usd.libs-1.0.0] startup\n",
      "[1.318s] [ext: omni.kit.telemetry-0.5.0] startup\n",
      "[1.341s] [ext: omni.kit.loop-isaac-1.2.0] startup\n",
      "[1.342s] [ext: omni.kit.test-0.0.0] startup\n",
      "[1.381s] [ext: omni.appwindow-1.1.8] startup\n",
      "[1.410s] [ext: omni.kit.renderer.core-1.0.1] startup\n",
      "[1.743s] [ext: omni.kit.renderer.capture-0.0.0] startup\n",
      "[1.744s] [ext: omni.kit.renderer.imgui-1.0.1] startup\n",
      "[1.826s] [ext: omni.ui-2.23.11] startup\n",
      "[1.833s] [ext: omni.kit.mainwindow-1.0.3] startup\n",
      "[1.834s] [ext: carb.audio-0.1.0] startup\n",
      "[1.838s] [ext: omni.uiaudio-1.0.0] startup\n",
      "[1.839s] [ext: omni.kit.uiapp-0.0.0] startup\n",
      "[1.839s] [ext: omni.usd.schema.forcefield-106.0.16] startup\n",
      "[1.948s] [ext: omni.usd.schema.scene.visualization-2.0.2] startup\n",
      "[1.949s] [ext: omni.usd.schema.audio-0.0.0] startup\n",
      "[1.951s] [ext: omni.usd.schema.semantics-0.0.0] startup\n",
      "[1.957s] [ext: omni.usd.schema.anim-0.0.0] startup\n",
      "[1.969s] [ext: omni.usd.schema.omnigraph-1.0.0] startup\n",
      "[1.974s] [ext: omni.usd.schema.physx-106.0.16] startup\n",
      "[1.997s] [ext: omni.anim.navigation.schema-106.0.2] startup\n",
      "[1.999s] [ext: omni.usd.schema.omniscripting-1.0.0] startup\n",
      "[2.003s] [ext: omni.anim.graph.schema-106.0.2] startup\n",
      "[2.008s] [ext: omni.usd.schema.geospatial-0.0.0] startup\n",
      "[2.010s] [ext: omni.usd.schema.isaac-2.0.2] startup\n",
      "[2.015s] [ext: omni.kit.window.popup_dialog-2.0.24] startup\n",
      "[2.018s] [ext: omni.graph.exec-0.9.3] startup\n",
      "[2.019s] [ext: omni.kit.widget.nucleus_connector-1.1.5] startup\n",
      "[2.020s] [ext: omni.kit.usd_undo-0.1.8] startup\n",
      "[2.020s] [ext: omni.kit.exec.core-0.13.0] startup\n",
      "[2.021s] [ext: omni.usd_resolver-1.0.0] startup\n",
      "[2.025s] [ext: omni.resourcemonitor-105.0.1] startup\n",
      "[2.026s] [ext: omni.kit.actions.core-1.0.0] startup\n",
      "[2.027s] [ext: omni.activity.core-1.0.1] startup\n",
      "[2.028s] [ext: omni.usd.core-1.2.11] startup\n",
      "[2.030s] [ext: omni.timeline-1.0.10] startup\n",
      "[2.031s] [ext: omni.kit.commands-1.4.9] startup\n",
      "[2.034s] [ext: usdrt.scenegraph-7.4.8] startup\n",
      "[2.064s] [ext: omni.kit.audiodeviceenum-1.0.1] startup\n",
      "[2.065s] [ext: omni.hydra.usdrt_delegate-7.4.7] startup\n",
      "[2.080s] [ext: omni.hydra.scene_delegate-0.3.3] startup\n",
      "[2.086s] [ext: omni.usd-1.10.39] startup\n",
      "[2.110s] [ext: omni.kit.widget.nucleus_info-1.0.2] startup\n",
      "[2.110s] [ext: omni.kit.notification_manager-1.0.8] startup\n",
      "[2.112s] [ext: omni.kit.clipboard-1.0.3] startup\n",
      "[2.112s] [ext: omni.kit.widget.path_field-2.0.9] startup\n",
      "[2.113s] [ext: omni.kit.helper.file_utils-0.1.7] startup\n",
      "[2.114s] [ext: omni.kit.search_core-1.0.5] startup\n",
      "[2.114s] [ext: omni.kit.widget.context_menu-1.2.1] startup\n",
      "[2.115s] [ext: omni.kit.widget.browser_bar-2.0.9] startup\n",
      "[2.115s] [ext: omni.kit.widget.filebrowser-2.3.47] startup\n",
      "[2.118s] [ext: omni.kit.collaboration.telemetry-1.0.0] startup\n",
      "[2.119s] [ext: omni.kit.widget.options_menu-1.1.4] startup\n",
      "[2.121s] [ext: omni.kit.widget.search_delegate-1.0.4] startup\n",
      "[2.121s] [ext: omni.ui.scene-1.9.3] startup\n",
      "[2.125s] [ext: omni.kit.widget.options_button-1.0.2] startup\n",
      "[2.125s] [ext: omni.iray.libs-0.0.0] startup\n",
      "[2.130s] [ext: omni.ujitso.processor.texture-1.0.0] startup\n",
      "[2.130s] [ext: omni.kit.window.filepicker-2.10.32] startup\n",
      "[2.138s] [ext: omni.kit.collaboration.channel_manager-1.0.11] startup\n",
      "[2.138s] [ext: omni.mdl.neuraylib-0.2.4] startup\n",
      "[2.139s] [ext: omni.ujitso.client-0.0.0] startup\n",
      "[2.140s] [ext: omni.mdl-52.0.1] startup\n",
      "[2.151s] [ext: omni.kit.window.file_importer-1.1.11] startup\n",
      "[2.153s] [ext: omni.kit.usd.layers-2.1.30] startup\n",
      "[2.158s] [ext: omni.volume-0.5.0] startup\n",
      "[2.160s] [ext: omni.kit.menu.utils-1.5.23] startup\n",
      "[2.168s] [ext: omni.kit.context_menu-1.8.0] startup\n",
      "[2.169s] [ext: omni.hydra.rtx-0.2.0] startup\n",
      "[2.179s] [ext: omni.kit.material.library-1.4.0] startup\n",
      "[2.182s] [ext: omni.hydra.engine.stats-1.0.1] startup\n",
      "[2.184s] [ext: omni.kit.widget.searchable_combobox-1.0.5] startup\n",
      "[2.185s] [ext: omni.kit.viewport.registry-104.0.6] startup\n",
      "[2.185s] [ext: omni.kit.raycast.query-1.0.5] startup\n",
      "[2.190s] [ext: omni.kit.primitive.mesh-1.0.16] startup\n",
      "[2.193s] [ext: omni.kit.widget.settings-1.1.0] startup\n",
      "[2.194s] [ext: omni.kit.widget.prompt-1.0.7] startup\n",
      "[2.194s] [ext: omni.kit.collaboration.presence_layer-1.0.8] startup\n",
      "[2.196s] [ext: omni.kit.hotkeys.core-1.3.3] startup\n",
      "[2.196s] [ext: omni.kit.window.preferences-1.5.1] startup\n",
      "[2.206s] [ext: omni.kit.widget.live_session_management.ui-1.0.1] startup\n",
      "[2.207s] [ext: omni.kit.window.drop_support-1.0.1] startup\n",
      "[2.207s] [ext: omni.kit.hydra_texture-1.2.6] startup\n",
      "[2.212s] [ext: omni.kit.window.file_exporter-1.0.29] startup\n",
      "[2.213s] [ext: omni.kit.widget.filter-1.1.3] startup\n",
      "[2.214s] [ext: omni.kit.widget.viewport-106.0.2] startup\n",
      "[2.217s] [ext: omni.kit.widget.stage-2.10.26] startup\n",
      "[2.223s] [ext: omni.kit.widget.live_session_management-1.2.18] startup\n",
      "[2.224s] [ext: omni.kit.viewport.window-106.0.8] startup\n",
      "[2.238s] [ext: omni.kit.stage_template.core-1.1.20] startup\n",
      "[2.239s] [ext: omni.graph.tools-1.77.0] startup\n",
      "[2.251s] [ext: omni.kit.window.cursor-1.1.2] startup\n",
      "[2.251s] [ext: omni.kit.viewport.utility-1.0.17] startup\n",
      "[2.252s] [ext: omni.kit.stage_templates-1.2.2] startup\n",
      "[2.253s] [ext: omni.kit.window.content_browser_registry-0.0.4] startup\n",
      "[2.253s] [ext: omni.kit.viewport.menubar.core-107.0.0] startup\n",
      "[2.262s] [ext: omni.kit.viewport.actions-105.2.8] startup\n",
      "[2.266s] [ext: omni.kit.window.file-1.3.52] startup\n",
      "[2.268s] [ext: omni.kit.widget.highlight_label-1.0.1] startup\n",
      "[2.269s] [ext: omni.inspect-1.0.1] startup\n",
      "[2.270s] [ext: omni.kit.window.content_browser-2.9.13] startup\n",
      "[2.280s] [ext: omni.kit.viewport.menubar.display-106.0.0] startup\n",
      "[2.281s] [ext: omni.kit.widget.graph-1.12.6] startup\n",
      "[2.284s] [ext: omni.usdphysics-106.0.16] startup\n",
      "[2.286s] [ext: omni.debugdraw-0.1.3] startup\n",
      "[2.289s] [ext: omni.kit.widget.searchfield-1.1.5] startup\n",
      "[2.290s] [ext: omni.kit.window.extensions-1.4.7] startup\n",
      "[2.293s] [ext: omni.usdphysics.ui-106.0.16] startup\n",
      "[2.311s] [ext: omni.kit.window.property-1.11.1] startup\n",
      "[2.312s] [ext: omni.graph.core-2.170.0] startup\n",
      "[2.314s] [ext: omni.kit.widget.text_editor-1.0.2] startup\n",
      "[2.315s] [ext: omni.kit.property.usd-3.21.26] startup\n",
      "[2.321s] [ext: omni.graph-1.135.0] startup\n",
      "[2.414s] [ext: omni.kvdb-106.0.16] startup\n",
      "[2.415s] [ext: omni.graph.ui-1.67.1] startup\n",
      "[2.431s] [ext: omni.graph.action_core-1.1.4] startup\n",
      "[2.437s] [ext: omni.localcache-106.0.16] startup\n",
      "[2.438s] [ext: omni.graph.action_nodes-1.21.3] startup\n",
      "[2.444s] [ext: omni.physx.foundation-106.0.16] startup\n",
      "[2.446s] [ext: omni.convexdecomposition-106.0.16] startup\n",
      "[2.450s] [ext: omni.graph.action-1.101.1] startup\n",
      "[2.451s] [ext: omni.physx.cooking-106.0.16] startup\n",
      "[2.461s] [ext: omni.kit.widget.toolbar-1.6.2] startup\n",
      "[2.486s] [ext: omni.kit.manipulator.transform-104.7.5] startup\n",
      "[2.493s] [ext: omni.physx-106.0.16] startup\n",
      "[2.505s] [ext: omni.kit.manipulator.tool.snap-1.4.5] startup\n",
      "[2.509s] [ext: omni.physx.stageupdate-106.0.16] startup\n",
      "[2.511s] [ext: omni.physx.commands-106.0.16] startup\n",
      "[2.514s] [ext: omni.ui_query-1.1.2] startup\n",
      "[2.516s] [ext: omni.graph.image.core-0.3.2] startup\n",
      "[2.519s] [ext: omni.physx.ui-106.0.16] startup\n",
      "[2.543s] [ext: omni.kit.ui_test-1.2.18] startup\n",
      "[2.545s] [ext: omni.graph.image.nodes-1.0.2] startup\n",
      "[2.547s] [ext: omni.physx.demos-106.0.16] startup\n",
      "[2.560s] [ext: omni.graph.ui_nodes-1.24.1] startup\n",
      "[2.564s] [ext: omni.graph.nodes-1.141.2] startup\n",
      "[2.574s] [ext: omni.physx.cct-106.0.16] startup\n",
      "[2.586s] [ext: omni.graph.bundle.action-2.0.4] startup\n",
      "[2.586s] [ext: omni.command.usd-1.0.3] startup\n",
      "[2.590s] [ext: omni.kit.numpy.common-0.1.2] startup\n",
      "[2.594s] [ext: omni.kit.property.material-1.9.3] startup\n",
      "[2.606s] [ext: omni.physx.graph-106.0.16] startup\n",
      "[2.633s] [ext: omni.kit.manipulator.selector-1.1.1] startup\n",
      "[2.641s] [ext: omni.isaac.dynamic_control-1.3.7] startup\n",
      "[2.672s] [ext: omni.kit.property.physx-106.0.16] startup\n",
      "[2.851s] [ext: omni.isaac.version-1.1.0] startup\n",
      "[2.852s] [ext: omni.kit.manipulator.viewport-107.0.0] startup\n",
      "[2.854s] [ext: omni.kit.viewport.manipulator.transform-106.0.0] startup\n",
      "ViewportTransformManipulatorExt on_startup\n",
      "[2.855s] [ext: omni.physics.tensors-106.0.16] startup\n",
      "[2.862s] [ext: omni.physx.vehicle-106.0.16] startup\n",
      "[2.876s] [ext: omni.isaac.nucleus-0.1.2] startup\n",
      "[2.878s] [ext: omni.fabric.commands-1.1.4] startup\n",
      "[2.881s] [ext: omni.kit.manipulator.prim.core-106.0.2] startup\n",
      "[2.886s] [ext: omni.physx.tensors-106.0.16] startup\n",
      "[2.889s] [ext: omni.warp.core-1.1.0] startup\n",
      "Warp 1.1.0 initialized:\n",
      "   CUDA Toolkit 11.5, Driver 12.2\n",
      "   Devices:\n",
      "     \"cpu\"      : \"x86_64\"\n",
      "     \"cuda:0\"   : \"NVIDIA GeForce RTX 4090\" (24 GiB, sm_89, mempool enabled)\n",
      "   Kernel cache:\n",
      "     /home/jianheng/.cache/warp/1.1.0\n",
      "[2.965s] [ext: omni.kit.graph.delegate.default-1.2.2] startup\n",
      "[2.966s] [ext: omni.graph.visualization.nodes-2.1.1] startup\n",
      "[2.973s] [ext: omni.kit.manipulator.prim.fabric-106.0.1] startup\n",
      "[2.975s] [ext: omni.kit.manipulator.prim.usd-106.0.1] startup\n",
      "[2.978s] [ext: omni.isaac.core-3.12.0] startup\n",
      "[3.911s] [ext: omni.kit.graph.editor.core-1.5.3] startup\n",
      "[3.914s] [ext: omni.kit.graph.usd.commands-1.3.1] startup\n",
      "[3.915s] [ext: omni.kit.widget.material_preview-1.0.16] startup\n",
      "[3.917s] [ext: omni.kit.manipulator.prim-106.0.0] startup\n",
      "[3.917s] [ext: omni.kit.manipulator.selection-104.0.9] startup\n",
      "[3.918s] [ext: omni.kit.window.toolbar-1.6.1] startup\n",
      "[3.921s] [ext: omni.kit.widget.layers-1.7.9] startup\n",
      "[3.930s] [ext: omni.kit.window.material_graph-1.8.15] startup\n",
      "[3.952s] [ext: omni.graph.scriptnode-1.18.2] startup\n",
      "[3.955s] [ext: omni.kit.graph.delegate.modern-1.10.6] startup\n",
      "[3.958s] [ext: omni.sensors.tiled-0.0.3] startup\n",
      "[3.962s] [ext: omni.physx.supportui-106.0.16] startup\n",
      "[3.973s] [ext: omni.physx.telemetry-106.0.16] startup\n",
      "[3.977s] [ext: omni.physx.camera-106.0.16] startup\n",
      "[3.986s] [ext: omni.isaac.ui-0.15.1] startup\n",
      "[3.990s] [ext: omni.warp-1.1.0] startup\n",
      "[3.996s] [ext: omni.syntheticdata-0.6.7] startup\n",
      "[4.011s] [ext: omni.physx.bundle-106.0.16] startup\n",
      "[4.013s] [ext: omni.isaac.utils-1.0.1] startup\n",
      "[4.023s] [ext: omni.replicator.core-1.11.8] startup\n",
      "\u001b[1m\u001b[93m2024-08-23 08:17:59 [4,070ms] [Warning] [omni.replicator.core.scripts.annotators] Annotator PostProcessDispatch is already registered, overwriting annotator template\n",
      "\u001b[0m[4.165s] [ext: omni.isaac.lula-3.0.1] startup\n",
      "[4.174s] [ext: omni.isaac.surface_gripper-1.0.1] startup\n",
      "[4.179s] [ext: omni.isaac.core_nodes-1.15.4] startup\n",
      "[4.198s] [ext: omni.isaac.cloner-0.8.1] startup\n",
      "[4.204s] [ext: omni.isaac.motion_generation-7.0.1] startup\n",
      "[4.230s] [ext: omni.isaac.manipulators-2.1.0] startup\n",
      "[4.247s] [ext: omni.kit.graph.widget.variables-2.1.0] startup\n",
      "[4.253s] [ext: omni.isaac.gym-0.11.3] startup\n",
      "[4.255s] [ext: omni.isaac.dofbot-0.3.2] startup\n",
      "[4.261s] [ext: omni.graph.window.core-1.107.1] startup\n",
      "[4.295s] [ext: omni.importer.mjcf-1.1.0] startup\n",
      "[4.307s] [ext: omni.isaac.menu-0.4.1] startup\n",
      "[4.312s] [ext: omni.graph.window.generic-1.23.1] startup\n",
      "[4.322s] [ext: omni.graph.window.action-1.25.2] startup\n",
      "[4.336s] [ext: omni.kit.actions.window-1.1.1] startup\n",
      "[4.347s] [ext: omni.kit.viewport.menubar.lighting-106.0.2] startup\n",
      "[4.352s] [ext: omni.isaac.wheeled_robots-2.1.3] startup\n",
      "[4.363s] [ext: omni.kit.ui.actions-1.0.1] startup\n",
      "[4.369s] [ext: omni.kit.hotkeys.window-1.4.5] startup\n",
      "[4.385s] [ext: omni.kit.selection-0.1.4] startup\n",
      "[4.388s] [ext: omni.kit.manipulator.camera-105.0.5] startup\n",
      "[4.397s] [ext: omni.kit.menu.common-1.1.5] startup\n",
      "[4.401s] [ext: omni.isaac.kit-1.10.0] startup\n",
      "[4.402s] [ext: omni.kit.menu.edit-1.1.19] startup\n",
      "[4.410s] [ext: omni.kit.menu.file-1.1.9] startup\n",
      "[4.413s] [ext: omni.isaac.franka-0.4.1] startup\n",
      "[4.415s] [ext: omni.kit.widget.stage_icons-1.0.3] startup\n",
      "[4.419s] [ext: omni.kit.profiler.window-2.2.1] startup\n",
      "\u001b[1m\u001b[93m2024-08-23 08:18:00 [4,444ms] [Warning] [omni.kit.profiler.window] remove _SpanInstance.__lt__ and use insort 'key' arg instead\n",
      "\u001b[0m[4.558s] [ext: omni.kit.property.audio-1.0.11] startup\n",
      "[4.560s] [ext: omni.isaac.cortex-0.3.8] startup\n",
      "[4.562s] [ext: omni.kit.window.stage-2.5.9] startup\n",
      "[4.567s] [ext: omni.kit.property.geometry-1.3.0] startup\n",
      "[4.569s] [ext: omni.kit.property.camera-1.0.6] startup\n",
      "[4.571s] [ext: omni.sensors.nv.common-1.0.1] startup\n",
      "[4.582s] [ext: omni.kit.menu.stage-1.2.5] startup\n",
      "[4.583s] [ext: omni.isaac.debug_draw-0.7.1] startup\n",
      "[4.587s] [ext: omni.sensors.nv.wpm-1.0.0] startup\n",
      "[4.588s] [ext: omni.sensors.nv.materials-1.0.0] startup\n",
      "[4.590s] [ext: omni.isaac.range_sensor-3.0.1] startup\n",
      "[4.599s] [ext: omni.hydra.scene_api-0.1.2] startup\n",
      "[4.605s] [ext: omni.sensors.nv.radar-1.0.1] startup\n",
      "[4.609s] [ext: omni.sensors.nv.lidar-1.0.1] startup\n",
      "[4.613s] [ext: omni.kit.property.render-1.1.1] startup\n",
      "[4.614s] [ext: omni.kit.property.transform-1.5.1] startup\n",
      "[4.617s] [ext: omni.kit.property.light-1.0.8] startup\n",
      "[4.619s] [ext: omni.isaac.sensor-11.3.0] startup\n",
      "[4.671s] [ext: omni.isaac.block_world-1.0.0] startup\n",
      "[4.678s] [ext: omni.kit.property.bundle-1.2.11] startup\n",
      "[4.680s] [ext: omni.kit.property.isaac-0.2.3] startup\n",
      "[4.685s] [ext: omni.isaac.quadruped-1.4.5] startup\n",
      "[4.704s] [ext: omni.isaac.occupancy_map-1.0.1] startup\n",
      "[4.712s] [ext: omni.rtx.window.settings-0.6.15] startup\n",
      "[4.719s] [ext: omni.kit.widget.live-2.1.6] startup\n",
      "[4.721s] [ext: omni.kit.widget.calendar-1.0.8] startup\n",
      "[4.723s] [ext: omni.kit.property.layer-1.1.5] startup\n",
      "[4.725s] [ext: omni.kit.viewport.menubar.render-106.1.2] startup\n",
      "[4.727s] [ext: omni.kit.viewport.menubar.settings-106.0.0] startup\n",
      "[4.731s] [ext: omni.kit.viewport.rtx-104.0.1] startup\n",
      "[4.731s] [ext: omni.kit.widget.cache_indicator-2.0.8] startup\n",
      "[4.732s] [ext: omni.kit.widget.extended_searchfield-1.0.27] startup\n",
      "[4.738s] [ext: omni.kit.widget.timeline-105.0.1] startup\n",
      "[4.742s] [ext: omni.kit.window.commands-0.2.5] startup\n",
      "[4.744s] [ext: omni.kit.window.console-0.2.12] startup\n",
      "[4.752s] [ext: omni.kit.window.script_editor-1.7.6] startup\n",
      "[4.754s] [ext: omni.kit.menu.create-1.0.12] startup\n",
      "[4.756s] [ext: omni.kit.window.status_bar-0.1.6] startup\n",
      "[4.759s] [ext: omni.kit.window.title-1.1.3] startup\n",
      "[4.760s] [ext: omni.replicator.isaac-1.15.0] startup\n",
      "[4.769s] [ext: omni.replicator.replicator_yaml-2.0.5] startup\n",
      "[4.791s] [ext: omni.rtx.settings.core-0.6.0] startup\n",
      "[4.800s] [ext: omni.kit.stage_column.payload-2.0.0] startup\n",
      "[4.806s] [ext: omni.isaac.scene_blox-0.1.1] startup\n",
      "[4.807s] [ext: omni.kit.stage_column.variant-1.0.13] startup\n",
      "[4.811s] [ext: semantics.schema.editor-0.3.4] startup\n",
      "[4.813s] [ext: semantics.schema.property-1.0.2] startup\n",
      "[4.817s] [ext: omni.kit.viewport.menubar.camera-105.1.8] startup\n",
      "[4.822s] [ext: omni.isaac.universal_robots-0.3.5] startup\n",
      "[4.823s] [ext: omni.isaac.cortex.sample_behaviors-1.0.5] startup\n",
      "[4.826s] [ext: omni.importer.urdf-1.14.1] startup\n",
      "[4.864s] [ext: omni.kit.window.stats-0.1.6] startup\n",
      "[4.871s] [ext: omni.isaac.sim.python-4.0.0] startup\n",
      "[4.872s] Simulation App Starting\n",
      "[6.865s] app ready\n",
      "[7.917s] Simulation App Startup Complete\n",
      "\u001b[2mCONFIG\u001b[0m\n",
      "\u001b[2m├── \u001b[0m\u001b[2malgo\u001b[0m\n",
      "\u001b[2m│   \u001b[0m\u001b[2m└── \u001b[0m\u001b[2;91;40mname\u001b[0m\u001b[2;97;40m:\u001b[0m\u001b[2;97;40m \u001b[0m\u001b[2;40mdreamer_v3                                                        \u001b[0m\n",
      "\u001b[2m│   \u001b[0m\u001b[2m    \u001b[0m\u001b[2;91;40mtotal_steps\u001b[0m\u001b[2;97;40m:\u001b[0m\u001b[2;97;40m \u001b[0m\u001b[2;40m1000000                                                    \u001b[0m\n",
      "\u001b[2m│   \u001b[0m\u001b[2m    \u001b[0m\u001b[2;91;40mper_rank_batch_size\u001b[0m\u001b[2;97;40m:\u001b[0m\u001b[2;97;40m \u001b[0m\u001b[2;40m16                                                 \u001b[0m\n",
      "\u001b[2m│   \u001b[0m\u001b[2m    \u001b[0m\u001b[2;91;40mrun_test\u001b[0m\u001b[2;97;40m:\u001b[0m\u001b[2;97;40m \u001b[0m\u001b[2;40mtrue                                                          \u001b[0m\n",
      "\u001b[2m│   \u001b[0m\u001b[2m    \u001b[0m\u001b[2;91;40mcnn_keys\u001b[0m\u001b[2;97;40m:\u001b[0m\u001b[2;40m                                                               \u001b[0m\n",
      "\u001b[2m│   \u001b[0m\u001b[2m    \u001b[0m\u001b[2;97;40m  \u001b[0m\u001b[2;91;40mencoder\u001b[0m\u001b[2;97;40m:\u001b[0m\u001b[2;40m                                                              \u001b[0m\n",
      "\u001b[2m│   \u001b[0m\u001b[2m    \u001b[0m\u001b[2;97;40m  \u001b[0m\u001b[2;40m-\u001b[0m\u001b[2;97;40m \u001b[0m\u001b[2;40mmap                                                                 \u001b[0m\n",
      "\u001b[2m│   \u001b[0m\u001b[2m    \u001b[0m\u001b[2;97;40m  \u001b[0m\u001b[2;40m-\u001b[0m\u001b[2;97;40m \u001b[0m\u001b[2;40mdepth                                                               \u001b[0m\n",
      "\u001b[2m│   \u001b[0m\u001b[2m    \u001b[0m\u001b[2;97;40m  \u001b[0m\u001b[2;91;40mdecoder\u001b[0m\u001b[2;97;40m:\u001b[0m\u001b[2;40m                                                              \u001b[0m\n",
      "\u001b[2m│   \u001b[0m\u001b[2m    \u001b[0m\u001b[2;97;40m  \u001b[0m\u001b[2;40m-\u001b[0m\u001b[2;97;40m \u001b[0m\u001b[2;40mmap                                                                 \u001b[0m\n",
      "\u001b[2m│   \u001b[0m\u001b[2m    \u001b[0m\u001b[2;91;40mmlp_keys\u001b[0m\u001b[2;97;40m:\u001b[0m\u001b[2;40m                                                               \u001b[0m\n",
      "\u001b[2m│   \u001b[0m\u001b[2m    \u001b[0m\u001b[2;97;40m  \u001b[0m\u001b[2;91;40mencoder\u001b[0m\u001b[2;97;40m:\u001b[0m\u001b[2;40m                                                              \u001b[0m\n",
      "\u001b[2m│   \u001b[0m\u001b[2m    \u001b[0m\u001b[2;97;40m  \u001b[0m\u001b[2;40m-\u001b[0m\u001b[2;97;40m \u001b[0m\u001b[2;40mgoal                                                                \u001b[0m\n",
      "\u001b[2m│   \u001b[0m\u001b[2m    \u001b[0m\u001b[2;97;40m  \u001b[0m\u001b[2;40m-\u001b[0m\u001b[2;97;40m \u001b[0m\u001b[2;40mheading                                                             \u001b[0m\n",
      "\u001b[2m│   \u001b[0m\u001b[2m    \u001b[0m\u001b[2;97;40m  \u001b[0m\u001b[2;40m-\u001b[0m\u001b[2;97;40m \u001b[0m\u001b[2;40morientation                                                         \u001b[0m\n",
      "\u001b[2m│   \u001b[0m\u001b[2m    \u001b[0m\u001b[2;97;40m  \u001b[0m\u001b[2;40m-\u001b[0m\u001b[2;97;40m \u001b[0m\u001b[2;40mdistance                                                            \u001b[0m\n",
      "\u001b[2m│   \u001b[0m\u001b[2m    \u001b[0m\u001b[2;97;40m  \u001b[0m\u001b[2;91;40mdecoder\u001b[0m\u001b[2;97;40m:\u001b[0m\u001b[2;40m                                                              \u001b[0m\n",
      "\u001b[2m│   \u001b[0m\u001b[2m    \u001b[0m\u001b[2;97;40m  \u001b[0m\u001b[2;40m-\u001b[0m\u001b[2;97;40m \u001b[0m\u001b[2;40mgoal                                                                \u001b[0m\n",
      "\u001b[2m│   \u001b[0m\u001b[2m    \u001b[0m\u001b[2;97;40m  \u001b[0m\u001b[2;40m-\u001b[0m\u001b[2;97;40m \u001b[0m\u001b[2;40mheading                                                             \u001b[0m\n",
      "\u001b[2m│   \u001b[0m\u001b[2m    \u001b[0m\u001b[2;97;40m  \u001b[0m\u001b[2;40m-\u001b[0m\u001b[2;97;40m \u001b[0m\u001b[2;40morientation                                                         \u001b[0m\n",
      "\u001b[2m│   \u001b[0m\u001b[2m    \u001b[0m\u001b[2;97;40m  \u001b[0m\u001b[2;40m-\u001b[0m\u001b[2;97;40m \u001b[0m\u001b[2;40mdistance                                                            \u001b[0m\n",
      "\u001b[2m│   \u001b[0m\u001b[2m    \u001b[0m\u001b[2;91;40mworld_model\u001b[0m\u001b[2;97;40m:\u001b[0m\u001b[2;40m                                                            \u001b[0m\n",
      "\u001b[2m│   \u001b[0m\u001b[2m    \u001b[0m\u001b[2;97;40m  \u001b[0m\u001b[2;91;40moptimizer\u001b[0m\u001b[2;97;40m:\u001b[0m\u001b[2;40m                                                            \u001b[0m\n",
      "\u001b[2m│   \u001b[0m\u001b[2m    \u001b[0m\u001b[2;97;40m    \u001b[0m\u001b[2;91;40m_target_\u001b[0m\u001b[2;97;40m:\u001b[0m\u001b[2;97;40m \u001b[0m\u001b[2;40mtorch.optim.Adam                                          \u001b[0m\n",
      "\u001b[2m│   \u001b[0m\u001b[2m    \u001b[0m\u001b[2;97;40m    \u001b[0m\u001b[2;91;40mlr\u001b[0m\u001b[2;97;40m:\u001b[0m\u001b[2;97;40m \u001b[0m\u001b[2;40m0.0001                                                          \u001b[0m\n",
      "\u001b[2m│   \u001b[0m\u001b[2m    \u001b[0m\u001b[2;97;40m    \u001b[0m\u001b[2;91;40meps\u001b[0m\u001b[2;97;40m:\u001b[0m\u001b[2;97;40m \u001b[0m\u001b[2;40m1.0e-08                                                        \u001b[0m\n",
      "\u001b[2m│   \u001b[0m\u001b[2m    \u001b[0m\u001b[2;97;40m    \u001b[0m\u001b[2;91;40mweight_decay\u001b[0m\u001b[2;97;40m:\u001b[0m\u001b[2;97;40m \u001b[0m\u001b[2;40m0                                                     \u001b[0m\n",
      "\u001b[2m│   \u001b[0m\u001b[2m    \u001b[0m\u001b[2;97;40m    \u001b[0m\u001b[2;91;40mbetas\u001b[0m\u001b[2;97;40m:\u001b[0m\u001b[2;40m                                                              \u001b[0m\n",
      "\u001b[2m│   \u001b[0m\u001b[2m    \u001b[0m\u001b[2;97;40m    \u001b[0m\u001b[2;40m-\u001b[0m\u001b[2;97;40m \u001b[0m\u001b[2;40m0.9                                                               \u001b[0m\n",
      "\u001b[2m│   \u001b[0m\u001b[2m    \u001b[0m\u001b[2;97;40m    \u001b[0m\u001b[2;40m-\u001b[0m\u001b[2;97;40m \u001b[0m\u001b[2;40m0.999                                                             \u001b[0m\n",
      "\u001b[2m│   \u001b[0m\u001b[2m    \u001b[0m\u001b[2;97;40m  \u001b[0m\u001b[2;91;40mdiscrete_size\u001b[0m\u001b[2;97;40m:\u001b[0m\u001b[2;97;40m \u001b[0m\u001b[2;40m32                                                     \u001b[0m\n",
      "\u001b[2m│   \u001b[0m\u001b[2m    \u001b[0m\u001b[2;97;40m  \u001b[0m\u001b[2;91;40mstochastic_size\u001b[0m\u001b[2;97;40m:\u001b[0m\u001b[2;97;40m \u001b[0m\u001b[2;40m32                                                   \u001b[0m\n",
      "\u001b[2m│   \u001b[0m\u001b[2m    \u001b[0m\u001b[2;97;40m  \u001b[0m\u001b[2;91;40mkl_dynamic\u001b[0m\u001b[2;97;40m:\u001b[0m\u001b[2;97;40m \u001b[0m\u001b[2;40m0.5                                                       \u001b[0m\n",
      "\u001b[2m│   \u001b[0m\u001b[2m    \u001b[0m\u001b[2;97;40m  \u001b[0m\u001b[2;91;40mkl_representation\u001b[0m\u001b[2;97;40m:\u001b[0m\u001b[2;97;40m \u001b[0m\u001b[2;40m0.1                                                \u001b[0m\n",
      "\u001b[2m│   \u001b[0m\u001b[2m    \u001b[0m\u001b[2;97;40m  \u001b[0m\u001b[2;91;40mkl_free_nats\u001b[0m\u001b[2;97;40m:\u001b[0m\u001b[2;97;40m \u001b[0m\u001b[2;40m1.0                                                     \u001b[0m\n",
      "\u001b[2m│   \u001b[0m\u001b[2m    \u001b[0m\u001b[2;97;40m  \u001b[0m\u001b[2;91;40mkl_regularizer\u001b[0m\u001b[2;97;40m:\u001b[0m\u001b[2;97;40m \u001b[0m\u001b[2;40m1.0                                                   \u001b[0m\n",
      "\u001b[2m│   \u001b[0m\u001b[2m    \u001b[0m\u001b[2;97;40m  \u001b[0m\u001b[2;91;40mcontinue_scale_factor\u001b[0m\u001b[2;97;40m:\u001b[0m\u001b[2;97;40m \u001b[0m\u001b[2;40m1.0                                            \u001b[0m\n",
      "\u001b[2m│   \u001b[0m\u001b[2m    \u001b[0m\u001b[2;97;40m  \u001b[0m\u001b[2;91;40mclip_gradients\u001b[0m\u001b[2;97;40m:\u001b[0m\u001b[2;97;40m \u001b[0m\u001b[2;40m1000.0                                                \u001b[0m\n",
      "\u001b[2m│   \u001b[0m\u001b[2m    \u001b[0m\u001b[2;97;40m  \u001b[0m\u001b[2;91;40mdecoupled_rssm\u001b[0m\u001b[2;97;40m:\u001b[0m\u001b[2;97;40m \u001b[0m\u001b[2;40mfalse                                                 \u001b[0m\n",
      "\u001b[2m│   \u001b[0m\u001b[2m    \u001b[0m\u001b[2;97;40m  \u001b[0m\u001b[2;91;40mlearnable_initial_recurrent_state\u001b[0m\u001b[2;97;40m:\u001b[0m\u001b[2;97;40m \u001b[0m\u001b[2;40mtrue                               \u001b[0m\n",
      "\u001b[2m│   \u001b[0m\u001b[2m    \u001b[0m\u001b[2;97;40m  \u001b[0m\u001b[2;91;40mencoder\u001b[0m\u001b[2;97;40m:\u001b[0m\u001b[2;40m                                                              \u001b[0m\n",
      "\u001b[2m│   \u001b[0m\u001b[2m    \u001b[0m\u001b[2;97;40m    \u001b[0m\u001b[2;91;40mcnn_channels_multiplier\u001b[0m\u001b[2;97;40m:\u001b[0m\u001b[2;97;40m \u001b[0m\u001b[2;40m64                                         \u001b[0m\n",
      "\u001b[2m│   \u001b[0m\u001b[2m    \u001b[0m\u001b[2;97;40m    \u001b[0m\u001b[2;91;40mcnn_act\u001b[0m\u001b[2;97;40m:\u001b[0m\u001b[2;97;40m \u001b[0m\u001b[2;40mtorch.nn.SiLU                                              \u001b[0m\n",
      "\u001b[2m│   \u001b[0m\u001b[2m    \u001b[0m\u001b[2;97;40m    \u001b[0m\u001b[2;91;40mdense_act\u001b[0m\u001b[2;97;40m:\u001b[0m\u001b[2;97;40m \u001b[0m\u001b[2;40mtorch.nn.SiLU                                            \u001b[0m\n",
      "\u001b[2m│   \u001b[0m\u001b[2m    \u001b[0m\u001b[2;97;40m    \u001b[0m\u001b[2;91;40mmlp_layers\u001b[0m\u001b[2;97;40m:\u001b[0m\u001b[2;97;40m \u001b[0m\u001b[2;40m4                                                       \u001b[0m\n",
      "\u001b[2m│   \u001b[0m\u001b[2m    \u001b[0m\u001b[2;97;40m    \u001b[0m\u001b[2;91;40mcnn_layer_norm\u001b[0m\u001b[2;97;40m:\u001b[0m\u001b[2;40m                                                     \u001b[0m\n",
      "\u001b[2m│   \u001b[0m\u001b[2m    \u001b[0m\u001b[2;97;40m      \u001b[0m\u001b[2;91;40mcls\u001b[0m\u001b[2;97;40m:\u001b[0m\u001b[2;97;40m \u001b[0m\u001b[2;40msheeprl.models.models.LayerNormChannelLast                   \u001b[0m\n",
      "\u001b[2m│   \u001b[0m\u001b[2m    \u001b[0m\u001b[2;97;40m      \u001b[0m\u001b[2;91;40mkw\u001b[0m\u001b[2;97;40m:\u001b[0m\u001b[2;40m                                                               \u001b[0m\n",
      "\u001b[2m│   \u001b[0m\u001b[2m    \u001b[0m\u001b[2;97;40m        \u001b[0m\u001b[2;91;40meps\u001b[0m\u001b[2;97;40m:\u001b[0m\u001b[2;97;40m \u001b[0m\u001b[2;40m0.001                                                      \u001b[0m\n",
      "\u001b[2m│   \u001b[0m\u001b[2m    \u001b[0m\u001b[2;97;40m    \u001b[0m\u001b[2;91;40mmlp_layer_norm\u001b[0m\u001b[2;97;40m:\u001b[0m\u001b[2;40m                                                     \u001b[0m\n",
      "\u001b[2m│   \u001b[0m\u001b[2m    \u001b[0m\u001b[2;97;40m      \u001b[0m\u001b[2;91;40mcls\u001b[0m\u001b[2;97;40m:\u001b[0m\u001b[2;97;40m \u001b[0m\u001b[2;40msheeprl.models.models.LayerNorm                              \u001b[0m\n",
      "\u001b[2m│   \u001b[0m\u001b[2m    \u001b[0m\u001b[2;97;40m      \u001b[0m\u001b[2;91;40mkw\u001b[0m\u001b[2;97;40m:\u001b[0m\u001b[2;40m                                                               \u001b[0m\n",
      "\u001b[2m│   \u001b[0m\u001b[2m    \u001b[0m\u001b[2;97;40m        \u001b[0m\u001b[2;91;40meps\u001b[0m\u001b[2;97;40m:\u001b[0m\u001b[2;97;40m \u001b[0m\u001b[2;40m0.001                                                      \u001b[0m\n",
      "\u001b[2m│   \u001b[0m\u001b[2m    \u001b[0m\u001b[2;97;40m    \u001b[0m\u001b[2;91;40mdense_units\u001b[0m\u001b[2;97;40m:\u001b[0m\u001b[2;97;40m \u001b[0m\u001b[2;40m768                                                    \u001b[0m\n",
      "\u001b[2m│   \u001b[0m\u001b[2m    \u001b[0m\u001b[2;97;40m  \u001b[0m\u001b[2;91;40mrecurrent_model\u001b[0m\u001b[2;97;40m:\u001b[0m\u001b[2;40m                                                      \u001b[0m\n",
      "\u001b[2m│   \u001b[0m\u001b[2m    \u001b[0m\u001b[2;97;40m    \u001b[0m\u001b[2;91;40mrecurrent_state_size\u001b[0m\u001b[2;97;40m:\u001b[0m\u001b[2;97;40m \u001b[0m\u001b[2;40m2048                                          \u001b[0m\n",
      "\u001b[2m│   \u001b[0m\u001b[2m    \u001b[0m\u001b[2;97;40m    \u001b[0m\u001b[2;91;40mlayer_norm\u001b[0m\u001b[2;97;40m:\u001b[0m\u001b[2;40m                                                         \u001b[0m\n",
      "\u001b[2m│   \u001b[0m\u001b[2m    \u001b[0m\u001b[2;97;40m      \u001b[0m\u001b[2;91;40mcls\u001b[0m\u001b[2;97;40m:\u001b[0m\u001b[2;97;40m \u001b[0m\u001b[2;40msheeprl.models.models.LayerNorm                              \u001b[0m\n",
      "\u001b[2m│   \u001b[0m\u001b[2m    \u001b[0m\u001b[2;97;40m      \u001b[0m\u001b[2;91;40mkw\u001b[0m\u001b[2;97;40m:\u001b[0m\u001b[2;40m                                                               \u001b[0m\n",
      "\u001b[2m│   \u001b[0m\u001b[2m    \u001b[0m\u001b[2;97;40m        \u001b[0m\u001b[2;91;40meps\u001b[0m\u001b[2;97;40m:\u001b[0m\u001b[2;97;40m \u001b[0m\u001b[2;40m0.001                                                      \u001b[0m\n",
      "\u001b[2m│   \u001b[0m\u001b[2m    \u001b[0m\u001b[2;97;40m    \u001b[0m\u001b[2;91;40mdense_units\u001b[0m\u001b[2;97;40m:\u001b[0m\u001b[2;97;40m \u001b[0m\u001b[2;40m768                                                    \u001b[0m\n",
      "\u001b[2m│   \u001b[0m\u001b[2m    \u001b[0m\u001b[2;97;40m  \u001b[0m\u001b[2;91;40mtransition_model\u001b[0m\u001b[2;97;40m:\u001b[0m\u001b[2;40m                                                     \u001b[0m\n",
      "\u001b[2m│   \u001b[0m\u001b[2m    \u001b[0m\u001b[2;97;40m    \u001b[0m\u001b[2;91;40mhidden_size\u001b[0m\u001b[2;97;40m:\u001b[0m\u001b[2;97;40m \u001b[0m\u001b[2;40m768                                                    \u001b[0m\n",
      "\u001b[2m│   \u001b[0m\u001b[2m    \u001b[0m\u001b[2;97;40m    \u001b[0m\u001b[2;91;40mdense_act\u001b[0m\u001b[2;97;40m:\u001b[0m\u001b[2;97;40m \u001b[0m\u001b[2;40mtorch.nn.SiLU                                            \u001b[0m\n",
      "\u001b[2m│   \u001b[0m\u001b[2m    \u001b[0m\u001b[2;97;40m    \u001b[0m\u001b[2;91;40mlayer_norm\u001b[0m\u001b[2;97;40m:\u001b[0m\u001b[2;40m                                                         \u001b[0m\n",
      "\u001b[2m│   \u001b[0m\u001b[2m    \u001b[0m\u001b[2;97;40m      \u001b[0m\u001b[2;91;40mcls\u001b[0m\u001b[2;97;40m:\u001b[0m\u001b[2;97;40m \u001b[0m\u001b[2;40msheeprl.models.models.LayerNorm                              \u001b[0m\n",
      "\u001b[2m│   \u001b[0m\u001b[2m    \u001b[0m\u001b[2;97;40m      \u001b[0m\u001b[2;91;40mkw\u001b[0m\u001b[2;97;40m:\u001b[0m\u001b[2;40m                                                               \u001b[0m\n",
      "\u001b[2m│   \u001b[0m\u001b[2m    \u001b[0m\u001b[2;97;40m        \u001b[0m\u001b[2;91;40meps\u001b[0m\u001b[2;97;40m:\u001b[0m\u001b[2;97;40m \u001b[0m\u001b[2;40m0.001                                                      \u001b[0m\n",
      "\u001b[2m│   \u001b[0m\u001b[2m    \u001b[0m\u001b[2;97;40m  \u001b[0m\u001b[2;91;40mrepresentation_model\u001b[0m\u001b[2;97;40m:\u001b[0m\u001b[2;40m                                                 \u001b[0m\n",
      "\u001b[2m│   \u001b[0m\u001b[2m    \u001b[0m\u001b[2;97;40m    \u001b[0m\u001b[2;91;40mhidden_size\u001b[0m\u001b[2;97;40m:\u001b[0m\u001b[2;97;40m \u001b[0m\u001b[2;40m768                                                    \u001b[0m\n",
      "\u001b[2m│   \u001b[0m\u001b[2m    \u001b[0m\u001b[2;97;40m    \u001b[0m\u001b[2;91;40mdense_act\u001b[0m\u001b[2;97;40m:\u001b[0m\u001b[2;97;40m \u001b[0m\u001b[2;40mtorch.nn.SiLU                                            \u001b[0m\n",
      "\u001b[2m│   \u001b[0m\u001b[2m    \u001b[0m\u001b[2;97;40m    \u001b[0m\u001b[2;91;40mlayer_norm\u001b[0m\u001b[2;97;40m:\u001b[0m\u001b[2;40m                                                         \u001b[0m\n",
      "\u001b[2m│   \u001b[0m\u001b[2m    \u001b[0m\u001b[2;97;40m      \u001b[0m\u001b[2;91;40mcls\u001b[0m\u001b[2;97;40m:\u001b[0m\u001b[2;97;40m \u001b[0m\u001b[2;40msheeprl.models.models.LayerNorm                              \u001b[0m\n",
      "\u001b[2m│   \u001b[0m\u001b[2m    \u001b[0m\u001b[2;97;40m      \u001b[0m\u001b[2;91;40mkw\u001b[0m\u001b[2;97;40m:\u001b[0m\u001b[2;40m                                                               \u001b[0m\n",
      "\u001b[2m│   \u001b[0m\u001b[2m    \u001b[0m\u001b[2;97;40m        \u001b[0m\u001b[2;91;40meps\u001b[0m\u001b[2;97;40m:\u001b[0m\u001b[2;97;40m \u001b[0m\u001b[2;40m0.001                                                      \u001b[0m\n",
      "\u001b[2m│   \u001b[0m\u001b[2m    \u001b[0m\u001b[2;97;40m  \u001b[0m\u001b[2;91;40mobservation_model\u001b[0m\u001b[2;97;40m:\u001b[0m\u001b[2;40m                                                    \u001b[0m\n",
      "\u001b[2m│   \u001b[0m\u001b[2m    \u001b[0m\u001b[2;97;40m    \u001b[0m\u001b[2;91;40mcnn_channels_multiplier\u001b[0m\u001b[2;97;40m:\u001b[0m\u001b[2;97;40m \u001b[0m\u001b[2;40m64                                         \u001b[0m\n",
      "\u001b[2m│   \u001b[0m\u001b[2m    \u001b[0m\u001b[2;97;40m    \u001b[0m\u001b[2;91;40mcnn_act\u001b[0m\u001b[2;97;40m:\u001b[0m\u001b[2;97;40m \u001b[0m\u001b[2;40mtorch.nn.SiLU                                              \u001b[0m\n",
      "\u001b[2m│   \u001b[0m\u001b[2m    \u001b[0m\u001b[2;97;40m    \u001b[0m\u001b[2;91;40mdense_act\u001b[0m\u001b[2;97;40m:\u001b[0m\u001b[2;97;40m \u001b[0m\u001b[2;40mtorch.nn.SiLU                                            \u001b[0m\n",
      "\u001b[2m│   \u001b[0m\u001b[2m    \u001b[0m\u001b[2;97;40m    \u001b[0m\u001b[2;91;40mmlp_layers\u001b[0m\u001b[2;97;40m:\u001b[0m\u001b[2;97;40m \u001b[0m\u001b[2;40m4                                                       \u001b[0m\n",
      "\u001b[2m│   \u001b[0m\u001b[2m    \u001b[0m\u001b[2;97;40m    \u001b[0m\u001b[2;91;40mcnn_layer_norm\u001b[0m\u001b[2;97;40m:\u001b[0m\u001b[2;40m                                                     \u001b[0m\n",
      "\u001b[2m│   \u001b[0m\u001b[2m    \u001b[0m\u001b[2;97;40m      \u001b[0m\u001b[2;91;40mcls\u001b[0m\u001b[2;97;40m:\u001b[0m\u001b[2;97;40m \u001b[0m\u001b[2;40msheeprl.models.models.LayerNormChannelLast                   \u001b[0m\n",
      "\u001b[2m│   \u001b[0m\u001b[2m    \u001b[0m\u001b[2;97;40m      \u001b[0m\u001b[2;91;40mkw\u001b[0m\u001b[2;97;40m:\u001b[0m\u001b[2;40m                                                               \u001b[0m\n",
      "\u001b[2m│   \u001b[0m\u001b[2m    \u001b[0m\u001b[2;97;40m        \u001b[0m\u001b[2;91;40meps\u001b[0m\u001b[2;97;40m:\u001b[0m\u001b[2;97;40m \u001b[0m\u001b[2;40m0.001                                                      \u001b[0m\n",
      "\u001b[2m│   \u001b[0m\u001b[2m    \u001b[0m\u001b[2;97;40m    \u001b[0m\u001b[2;91;40mmlp_layer_norm\u001b[0m\u001b[2;97;40m:\u001b[0m\u001b[2;40m                                                     \u001b[0m\n",
      "\u001b[2m│   \u001b[0m\u001b[2m    \u001b[0m\u001b[2;97;40m      \u001b[0m\u001b[2;91;40mcls\u001b[0m\u001b[2;97;40m:\u001b[0m\u001b[2;97;40m \u001b[0m\u001b[2;40msheeprl.models.models.LayerNorm                              \u001b[0m\n",
      "\u001b[2m│   \u001b[0m\u001b[2m    \u001b[0m\u001b[2;97;40m      \u001b[0m\u001b[2;91;40mkw\u001b[0m\u001b[2;97;40m:\u001b[0m\u001b[2;40m                                                               \u001b[0m\n",
      "\u001b[2m│   \u001b[0m\u001b[2m    \u001b[0m\u001b[2;97;40m        \u001b[0m\u001b[2;91;40meps\u001b[0m\u001b[2;97;40m:\u001b[0m\u001b[2;97;40m \u001b[0m\u001b[2;40m0.001                                                      \u001b[0m\n",
      "\u001b[2m│   \u001b[0m\u001b[2m    \u001b[0m\u001b[2;97;40m    \u001b[0m\u001b[2;91;40mdense_units\u001b[0m\u001b[2;97;40m:\u001b[0m\u001b[2;97;40m \u001b[0m\u001b[2;40m768                                                    \u001b[0m\n",
      "\u001b[2m│   \u001b[0m\u001b[2m    \u001b[0m\u001b[2;97;40m  \u001b[0m\u001b[2;91;40mreward_model\u001b[0m\u001b[2;97;40m:\u001b[0m\u001b[2;40m                                                         \u001b[0m\n",
      "\u001b[2m│   \u001b[0m\u001b[2m    \u001b[0m\u001b[2;97;40m    \u001b[0m\u001b[2;91;40mdense_act\u001b[0m\u001b[2;97;40m:\u001b[0m\u001b[2;97;40m \u001b[0m\u001b[2;40mtorch.nn.SiLU                                            \u001b[0m\n",
      "\u001b[2m│   \u001b[0m\u001b[2m    \u001b[0m\u001b[2;97;40m    \u001b[0m\u001b[2;91;40mmlp_layers\u001b[0m\u001b[2;97;40m:\u001b[0m\u001b[2;97;40m \u001b[0m\u001b[2;40m4                                                       \u001b[0m\n",
      "\u001b[2m│   \u001b[0m\u001b[2m    \u001b[0m\u001b[2;97;40m    \u001b[0m\u001b[2;91;40mlayer_norm\u001b[0m\u001b[2;97;40m:\u001b[0m\u001b[2;40m                                                         \u001b[0m\n",
      "\u001b[2m│   \u001b[0m\u001b[2m    \u001b[0m\u001b[2;97;40m      \u001b[0m\u001b[2;91;40mcls\u001b[0m\u001b[2;97;40m:\u001b[0m\u001b[2;97;40m \u001b[0m\u001b[2;40msheeprl.models.models.LayerNorm                              \u001b[0m\n",
      "\u001b[2m│   \u001b[0m\u001b[2m    \u001b[0m\u001b[2;97;40m      \u001b[0m\u001b[2;91;40mkw\u001b[0m\u001b[2;97;40m:\u001b[0m\u001b[2;40m                                                               \u001b[0m\n",
      "\u001b[2m│   \u001b[0m\u001b[2m    \u001b[0m\u001b[2;97;40m        \u001b[0m\u001b[2;91;40meps\u001b[0m\u001b[2;97;40m:\u001b[0m\u001b[2;97;40m \u001b[0m\u001b[2;40m0.001                                                      \u001b[0m\n",
      "\u001b[2m│   \u001b[0m\u001b[2m    \u001b[0m\u001b[2;97;40m    \u001b[0m\u001b[2;91;40mdense_units\u001b[0m\u001b[2;97;40m:\u001b[0m\u001b[2;97;40m \u001b[0m\u001b[2;40m768                                                    \u001b[0m\n",
      "\u001b[2m│   \u001b[0m\u001b[2m    \u001b[0m\u001b[2;97;40m    \u001b[0m\u001b[2;91;40mbins\u001b[0m\u001b[2;97;40m:\u001b[0m\u001b[2;97;40m \u001b[0m\u001b[2;40m255                                                           \u001b[0m\n",
      "\u001b[2m│   \u001b[0m\u001b[2m    \u001b[0m\u001b[2;97;40m  \u001b[0m\u001b[2;91;40mdiscount_model\u001b[0m\u001b[2;97;40m:\u001b[0m\u001b[2;40m                                                       \u001b[0m\n",
      "\u001b[2m│   \u001b[0m\u001b[2m    \u001b[0m\u001b[2;97;40m    \u001b[0m\u001b[2;91;40mlearnable\u001b[0m\u001b[2;97;40m:\u001b[0m\u001b[2;97;40m \u001b[0m\u001b[2;40mtrue                                                     \u001b[0m\n",
      "\u001b[2m│   \u001b[0m\u001b[2m    \u001b[0m\u001b[2;97;40m    \u001b[0m\u001b[2;91;40mdense_act\u001b[0m\u001b[2;97;40m:\u001b[0m\u001b[2;97;40m \u001b[0m\u001b[2;40mtorch.nn.SiLU                                            \u001b[0m\n",
      "\u001b[2m│   \u001b[0m\u001b[2m    \u001b[0m\u001b[2;97;40m    \u001b[0m\u001b[2;91;40mmlp_layers\u001b[0m\u001b[2;97;40m:\u001b[0m\u001b[2;97;40m \u001b[0m\u001b[2;40m4                                                       \u001b[0m\n",
      "\u001b[2m│   \u001b[0m\u001b[2m    \u001b[0m\u001b[2;97;40m    \u001b[0m\u001b[2;91;40mlayer_norm\u001b[0m\u001b[2;97;40m:\u001b[0m\u001b[2;40m                                                         \u001b[0m\n",
      "\u001b[2m│   \u001b[0m\u001b[2m    \u001b[0m\u001b[2;97;40m      \u001b[0m\u001b[2;91;40mcls\u001b[0m\u001b[2;97;40m:\u001b[0m\u001b[2;97;40m \u001b[0m\u001b[2;40msheeprl.models.models.LayerNorm                              \u001b[0m\n",
      "\u001b[2m│   \u001b[0m\u001b[2m    \u001b[0m\u001b[2;97;40m      \u001b[0m\u001b[2;91;40mkw\u001b[0m\u001b[2;97;40m:\u001b[0m\u001b[2;40m                                                               \u001b[0m\n",
      "\u001b[2m│   \u001b[0m\u001b[2m    \u001b[0m\u001b[2;97;40m        \u001b[0m\u001b[2;91;40meps\u001b[0m\u001b[2;97;40m:\u001b[0m\u001b[2;97;40m \u001b[0m\u001b[2;40m0.001                                                      \u001b[0m\n",
      "\u001b[2m│   \u001b[0m\u001b[2m    \u001b[0m\u001b[2;97;40m    \u001b[0m\u001b[2;91;40mdense_units\u001b[0m\u001b[2;97;40m:\u001b[0m\u001b[2;97;40m \u001b[0m\u001b[2;40m768                                                    \u001b[0m\n",
      "\u001b[2m│   \u001b[0m\u001b[2m    \u001b[0m\u001b[2;91;40mactor\u001b[0m\u001b[2;97;40m:\u001b[0m\u001b[2;40m                                                                  \u001b[0m\n",
      "\u001b[2m│   \u001b[0m\u001b[2m    \u001b[0m\u001b[2;97;40m  \u001b[0m\u001b[2;91;40moptimizer\u001b[0m\u001b[2;97;40m:\u001b[0m\u001b[2;40m                                                            \u001b[0m\n",
      "\u001b[2m│   \u001b[0m\u001b[2m    \u001b[0m\u001b[2;97;40m    \u001b[0m\u001b[2;91;40m_target_\u001b[0m\u001b[2;97;40m:\u001b[0m\u001b[2;97;40m \u001b[0m\u001b[2;40mtorch.optim.Adam                                          \u001b[0m\n",
      "\u001b[2m│   \u001b[0m\u001b[2m    \u001b[0m\u001b[2;97;40m    \u001b[0m\u001b[2;91;40mlr\u001b[0m\u001b[2;97;40m:\u001b[0m\u001b[2;97;40m \u001b[0m\u001b[2;40m8.0e-05                                                         \u001b[0m\n",
      "\u001b[2m│   \u001b[0m\u001b[2m    \u001b[0m\u001b[2;97;40m    \u001b[0m\u001b[2;91;40meps\u001b[0m\u001b[2;97;40m:\u001b[0m\u001b[2;97;40m \u001b[0m\u001b[2;40m1.0e-05                                                        \u001b[0m\n",
      "\u001b[2m│   \u001b[0m\u001b[2m    \u001b[0m\u001b[2;97;40m    \u001b[0m\u001b[2;91;40mweight_decay\u001b[0m\u001b[2;97;40m:\u001b[0m\u001b[2;97;40m \u001b[0m\u001b[2;40m0                                                     \u001b[0m\n",
      "\u001b[2m│   \u001b[0m\u001b[2m    \u001b[0m\u001b[2;97;40m    \u001b[0m\u001b[2;91;40mbetas\u001b[0m\u001b[2;97;40m:\u001b[0m\u001b[2;40m                                                              \u001b[0m\n",
      "\u001b[2m│   \u001b[0m\u001b[2m    \u001b[0m\u001b[2;97;40m    \u001b[0m\u001b[2;40m-\u001b[0m\u001b[2;97;40m \u001b[0m\u001b[2;40m0.9                                                               \u001b[0m\n",
      "\u001b[2m│   \u001b[0m\u001b[2m    \u001b[0m\u001b[2;97;40m    \u001b[0m\u001b[2;40m-\u001b[0m\u001b[2;97;40m \u001b[0m\u001b[2;40m0.999                                                             \u001b[0m\n",
      "\u001b[2m│   \u001b[0m\u001b[2m    \u001b[0m\u001b[2;97;40m  \u001b[0m\u001b[2;91;40mcls\u001b[0m\u001b[2;97;40m:\u001b[0m\u001b[2;97;40m \u001b[0m\u001b[2;40msheeprl.algos.dreamer_v3.agent.Actor                             \u001b[0m\n",
      "\u001b[2m│   \u001b[0m\u001b[2m    \u001b[0m\u001b[2;97;40m  \u001b[0m\u001b[2;91;40ment_coef\u001b[0m\u001b[2;97;40m:\u001b[0m\u001b[2;97;40m \u001b[0m\u001b[2;40m0.0003                                                      \u001b[0m\n",
      "\u001b[2m│   \u001b[0m\u001b[2m    \u001b[0m\u001b[2;97;40m  \u001b[0m\u001b[2;91;40mmin_std\u001b[0m\u001b[2;97;40m:\u001b[0m\u001b[2;97;40m \u001b[0m\u001b[2;40m0.1                                                          \u001b[0m\n",
      "\u001b[2m│   \u001b[0m\u001b[2m    \u001b[0m\u001b[2;97;40m  \u001b[0m\u001b[2;91;40mmax_std\u001b[0m\u001b[2;97;40m:\u001b[0m\u001b[2;97;40m \u001b[0m\u001b[2;40m1.0                                                          \u001b[0m\n",
      "\u001b[2m│   \u001b[0m\u001b[2m    \u001b[0m\u001b[2;97;40m  \u001b[0m\u001b[2;91;40minit_std\u001b[0m\u001b[2;97;40m:\u001b[0m\u001b[2;97;40m \u001b[0m\u001b[2;40m2.0                                                         \u001b[0m\n",
      "\u001b[2m│   \u001b[0m\u001b[2m    \u001b[0m\u001b[2;97;40m  \u001b[0m\u001b[2;91;40mdense_act\u001b[0m\u001b[2;97;40m:\u001b[0m\u001b[2;97;40m \u001b[0m\u001b[2;40mtorch.nn.SiLU                                              \u001b[0m\n",
      "\u001b[2m│   \u001b[0m\u001b[2m    \u001b[0m\u001b[2;97;40m  \u001b[0m\u001b[2;91;40mmlp_layers\u001b[0m\u001b[2;97;40m:\u001b[0m\u001b[2;97;40m \u001b[0m\u001b[2;40m4                                                         \u001b[0m\n",
      "\u001b[2m│   \u001b[0m\u001b[2m    \u001b[0m\u001b[2;97;40m  \u001b[0m\u001b[2;91;40mlayer_norm\u001b[0m\u001b[2;97;40m:\u001b[0m\u001b[2;40m                                                           \u001b[0m\n",
      "\u001b[2m│   \u001b[0m\u001b[2m    \u001b[0m\u001b[2;97;40m    \u001b[0m\u001b[2;91;40mcls\u001b[0m\u001b[2;97;40m:\u001b[0m\u001b[2;97;40m \u001b[0m\u001b[2;40msheeprl.models.models.LayerNorm                                \u001b[0m\n",
      "\u001b[2m│   \u001b[0m\u001b[2m    \u001b[0m\u001b[2;97;40m    \u001b[0m\u001b[2;91;40mkw\u001b[0m\u001b[2;97;40m:\u001b[0m\u001b[2;40m                                                                 \u001b[0m\n",
      "\u001b[2m│   \u001b[0m\u001b[2m    \u001b[0m\u001b[2;97;40m      \u001b[0m\u001b[2;91;40meps\u001b[0m\u001b[2;97;40m:\u001b[0m\u001b[2;97;40m \u001b[0m\u001b[2;40m0.001                                                        \u001b[0m\n",
      "\u001b[2m│   \u001b[0m\u001b[2m    \u001b[0m\u001b[2;97;40m  \u001b[0m\u001b[2;91;40mdense_units\u001b[0m\u001b[2;97;40m:\u001b[0m\u001b[2;97;40m \u001b[0m\u001b[2;40m768                                                      \u001b[0m\n",
      "\u001b[2m│   \u001b[0m\u001b[2m    \u001b[0m\u001b[2;97;40m  \u001b[0m\u001b[2;91;40mclip_gradients\u001b[0m\u001b[2;97;40m:\u001b[0m\u001b[2;97;40m \u001b[0m\u001b[2;40m100.0                                                 \u001b[0m\n",
      "\u001b[2m│   \u001b[0m\u001b[2m    \u001b[0m\u001b[2;97;40m  \u001b[0m\u001b[2;91;40munimix\u001b[0m\u001b[2;97;40m:\u001b[0m\u001b[2;97;40m \u001b[0m\u001b[2;40m0.01                                                          \u001b[0m\n",
      "\u001b[2m│   \u001b[0m\u001b[2m    \u001b[0m\u001b[2;97;40m  \u001b[0m\u001b[2;91;40maction_clip\u001b[0m\u001b[2;97;40m:\u001b[0m\u001b[2;97;40m \u001b[0m\u001b[2;40m1.0                                                      \u001b[0m\n",
      "\u001b[2m│   \u001b[0m\u001b[2m    \u001b[0m\u001b[2;97;40m  \u001b[0m\u001b[2;91;40mmoments\u001b[0m\u001b[2;97;40m:\u001b[0m\u001b[2;40m                                                              \u001b[0m\n",
      "\u001b[2m│   \u001b[0m\u001b[2m    \u001b[0m\u001b[2;97;40m    \u001b[0m\u001b[2;91;40mdecay\u001b[0m\u001b[2;97;40m:\u001b[0m\u001b[2;97;40m \u001b[0m\u001b[2;40m0.99                                                         \u001b[0m\n",
      "\u001b[2m│   \u001b[0m\u001b[2m    \u001b[0m\u001b[2;97;40m    \u001b[0m\u001b[2;91;40mmax\u001b[0m\u001b[2;97;40m:\u001b[0m\u001b[2;97;40m \u001b[0m\u001b[2;40m1.0                                                            \u001b[0m\n",
      "\u001b[2m│   \u001b[0m\u001b[2m    \u001b[0m\u001b[2;97;40m    \u001b[0m\u001b[2;91;40mpercentile\u001b[0m\u001b[2;97;40m:\u001b[0m\u001b[2;40m                                                         \u001b[0m\n",
      "\u001b[2m│   \u001b[0m\u001b[2m    \u001b[0m\u001b[2;97;40m      \u001b[0m\u001b[2;91;40mlow\u001b[0m\u001b[2;97;40m:\u001b[0m\u001b[2;97;40m \u001b[0m\u001b[2;40m0.05                                                         \u001b[0m\n",
      "\u001b[2m│   \u001b[0m\u001b[2m    \u001b[0m\u001b[2;97;40m      \u001b[0m\u001b[2;91;40mhigh\u001b[0m\u001b[2;97;40m:\u001b[0m\u001b[2;97;40m \u001b[0m\u001b[2;40m0.95                                                        \u001b[0m\n",
      "\u001b[2m│   \u001b[0m\u001b[2m    \u001b[0m\u001b[2;91;40mcritic\u001b[0m\u001b[2;97;40m:\u001b[0m\u001b[2;40m                                                                 \u001b[0m\n",
      "\u001b[2m│   \u001b[0m\u001b[2m    \u001b[0m\u001b[2;97;40m  \u001b[0m\u001b[2;91;40moptimizer\u001b[0m\u001b[2;97;40m:\u001b[0m\u001b[2;40m                                                            \u001b[0m\n",
      "\u001b[2m│   \u001b[0m\u001b[2m    \u001b[0m\u001b[2;97;40m    \u001b[0m\u001b[2;91;40m_target_\u001b[0m\u001b[2;97;40m:\u001b[0m\u001b[2;97;40m \u001b[0m\u001b[2;40mtorch.optim.Adam                                          \u001b[0m\n",
      "\u001b[2m│   \u001b[0m\u001b[2m    \u001b[0m\u001b[2;97;40m    \u001b[0m\u001b[2;91;40mlr\u001b[0m\u001b[2;97;40m:\u001b[0m\u001b[2;97;40m \u001b[0m\u001b[2;40m8.0e-05                                                         \u001b[0m\n",
      "\u001b[2m│   \u001b[0m\u001b[2m    \u001b[0m\u001b[2;97;40m    \u001b[0m\u001b[2;91;40meps\u001b[0m\u001b[2;97;40m:\u001b[0m\u001b[2;97;40m \u001b[0m\u001b[2;40m1.0e-05                                                        \u001b[0m\n",
      "\u001b[2m│   \u001b[0m\u001b[2m    \u001b[0m\u001b[2;97;40m    \u001b[0m\u001b[2;91;40mweight_decay\u001b[0m\u001b[2;97;40m:\u001b[0m\u001b[2;97;40m \u001b[0m\u001b[2;40m0                                                     \u001b[0m\n",
      "\u001b[2m│   \u001b[0m\u001b[2m    \u001b[0m\u001b[2;97;40m    \u001b[0m\u001b[2;91;40mbetas\u001b[0m\u001b[2;97;40m:\u001b[0m\u001b[2;40m                                                              \u001b[0m\n",
      "\u001b[2m│   \u001b[0m\u001b[2m    \u001b[0m\u001b[2;97;40m    \u001b[0m\u001b[2;40m-\u001b[0m\u001b[2;97;40m \u001b[0m\u001b[2;40m0.9                                                               \u001b[0m\n",
      "\u001b[2m│   \u001b[0m\u001b[2m    \u001b[0m\u001b[2;97;40m    \u001b[0m\u001b[2;40m-\u001b[0m\u001b[2;97;40m \u001b[0m\u001b[2;40m0.999                                                             \u001b[0m\n",
      "\u001b[2m│   \u001b[0m\u001b[2m    \u001b[0m\u001b[2;97;40m  \u001b[0m\u001b[2;91;40mdense_act\u001b[0m\u001b[2;97;40m:\u001b[0m\u001b[2;97;40m \u001b[0m\u001b[2;40mtorch.nn.SiLU                                              \u001b[0m\n",
      "\u001b[2m│   \u001b[0m\u001b[2m    \u001b[0m\u001b[2;97;40m  \u001b[0m\u001b[2;91;40mmlp_layers\u001b[0m\u001b[2;97;40m:\u001b[0m\u001b[2;97;40m \u001b[0m\u001b[2;40m4                                                         \u001b[0m\n",
      "\u001b[2m│   \u001b[0m\u001b[2m    \u001b[0m\u001b[2;97;40m  \u001b[0m\u001b[2;91;40mlayer_norm\u001b[0m\u001b[2;97;40m:\u001b[0m\u001b[2;40m                                                           \u001b[0m\n",
      "\u001b[2m│   \u001b[0m\u001b[2m    \u001b[0m\u001b[2;97;40m    \u001b[0m\u001b[2;91;40mcls\u001b[0m\u001b[2;97;40m:\u001b[0m\u001b[2;97;40m \u001b[0m\u001b[2;40msheeprl.models.models.LayerNorm                                \u001b[0m\n",
      "\u001b[2m│   \u001b[0m\u001b[2m    \u001b[0m\u001b[2;97;40m    \u001b[0m\u001b[2;91;40mkw\u001b[0m\u001b[2;97;40m:\u001b[0m\u001b[2;40m                                                                 \u001b[0m\n",
      "\u001b[2m│   \u001b[0m\u001b[2m    \u001b[0m\u001b[2;97;40m      \u001b[0m\u001b[2;91;40meps\u001b[0m\u001b[2;97;40m:\u001b[0m\u001b[2;97;40m \u001b[0m\u001b[2;40m0.001                                                        \u001b[0m\n",
      "\u001b[2m│   \u001b[0m\u001b[2m    \u001b[0m\u001b[2;97;40m  \u001b[0m\u001b[2;91;40mdense_units\u001b[0m\u001b[2;97;40m:\u001b[0m\u001b[2;97;40m \u001b[0m\u001b[2;40m768                                                      \u001b[0m\n",
      "\u001b[2m│   \u001b[0m\u001b[2m    \u001b[0m\u001b[2;97;40m  \u001b[0m\u001b[2;91;40mper_rank_target_network_update_freq\u001b[0m\u001b[2;97;40m:\u001b[0m\u001b[2;97;40m \u001b[0m\u001b[2;40m1                                \u001b[0m\n",
      "\u001b[2m│   \u001b[0m\u001b[2m    \u001b[0m\u001b[2;97;40m  \u001b[0m\u001b[2;91;40mtau\u001b[0m\u001b[2;97;40m:\u001b[0m\u001b[2;97;40m \u001b[0m\u001b[2;40m0.02                                                             \u001b[0m\n",
      "\u001b[2m│   \u001b[0m\u001b[2m    \u001b[0m\u001b[2;97;40m  \u001b[0m\u001b[2;91;40mbins\u001b[0m\u001b[2;97;40m:\u001b[0m\u001b[2;97;40m \u001b[0m\u001b[2;40m255                                                             \u001b[0m\n",
      "\u001b[2m│   \u001b[0m\u001b[2m    \u001b[0m\u001b[2;97;40m  \u001b[0m\u001b[2;91;40mclip_gradients\u001b[0m\u001b[2;97;40m:\u001b[0m\u001b[2;97;40m \u001b[0m\u001b[2;40m100.0                                                 \u001b[0m\n",
      "\u001b[2m│   \u001b[0m\u001b[2m    \u001b[0m\u001b[2;91;40mgamma\u001b[0m\u001b[2;97;40m:\u001b[0m\u001b[2;97;40m \u001b[0m\u001b[2;40m0.996996996996997                                                \u001b[0m\n",
      "\u001b[2m│   \u001b[0m\u001b[2m    \u001b[0m\u001b[2;91;40mlmbda\u001b[0m\u001b[2;97;40m:\u001b[0m\u001b[2;97;40m \u001b[0m\u001b[2;40m0.95                                                             \u001b[0m\n",
      "\u001b[2m│   \u001b[0m\u001b[2m    \u001b[0m\u001b[2;91;40mhorizon\u001b[0m\u001b[2;97;40m:\u001b[0m\u001b[2;97;40m \u001b[0m\u001b[2;40m15                                                             \u001b[0m\n",
      "\u001b[2m│   \u001b[0m\u001b[2m    \u001b[0m\u001b[2;91;40mreplay_ratio\u001b[0m\u001b[2;97;40m:\u001b[0m\u001b[2;97;40m \u001b[0m\u001b[2;40m0.5                                                       \u001b[0m\n",
      "\u001b[2m│   \u001b[0m\u001b[2m    \u001b[0m\u001b[2;91;40mlearning_starts\u001b[0m\u001b[2;97;40m:\u001b[0m\u001b[2;97;40m \u001b[0m\u001b[2;40m10000                                                  \u001b[0m\n",
      "\u001b[2m│   \u001b[0m\u001b[2m    \u001b[0m\u001b[2;91;40mper_rank_pretrain_steps\u001b[0m\u001b[2;97;40m:\u001b[0m\u001b[2;97;40m \u001b[0m\u001b[2;40m0                                              \u001b[0m\n",
      "\u001b[2m│   \u001b[0m\u001b[2m    \u001b[0m\u001b[2;91;40mper_rank_sequence_length\u001b[0m\u001b[2;97;40m:\u001b[0m\u001b[2;97;40m \u001b[0m\u001b[2;40m32                                            \u001b[0m\n",
      "\u001b[2m│   \u001b[0m\u001b[2m    \u001b[0m\u001b[2;91;40mcnn_layer_norm\u001b[0m\u001b[2;97;40m:\u001b[0m\u001b[2;40m                                                         \u001b[0m\n",
      "\u001b[2m│   \u001b[0m\u001b[2m    \u001b[0m\u001b[2;97;40m  \u001b[0m\u001b[2;91;40mcls\u001b[0m\u001b[2;97;40m:\u001b[0m\u001b[2;97;40m \u001b[0m\u001b[2;40msheeprl.models.models.LayerNormChannelLast                       \u001b[0m\n",
      "\u001b[2m│   \u001b[0m\u001b[2m    \u001b[0m\u001b[2;97;40m  \u001b[0m\u001b[2;91;40mkw\u001b[0m\u001b[2;97;40m:\u001b[0m\u001b[2;40m                                                                   \u001b[0m\n",
      "\u001b[2m│   \u001b[0m\u001b[2m    \u001b[0m\u001b[2;97;40m    \u001b[0m\u001b[2;91;40meps\u001b[0m\u001b[2;97;40m:\u001b[0m\u001b[2;97;40m \u001b[0m\u001b[2;40m0.001                                                          \u001b[0m\n",
      "\u001b[2m│   \u001b[0m\u001b[2m    \u001b[0m\u001b[2;91;40mmlp_layer_norm\u001b[0m\u001b[2;97;40m:\u001b[0m\u001b[2;40m                                                         \u001b[0m\n",
      "\u001b[2m│   \u001b[0m\u001b[2m    \u001b[0m\u001b[2;97;40m  \u001b[0m\u001b[2;91;40mcls\u001b[0m\u001b[2;97;40m:\u001b[0m\u001b[2;97;40m \u001b[0m\u001b[2;40msheeprl.models.models.LayerNorm                                  \u001b[0m\n",
      "\u001b[2m│   \u001b[0m\u001b[2m    \u001b[0m\u001b[2;97;40m  \u001b[0m\u001b[2;91;40mkw\u001b[0m\u001b[2;97;40m:\u001b[0m\u001b[2;40m                                                                   \u001b[0m\n",
      "\u001b[2m│   \u001b[0m\u001b[2m    \u001b[0m\u001b[2;97;40m    \u001b[0m\u001b[2;91;40meps\u001b[0m\u001b[2;97;40m:\u001b[0m\u001b[2;97;40m \u001b[0m\u001b[2;40m0.001                                                          \u001b[0m\n",
      "\u001b[2m│   \u001b[0m\u001b[2m    \u001b[0m\u001b[2;91;40mdense_units\u001b[0m\u001b[2;97;40m:\u001b[0m\u001b[2;97;40m \u001b[0m\u001b[2;40m768                                                        \u001b[0m\n",
      "\u001b[2m│   \u001b[0m\u001b[2m    \u001b[0m\u001b[2;91;40mmlp_layers\u001b[0m\u001b[2;97;40m:\u001b[0m\u001b[2;97;40m \u001b[0m\u001b[2;40m4                                                           \u001b[0m\n",
      "\u001b[2m│   \u001b[0m\u001b[2m    \u001b[0m\u001b[2;91;40mdense_act\u001b[0m\u001b[2;97;40m:\u001b[0m\u001b[2;97;40m \u001b[0m\u001b[2;40mtorch.nn.SiLU                                                \u001b[0m\n",
      "\u001b[2m│   \u001b[0m\u001b[2m    \u001b[0m\u001b[2;91;40mcnn_act\u001b[0m\u001b[2;97;40m:\u001b[0m\u001b[2;97;40m \u001b[0m\u001b[2;40mtorch.nn.SiLU                                                  \u001b[0m\n",
      "\u001b[2m│   \u001b[0m\u001b[2m    \u001b[0m\u001b[2;91;40munimix\u001b[0m\u001b[2;97;40m:\u001b[0m\u001b[2;97;40m \u001b[0m\u001b[2;40m0.01                                                            \u001b[0m\n",
      "\u001b[2m│   \u001b[0m\u001b[2m    \u001b[0m\u001b[2;91;40mhafner_initialization\u001b[0m\u001b[2;97;40m:\u001b[0m\u001b[2;97;40m \u001b[0m\u001b[2;40mtrue                                             \u001b[0m\n",
      "\u001b[2m│   \u001b[0m\u001b[2m    \u001b[0m\u001b[2;91;40mplayer\u001b[0m\u001b[2;97;40m:\u001b[0m\u001b[2;40m                                                                 \u001b[0m\n",
      "\u001b[2m│   \u001b[0m\u001b[2m    \u001b[0m\u001b[2;97;40m  \u001b[0m\u001b[2;91;40mdiscrete_size\u001b[0m\u001b[2;97;40m:\u001b[0m\u001b[2;97;40m \u001b[0m\u001b[2;40m32                                                     \u001b[0m\n",
      "\u001b[2m│   \u001b[0m\u001b[2m    \u001b[0m\u001b[2;40m                                                                        \u001b[0m\n",
      "\u001b[2m├── \u001b[0m\u001b[2mbuffer\u001b[0m\n",
      "\u001b[2m│   \u001b[0m\u001b[2m└── \u001b[0m\u001b[2;91;40msize\u001b[0m\u001b[2;97;40m:\u001b[0m\u001b[2;97;40m \u001b[0m\u001b[2;40m1000000                                                           \u001b[0m\n",
      "\u001b[2m│   \u001b[0m\u001b[2m    \u001b[0m\u001b[2;91;40mmemmap\u001b[0m\u001b[2;97;40m:\u001b[0m\u001b[2;97;40m \u001b[0m\u001b[2;40mtrue                                                            \u001b[0m\n",
      "\u001b[2m│   \u001b[0m\u001b[2m    \u001b[0m\u001b[2;91;40mvalidate_args\u001b[0m\u001b[2;97;40m:\u001b[0m\u001b[2;97;40m \u001b[0m\u001b[2;40mfalse                                                    \u001b[0m\n",
      "\u001b[2m│   \u001b[0m\u001b[2m    \u001b[0m\u001b[2;91;40mfrom_numpy\u001b[0m\u001b[2;97;40m:\u001b[0m\u001b[2;97;40m \u001b[0m\u001b[2;40mfalse                                                       \u001b[0m\n",
      "\u001b[2m│   \u001b[0m\u001b[2m    \u001b[0m\u001b[2;91;40mcheckpoint\u001b[0m\u001b[2;97;40m:\u001b[0m\u001b[2;97;40m \u001b[0m\u001b[2;40mtrue                                                        \u001b[0m\n",
      "\u001b[2m│   \u001b[0m\u001b[2m    \u001b[0m\u001b[2;40m                                                                        \u001b[0m\n",
      "\u001b[2m├── \u001b[0m\u001b[2mcheckpoint\u001b[0m\n",
      "\u001b[2m│   \u001b[0m\u001b[2m└── \u001b[0m\u001b[2;91;40mevery\u001b[0m\u001b[2;97;40m:\u001b[0m\u001b[2;97;40m \u001b[0m\u001b[2;40m10000                                                            \u001b[0m\n",
      "\u001b[2m│   \u001b[0m\u001b[2m    \u001b[0m\u001b[2;91;40mresume_from\u001b[0m\u001b[2;97;40m:\u001b[0m\u001b[2;97;40m \u001b[0m\u001b[2;40mnull                                                       \u001b[0m\n",
      "\u001b[2m│   \u001b[0m\u001b[2m    \u001b[0m\u001b[2;91;40msave_last\u001b[0m\u001b[2;97;40m:\u001b[0m\u001b[2;97;40m \u001b[0m\u001b[2;40mtrue                                                         \u001b[0m\n",
      "\u001b[2m│   \u001b[0m\u001b[2m    \u001b[0m\u001b[2;91;40mkeep_last\u001b[0m\u001b[2;97;40m:\u001b[0m\u001b[2;97;40m \u001b[0m\u001b[2;40m5                                                            \u001b[0m\n",
      "\u001b[2m│   \u001b[0m\u001b[2m    \u001b[0m\u001b[2;40m                                                                        \u001b[0m\n",
      "\u001b[2m├── \u001b[0m\u001b[2menv\u001b[0m\n",
      "\u001b[2m│   \u001b[0m\u001b[2m└── \u001b[0m\u001b[2;91;40mid\u001b[0m\u001b[2;97;40m:\u001b[0m\u001b[2;97;40m \u001b[0m\u001b[2;40misaac_sim_environment                                               \u001b[0m\n",
      "\u001b[2m│   \u001b[0m\u001b[2m    \u001b[0m\u001b[2;91;40mnum_envs\u001b[0m\u001b[2;97;40m:\u001b[0m\u001b[2;97;40m \u001b[0m\u001b[2;40m8                                                             \u001b[0m\n",
      "\u001b[2m│   \u001b[0m\u001b[2m    \u001b[0m\u001b[2;91;40mframe_stack\u001b[0m\u001b[2;97;40m:\u001b[0m\u001b[2;97;40m \u001b[0m\u001b[2;40m1                                                          \u001b[0m\n",
      "\u001b[2m│   \u001b[0m\u001b[2m    \u001b[0m\u001b[2;91;40msync_env\u001b[0m\u001b[2;97;40m:\u001b[0m\u001b[2;97;40m \u001b[0m\u001b[2;40mtrue                                                          \u001b[0m\n",
      "\u001b[2m│   \u001b[0m\u001b[2m    \u001b[0m\u001b[2;91;40mscreen_size\u001b[0m\u001b[2;97;40m:\u001b[0m\u001b[2;97;40m \u001b[0m\u001b[2;40m128                                                        \u001b[0m\n",
      "\u001b[2m│   \u001b[0m\u001b[2m    \u001b[0m\u001b[2;91;40maction_repeat\u001b[0m\u001b[2;97;40m:\u001b[0m\u001b[2;97;40m \u001b[0m\u001b[2;40m1                                                        \u001b[0m\n",
      "\u001b[2m│   \u001b[0m\u001b[2m    \u001b[0m\u001b[2;91;40mgrayscale\u001b[0m\u001b[2;97;40m:\u001b[0m\u001b[2;97;40m \u001b[0m\u001b[2;40mfalse                                                        \u001b[0m\n",
      "\u001b[2m│   \u001b[0m\u001b[2m    \u001b[0m\u001b[2;91;40mclip_rewards\u001b[0m\u001b[2;97;40m:\u001b[0m\u001b[2;97;40m \u001b[0m\u001b[2;40mfalse                                                     \u001b[0m\n",
      "\u001b[2m│   \u001b[0m\u001b[2m    \u001b[0m\u001b[2;91;40mcapture_video\u001b[0m\u001b[2;97;40m:\u001b[0m\u001b[2;97;40m \u001b[0m\u001b[2;40mtrue                                                     \u001b[0m\n",
      "\u001b[2m│   \u001b[0m\u001b[2m    \u001b[0m\u001b[2;91;40mframe_stack_dilation\u001b[0m\u001b[2;97;40m:\u001b[0m\u001b[2;97;40m \u001b[0m\u001b[2;40m1                                                 \u001b[0m\n",
      "\u001b[2m│   \u001b[0m\u001b[2m    \u001b[0m\u001b[2;91;40mmax_episode_steps\u001b[0m\u001b[2;97;40m:\u001b[0m\u001b[2;97;40m \u001b[0m\u001b[2;40m-1                                                   \u001b[0m\n",
      "\u001b[2m│   \u001b[0m\u001b[2m    \u001b[0m\u001b[2;91;40mreward_as_observation\u001b[0m\u001b[2;97;40m:\u001b[0m\u001b[2;97;40m \u001b[0m\u001b[2;40mfalse                                            \u001b[0m\n",
      "\u001b[2m│   \u001b[0m\u001b[2m    \u001b[0m\u001b[2;91;40mwrapper\u001b[0m\u001b[2;97;40m:\u001b[0m\u001b[2;40m                                                                \u001b[0m\n",
      "\u001b[2m│   \u001b[0m\u001b[2m    \u001b[0m\u001b[2;97;40m  \u001b[0m\u001b[2;91;40m_target_\u001b[0m\u001b[2;97;40m:\u001b[0m\u001b[2;97;40m \u001b[0m\u001b[2;40msheeprl.envs.isaacsim.IsaacSimWrapper                       \u001b[0m\n",
      "\u001b[2m│   \u001b[0m\u001b[2m    \u001b[0m\u001b[2;97;40m  \u001b[0m\u001b[2;91;40mmap_size\u001b[0m\u001b[2;97;40m:\u001b[0m\u001b[2;40m                                                             \u001b[0m\n",
      "\u001b[2m│   \u001b[0m\u001b[2m    \u001b[0m\u001b[2;97;40m  \u001b[0m\u001b[2;40m-\u001b[0m\u001b[2;97;40m \u001b[0m\u001b[2;40m128                                                                 \u001b[0m\n",
      "\u001b[2m│   \u001b[0m\u001b[2m    \u001b[0m\u001b[2;97;40m  \u001b[0m\u001b[2;40m-\u001b[0m\u001b[2;97;40m \u001b[0m\u001b[2;40m128                                                                 \u001b[0m\n",
      "\u001b[2m│   \u001b[0m\u001b[2m    \u001b[0m\u001b[2;97;40m  \u001b[0m\u001b[2;91;40mcamera_rgb_size\u001b[0m\u001b[2;97;40m:\u001b[0m\u001b[2;40m                                                      \u001b[0m\n",
      "\u001b[2m│   \u001b[0m\u001b[2m    \u001b[0m\u001b[2;97;40m  \u001b[0m\u001b[2;40m-\u001b[0m\u001b[2;97;40m \u001b[0m\u001b[2;40m128                                                                 \u001b[0m\n",
      "\u001b[2m│   \u001b[0m\u001b[2m    \u001b[0m\u001b[2;97;40m  \u001b[0m\u001b[2;40m-\u001b[0m\u001b[2;97;40m \u001b[0m\u001b[2;40m128                                                                 \u001b[0m\n",
      "\u001b[2m│   \u001b[0m\u001b[2m    \u001b[0m\u001b[2;97;40m  \u001b[0m\u001b[2;91;40mcamera_height\u001b[0m\u001b[2;97;40m:\u001b[0m\u001b[2;97;40m \u001b[0m\u001b[2;40m0.6                                                    \u001b[0m\n",
      "\u001b[2m│   \u001b[0m\u001b[2m    \u001b[0m\u001b[2;97;40m  \u001b[0m\u001b[2;91;40mcamera_linear_velocity\u001b[0m\u001b[2;97;40m:\u001b[0m\u001b[2;97;40m \u001b[0m\u001b[2;40m2.0                                           \u001b[0m\n",
      "\u001b[2m│   \u001b[0m\u001b[2m    \u001b[0m\u001b[2;97;40m  \u001b[0m\u001b[2;91;40mcamera_angular_velocity\u001b[0m\u001b[2;97;40m:\u001b[0m\u001b[2;97;40m \u001b[0m\u001b[2;40m3.1415927                                    \u001b[0m\n",
      "\u001b[2m│   \u001b[0m\u001b[2m    \u001b[0m\u001b[2;97;40m  \u001b[0m\u001b[2;91;40mcamera_parent\u001b[0m\u001b[2;97;40m:\u001b[0m\u001b[2;97;40m \u001b[0m\u001b[2;40m/World                                                 \u001b[0m\n",
      "\u001b[2m│   \u001b[0m\u001b[2m    \u001b[0m\u001b[2;97;40m  \u001b[0m\u001b[2;91;40mcamera_name\u001b[0m\u001b[2;97;40m:\u001b[0m\u001b[2;97;40m \u001b[0m\u001b[2;40mCamera_                                                  \u001b[0m\n",
      "\u001b[2m│   \u001b[0m\u001b[2m    \u001b[0m\u001b[2;97;40m  \u001b[0m\u001b[2;91;40mseed\u001b[0m\u001b[2;97;40m:\u001b[0m\u001b[2;97;40m \u001b[0m\u001b[2;40mnull                                                            \u001b[0m\n",
      "\u001b[2m│   \u001b[0m\u001b[2m    \u001b[0m\u001b[2;40m                                                                        \u001b[0m\n",
      "\u001b[2m├── \u001b[0m\u001b[2mfabric\u001b[0m\n",
      "\u001b[2m│   \u001b[0m\u001b[2m└── \u001b[0m\u001b[2;91;40m_target_\u001b[0m\u001b[2;97;40m:\u001b[0m\u001b[2;97;40m \u001b[0m\u001b[2;40mlightning.fabric.Fabric                                       \u001b[0m\n",
      "\u001b[2m│   \u001b[0m\u001b[2m    \u001b[0m\u001b[2;91;40mdevices\u001b[0m\u001b[2;97;40m:\u001b[0m\u001b[2;97;40m \u001b[0m\u001b[2;40m1                                                              \u001b[0m\n",
      "\u001b[2m│   \u001b[0m\u001b[2m    \u001b[0m\u001b[2;91;40mnum_nodes\u001b[0m\u001b[2;97;40m:\u001b[0m\u001b[2;97;40m \u001b[0m\u001b[2;40m1                                                            \u001b[0m\n",
      "\u001b[2m│   \u001b[0m\u001b[2m    \u001b[0m\u001b[2;91;40mstrategy\u001b[0m\u001b[2;97;40m:\u001b[0m\u001b[2;97;40m \u001b[0m\u001b[2;40mauto                                                          \u001b[0m\n",
      "\u001b[2m│   \u001b[0m\u001b[2m    \u001b[0m\u001b[2;91;40maccelerator\u001b[0m\u001b[2;97;40m:\u001b[0m\u001b[2;97;40m \u001b[0m\u001b[2;40mcuda                                                       \u001b[0m\n",
      "\u001b[2m│   \u001b[0m\u001b[2m    \u001b[0m\u001b[2;91;40mprecision\u001b[0m\u001b[2;97;40m:\u001b[0m\u001b[2;97;40m \u001b[0m\u001b[2;40mbf16-mixed                                                   \u001b[0m\n",
      "\u001b[2m│   \u001b[0m\u001b[2m    \u001b[0m\u001b[2;91;40mcallbacks\u001b[0m\u001b[2;97;40m:\u001b[0m\u001b[2;40m                                                              \u001b[0m\n",
      "\u001b[2m│   \u001b[0m\u001b[2m    \u001b[0m\u001b[2;40m-\u001b[0m\u001b[2;97;40m \u001b[0m\u001b[2;91;40m_target_\u001b[0m\u001b[2;97;40m:\u001b[0m\u001b[2;97;40m \u001b[0m\u001b[2;40msheeprl.utils.callback.CheckpointCallback                   \u001b[0m\n",
      "\u001b[2m│   \u001b[0m\u001b[2m    \u001b[0m\u001b[2;97;40m  \u001b[0m\u001b[2;91;40mkeep_last\u001b[0m\u001b[2;97;40m:\u001b[0m\u001b[2;97;40m \u001b[0m\u001b[2;40m5                                                          \u001b[0m\n",
      "\u001b[2m│   \u001b[0m\u001b[2m    \u001b[0m\u001b[2;40m                                                                        \u001b[0m\n",
      "\u001b[2m└── \u001b[0m\u001b[2mmetric\u001b[0m\n",
      "\u001b[2m    \u001b[0m\u001b[2m└── \u001b[0m\u001b[2;91;40mlog_every\u001b[0m\u001b[2;97;40m:\u001b[0m\u001b[2;97;40m \u001b[0m\u001b[2;40m1000                                                         \u001b[0m\n",
      "\u001b[2m    \u001b[0m\u001b[2m    \u001b[0m\u001b[2;91;40mdisable_timer\u001b[0m\u001b[2;97;40m:\u001b[0m\u001b[2;97;40m \u001b[0m\u001b[2;40mfalse                                                    \u001b[0m\n",
      "\u001b[2m    \u001b[0m\u001b[2m    \u001b[0m\u001b[2;91;40mlog_level\u001b[0m\u001b[2;97;40m:\u001b[0m\u001b[2;97;40m \u001b[0m\u001b[2;40m1                                                            \u001b[0m\n",
      "\u001b[2m    \u001b[0m\u001b[2m    \u001b[0m\u001b[2;91;40msync_on_compute\u001b[0m\u001b[2;97;40m:\u001b[0m\u001b[2;97;40m \u001b[0m\u001b[2;40mfalse                                                  \u001b[0m\n",
      "\u001b[2m    \u001b[0m\u001b[2m    \u001b[0m\u001b[2;91;40maggregator\u001b[0m\u001b[2;97;40m:\u001b[0m\u001b[2;40m                                                             \u001b[0m\n",
      "\u001b[2m    \u001b[0m\u001b[2m    \u001b[0m\u001b[2;97;40m  \u001b[0m\u001b[2;91;40m_target_\u001b[0m\u001b[2;97;40m:\u001b[0m\u001b[2;97;40m \u001b[0m\u001b[2;40msheeprl.utils.metric.MetricAggregator                       \u001b[0m\n",
      "\u001b[2m    \u001b[0m\u001b[2m    \u001b[0m\u001b[2;97;40m  \u001b[0m\u001b[2;91;40mraise_on_missing\u001b[0m\u001b[2;97;40m:\u001b[0m\u001b[2;97;40m \u001b[0m\u001b[2;40mfalse                                               \u001b[0m\n",
      "\u001b[2m    \u001b[0m\u001b[2m    \u001b[0m\u001b[2;97;40m  \u001b[0m\u001b[2;91;40mmetrics\u001b[0m\u001b[2;97;40m:\u001b[0m\u001b[2;40m                                                              \u001b[0m\n",
      "\u001b[2m    \u001b[0m\u001b[2m    \u001b[0m\u001b[2;97;40m    \u001b[0m\u001b[2;91;40mRewards/rew_avg\u001b[0m\u001b[2;97;40m:\u001b[0m\u001b[2;40m                                                    \u001b[0m\n",
      "\u001b[2m    \u001b[0m\u001b[2m    \u001b[0m\u001b[2;97;40m      \u001b[0m\u001b[2;91;40m_target_\u001b[0m\u001b[2;97;40m:\u001b[0m\u001b[2;97;40m \u001b[0m\u001b[2;40mtorchmetrics.MeanMetric                                 \u001b[0m\n",
      "\u001b[2m    \u001b[0m\u001b[2m    \u001b[0m\u001b[2;97;40m      \u001b[0m\u001b[2;91;40msync_on_compute\u001b[0m\u001b[2;97;40m:\u001b[0m\u001b[2;97;40m \u001b[0m\u001b[2;40mfalse                                            \u001b[0m\n",
      "\u001b[2m    \u001b[0m\u001b[2m    \u001b[0m\u001b[2;97;40m    \u001b[0m\u001b[2;91;40mGame/ep_len_avg\u001b[0m\u001b[2;97;40m:\u001b[0m\u001b[2;40m                                                    \u001b[0m\n",
      "\u001b[2m    \u001b[0m\u001b[2m    \u001b[0m\u001b[2;97;40m      \u001b[0m\u001b[2;91;40m_target_\u001b[0m\u001b[2;97;40m:\u001b[0m\u001b[2;97;40m \u001b[0m\u001b[2;40mtorchmetrics.MeanMetric                                 \u001b[0m\n",
      "\u001b[2m    \u001b[0m\u001b[2m    \u001b[0m\u001b[2;97;40m      \u001b[0m\u001b[2;91;40msync_on_compute\u001b[0m\u001b[2;97;40m:\u001b[0m\u001b[2;97;40m \u001b[0m\u001b[2;40mfalse                                            \u001b[0m\n",
      "\u001b[2m    \u001b[0m\u001b[2m    \u001b[0m\u001b[2;97;40m    \u001b[0m\u001b[2;91;40mLoss/world_model_loss\u001b[0m\u001b[2;97;40m:\u001b[0m\u001b[2;40m                                              \u001b[0m\n",
      "\u001b[2m    \u001b[0m\u001b[2m    \u001b[0m\u001b[2;97;40m      \u001b[0m\u001b[2;91;40m_target_\u001b[0m\u001b[2;97;40m:\u001b[0m\u001b[2;97;40m \u001b[0m\u001b[2;40mtorchmetrics.MeanMetric                                 \u001b[0m\n",
      "\u001b[2m    \u001b[0m\u001b[2m    \u001b[0m\u001b[2;97;40m      \u001b[0m\u001b[2;91;40msync_on_compute\u001b[0m\u001b[2;97;40m:\u001b[0m\u001b[2;97;40m \u001b[0m\u001b[2;40mfalse                                            \u001b[0m\n",
      "\u001b[2m    \u001b[0m\u001b[2m    \u001b[0m\u001b[2;97;40m    \u001b[0m\u001b[2;91;40mLoss/value_loss\u001b[0m\u001b[2;97;40m:\u001b[0m\u001b[2;40m                                                    \u001b[0m\n",
      "\u001b[2m    \u001b[0m\u001b[2m    \u001b[0m\u001b[2;97;40m      \u001b[0m\u001b[2;91;40m_target_\u001b[0m\u001b[2;97;40m:\u001b[0m\u001b[2;97;40m \u001b[0m\u001b[2;40mtorchmetrics.MeanMetric                                 \u001b[0m\n",
      "\u001b[2m    \u001b[0m\u001b[2m    \u001b[0m\u001b[2;97;40m      \u001b[0m\u001b[2;91;40msync_on_compute\u001b[0m\u001b[2;97;40m:\u001b[0m\u001b[2;97;40m \u001b[0m\u001b[2;40mfalse                                            \u001b[0m\n",
      "\u001b[2m    \u001b[0m\u001b[2m    \u001b[0m\u001b[2;97;40m    \u001b[0m\u001b[2;91;40mLoss/policy_loss\u001b[0m\u001b[2;97;40m:\u001b[0m\u001b[2;40m                                                   \u001b[0m\n",
      "\u001b[2m    \u001b[0m\u001b[2m    \u001b[0m\u001b[2;97;40m      \u001b[0m\u001b[2;91;40m_target_\u001b[0m\u001b[2;97;40m:\u001b[0m\u001b[2;97;40m \u001b[0m\u001b[2;40mtorchmetrics.MeanMetric                                 \u001b[0m\n",
      "\u001b[2m    \u001b[0m\u001b[2m    \u001b[0m\u001b[2;97;40m      \u001b[0m\u001b[2;91;40msync_on_compute\u001b[0m\u001b[2;97;40m:\u001b[0m\u001b[2;97;40m \u001b[0m\u001b[2;40mfalse                                            \u001b[0m\n",
      "\u001b[2m    \u001b[0m\u001b[2m    \u001b[0m\u001b[2;97;40m    \u001b[0m\u001b[2;91;40mLoss/observation_loss\u001b[0m\u001b[2;97;40m:\u001b[0m\u001b[2;40m                                              \u001b[0m\n",
      "\u001b[2m    \u001b[0m\u001b[2m    \u001b[0m\u001b[2;97;40m      \u001b[0m\u001b[2;91;40m_target_\u001b[0m\u001b[2;97;40m:\u001b[0m\u001b[2;97;40m \u001b[0m\u001b[2;40mtorchmetrics.MeanMetric                                 \u001b[0m\n",
      "\u001b[2m    \u001b[0m\u001b[2m    \u001b[0m\u001b[2;97;40m      \u001b[0m\u001b[2;91;40msync_on_compute\u001b[0m\u001b[2;97;40m:\u001b[0m\u001b[2;97;40m \u001b[0m\u001b[2;40mfalse                                            \u001b[0m\n",
      "\u001b[2m    \u001b[0m\u001b[2m    \u001b[0m\u001b[2;97;40m    \u001b[0m\u001b[2;91;40mLoss/reward_loss\u001b[0m\u001b[2;97;40m:\u001b[0m\u001b[2;40m                                                   \u001b[0m\n",
      "\u001b[2m    \u001b[0m\u001b[2m    \u001b[0m\u001b[2;97;40m      \u001b[0m\u001b[2;91;40m_target_\u001b[0m\u001b[2;97;40m:\u001b[0m\u001b[2;97;40m \u001b[0m\u001b[2;40mtorchmetrics.MeanMetric                                 \u001b[0m\n",
      "\u001b[2m    \u001b[0m\u001b[2m    \u001b[0m\u001b[2;97;40m      \u001b[0m\u001b[2;91;40msync_on_compute\u001b[0m\u001b[2;97;40m:\u001b[0m\u001b[2;97;40m \u001b[0m\u001b[2;40mfalse                                            \u001b[0m\n",
      "\u001b[2m    \u001b[0m\u001b[2m    \u001b[0m\u001b[2;97;40m    \u001b[0m\u001b[2;91;40mLoss/state_loss\u001b[0m\u001b[2;97;40m:\u001b[0m\u001b[2;40m                                                    \u001b[0m\n",
      "\u001b[2m    \u001b[0m\u001b[2m    \u001b[0m\u001b[2;97;40m      \u001b[0m\u001b[2;91;40m_target_\u001b[0m\u001b[2;97;40m:\u001b[0m\u001b[2;97;40m \u001b[0m\u001b[2;40mtorchmetrics.MeanMetric                                 \u001b[0m\n",
      "\u001b[2m    \u001b[0m\u001b[2m    \u001b[0m\u001b[2;97;40m      \u001b[0m\u001b[2;91;40msync_on_compute\u001b[0m\u001b[2;97;40m:\u001b[0m\u001b[2;97;40m \u001b[0m\u001b[2;40mfalse                                            \u001b[0m\n",
      "\u001b[2m    \u001b[0m\u001b[2m    \u001b[0m\u001b[2;97;40m    \u001b[0m\u001b[2;91;40mLoss/continue_loss\u001b[0m\u001b[2;97;40m:\u001b[0m\u001b[2;40m                                                 \u001b[0m\n",
      "\u001b[2m    \u001b[0m\u001b[2m    \u001b[0m\u001b[2;97;40m      \u001b[0m\u001b[2;91;40m_target_\u001b[0m\u001b[2;97;40m:\u001b[0m\u001b[2;97;40m \u001b[0m\u001b[2;40mtorchmetrics.MeanMetric                                 \u001b[0m\n",
      "\u001b[2m    \u001b[0m\u001b[2m    \u001b[0m\u001b[2;97;40m      \u001b[0m\u001b[2;91;40msync_on_compute\u001b[0m\u001b[2;97;40m:\u001b[0m\u001b[2;97;40m \u001b[0m\u001b[2;40mfalse                                            \u001b[0m\n",
      "\u001b[2m    \u001b[0m\u001b[2m    \u001b[0m\u001b[2;97;40m    \u001b[0m\u001b[2;91;40mState/kl\u001b[0m\u001b[2;97;40m:\u001b[0m\u001b[2;40m                                                           \u001b[0m\n",
      "\u001b[2m    \u001b[0m\u001b[2m    \u001b[0m\u001b[2;97;40m      \u001b[0m\u001b[2;91;40m_target_\u001b[0m\u001b[2;97;40m:\u001b[0m\u001b[2;97;40m \u001b[0m\u001b[2;40mtorchmetrics.MeanMetric                                 \u001b[0m\n",
      "\u001b[2m    \u001b[0m\u001b[2m    \u001b[0m\u001b[2;97;40m      \u001b[0m\u001b[2;91;40msync_on_compute\u001b[0m\u001b[2;97;40m:\u001b[0m\u001b[2;97;40m \u001b[0m\u001b[2;40mfalse                                            \u001b[0m\n",
      "\u001b[2m    \u001b[0m\u001b[2m    \u001b[0m\u001b[2;97;40m    \u001b[0m\u001b[2;91;40mState/post_entropy\u001b[0m\u001b[2;97;40m:\u001b[0m\u001b[2;40m                                                 \u001b[0m\n",
      "\u001b[2m    \u001b[0m\u001b[2m    \u001b[0m\u001b[2;97;40m      \u001b[0m\u001b[2;91;40m_target_\u001b[0m\u001b[2;97;40m:\u001b[0m\u001b[2;97;40m \u001b[0m\u001b[2;40mtorchmetrics.MeanMetric                                 \u001b[0m\n",
      "\u001b[2m    \u001b[0m\u001b[2m    \u001b[0m\u001b[2;97;40m      \u001b[0m\u001b[2;91;40msync_on_compute\u001b[0m\u001b[2;97;40m:\u001b[0m\u001b[2;97;40m \u001b[0m\u001b[2;40mfalse                                            \u001b[0m\n",
      "\u001b[2m    \u001b[0m\u001b[2m    \u001b[0m\u001b[2;97;40m    \u001b[0m\u001b[2;91;40mState/prior_entropy\u001b[0m\u001b[2;97;40m:\u001b[0m\u001b[2;40m                                                \u001b[0m\n",
      "\u001b[2m    \u001b[0m\u001b[2m    \u001b[0m\u001b[2;97;40m      \u001b[0m\u001b[2;91;40m_target_\u001b[0m\u001b[2;97;40m:\u001b[0m\u001b[2;97;40m \u001b[0m\u001b[2;40mtorchmetrics.MeanMetric                                 \u001b[0m\n",
      "\u001b[2m    \u001b[0m\u001b[2m    \u001b[0m\u001b[2;97;40m      \u001b[0m\u001b[2;91;40msync_on_compute\u001b[0m\u001b[2;97;40m:\u001b[0m\u001b[2;97;40m \u001b[0m\u001b[2;40mfalse                                            \u001b[0m\n",
      "\u001b[2m    \u001b[0m\u001b[2m    \u001b[0m\u001b[2;97;40m    \u001b[0m\u001b[2;91;40mGrads/world_model\u001b[0m\u001b[2;97;40m:\u001b[0m\u001b[2;40m                                                  \u001b[0m\n",
      "\u001b[2m    \u001b[0m\u001b[2m    \u001b[0m\u001b[2;97;40m      \u001b[0m\u001b[2;91;40m_target_\u001b[0m\u001b[2;97;40m:\u001b[0m\u001b[2;97;40m \u001b[0m\u001b[2;40mtorchmetrics.MeanMetric                                 \u001b[0m\n",
      "\u001b[2m    \u001b[0m\u001b[2m    \u001b[0m\u001b[2;97;40m      \u001b[0m\u001b[2;91;40msync_on_compute\u001b[0m\u001b[2;97;40m:\u001b[0m\u001b[2;97;40m \u001b[0m\u001b[2;40mfalse                                            \u001b[0m\n",
      "\u001b[2m    \u001b[0m\u001b[2m    \u001b[0m\u001b[2;97;40m    \u001b[0m\u001b[2;91;40mGrads/actor\u001b[0m\u001b[2;97;40m:\u001b[0m\u001b[2;40m                                                        \u001b[0m\n",
      "\u001b[2m    \u001b[0m\u001b[2m    \u001b[0m\u001b[2;97;40m      \u001b[0m\u001b[2;91;40m_target_\u001b[0m\u001b[2;97;40m:\u001b[0m\u001b[2;97;40m \u001b[0m\u001b[2;40mtorchmetrics.MeanMetric                                 \u001b[0m\n",
      "\u001b[2m    \u001b[0m\u001b[2m    \u001b[0m\u001b[2;97;40m      \u001b[0m\u001b[2;91;40msync_on_compute\u001b[0m\u001b[2;97;40m:\u001b[0m\u001b[2;97;40m \u001b[0m\u001b[2;40mfalse                                            \u001b[0m\n",
      "\u001b[2m    \u001b[0m\u001b[2m    \u001b[0m\u001b[2;97;40m    \u001b[0m\u001b[2;91;40mGrads/critic\u001b[0m\u001b[2;97;40m:\u001b[0m\u001b[2;40m                                                       \u001b[0m\n",
      "\u001b[2m    \u001b[0m\u001b[2m    \u001b[0m\u001b[2;97;40m      \u001b[0m\u001b[2;91;40m_target_\u001b[0m\u001b[2;97;40m:\u001b[0m\u001b[2;97;40m \u001b[0m\u001b[2;40mtorchmetrics.MeanMetric                                 \u001b[0m\n",
      "\u001b[2m    \u001b[0m\u001b[2m    \u001b[0m\u001b[2;97;40m      \u001b[0m\u001b[2;91;40msync_on_compute\u001b[0m\u001b[2;97;40m:\u001b[0m\u001b[2;97;40m \u001b[0m\u001b[2;40mfalse                                            \u001b[0m\n",
      "\u001b[2m    \u001b[0m\u001b[2m    \u001b[0m\u001b[2;97;40m    \u001b[0m\u001b[2;91;40mGame/curriculum_level\u001b[0m\u001b[2;97;40m:\u001b[0m\u001b[2;40m                                              \u001b[0m\n",
      "\u001b[2m    \u001b[0m\u001b[2m    \u001b[0m\u001b[2;97;40m      \u001b[0m\u001b[2;91;40m_target_\u001b[0m\u001b[2;97;40m:\u001b[0m\u001b[2;97;40m \u001b[0m\u001b[2;40mtorchmetrics.MeanMetric                                 \u001b[0m\n",
      "\u001b[2m    \u001b[0m\u001b[2m    \u001b[0m\u001b[2;97;40m      \u001b[0m\u001b[2;91;40msync_on_compute\u001b[0m\u001b[2;97;40m:\u001b[0m\u001b[2;97;40m \u001b[0m\u001b[2;40mfalse                                            \u001b[0m\n",
      "\u001b[2m    \u001b[0m\u001b[2m    \u001b[0m\u001b[2;97;40m    \u001b[0m\u001b[2;91;40mGame/goal\u001b[0m\u001b[2;97;40m:\u001b[0m\u001b[2;40m                                                          \u001b[0m\n",
      "\u001b[2m    \u001b[0m\u001b[2m    \u001b[0m\u001b[2;97;40m      \u001b[0m\u001b[2;91;40m_target_\u001b[0m\u001b[2;97;40m:\u001b[0m\u001b[2;97;40m \u001b[0m\u001b[2;40mtorchmetrics.MeanMetric                                 \u001b[0m\n",
      "\u001b[2m    \u001b[0m\u001b[2m    \u001b[0m\u001b[2;97;40m      \u001b[0m\u001b[2;91;40msync_on_compute\u001b[0m\u001b[2;97;40m:\u001b[0m\u001b[2;97;40m \u001b[0m\u001b[2;40mfalse                                            \u001b[0m\n",
      "\u001b[2m    \u001b[0m\u001b[2m    \u001b[0m\u001b[2;97;40m    \u001b[0m\u001b[2;91;40mGame/collision\u001b[0m\u001b[2;97;40m:\u001b[0m\u001b[2;40m                                                     \u001b[0m\n",
      "\u001b[2m    \u001b[0m\u001b[2m    \u001b[0m\u001b[2;97;40m      \u001b[0m\u001b[2;91;40m_target_\u001b[0m\u001b[2;97;40m:\u001b[0m\u001b[2;97;40m \u001b[0m\u001b[2;40mtorchmetrics.MeanMetric                                 \u001b[0m\n",
      "\u001b[2m    \u001b[0m\u001b[2m    \u001b[0m\u001b[2;97;40m      \u001b[0m\u001b[2;91;40msync_on_compute\u001b[0m\u001b[2;97;40m:\u001b[0m\u001b[2;97;40m \u001b[0m\u001b[2;40mfalse                                            \u001b[0m\n",
      "\u001b[2m    \u001b[0m\u001b[2m    \u001b[0m\u001b[2;97;40m    \u001b[0m\u001b[2;91;40mGame/success_rate\u001b[0m\u001b[2;97;40m:\u001b[0m\u001b[2;40m                                                  \u001b[0m\n",
      "\u001b[2m    \u001b[0m\u001b[2m    \u001b[0m\u001b[2;97;40m      \u001b[0m\u001b[2;91;40m_target_\u001b[0m\u001b[2;97;40m:\u001b[0m\u001b[2;97;40m \u001b[0m\u001b[2;40mtorchmetrics.MeanMetric                                 \u001b[0m\n",
      "\u001b[2m    \u001b[0m\u001b[2m    \u001b[0m\u001b[2;97;40m      \u001b[0m\u001b[2;91;40msync_on_compute\u001b[0m\u001b[2;97;40m:\u001b[0m\u001b[2;97;40m \u001b[0m\u001b[2;40mfalse                                            \u001b[0m\n",
      "\u001b[2m    \u001b[0m\u001b[2m    \u001b[0m\u001b[2;97;40m    \u001b[0m\u001b[2;91;40mGame/collision_dynamic\u001b[0m\u001b[2;97;40m:\u001b[0m\u001b[2;40m                                             \u001b[0m\n",
      "\u001b[2m    \u001b[0m\u001b[2m    \u001b[0m\u001b[2;97;40m      \u001b[0m\u001b[2;91;40m_target_\u001b[0m\u001b[2;97;40m:\u001b[0m\u001b[2;97;40m \u001b[0m\u001b[2;40mtorchmetrics.MeanMetric                                 \u001b[0m\n",
      "\u001b[2m    \u001b[0m\u001b[2m    \u001b[0m\u001b[2;97;40m      \u001b[0m\u001b[2;91;40msync_on_compute\u001b[0m\u001b[2;97;40m:\u001b[0m\u001b[2;97;40m \u001b[0m\u001b[2;40mfalse                                            \u001b[0m\n",
      "\u001b[2m    \u001b[0m\u001b[2m    \u001b[0m\u001b[2;97;40m    \u001b[0m\u001b[2;91;40mGame/collision_static\u001b[0m\u001b[2;97;40m:\u001b[0m\u001b[2;40m                                              \u001b[0m\n",
      "\u001b[2m    \u001b[0m\u001b[2m    \u001b[0m\u001b[2;97;40m      \u001b[0m\u001b[2;91;40m_target_\u001b[0m\u001b[2;97;40m:\u001b[0m\u001b[2;97;40m \u001b[0m\u001b[2;40mtorchmetrics.MeanMetric                                 \u001b[0m\n",
      "\u001b[2m    \u001b[0m\u001b[2m    \u001b[0m\u001b[2;97;40m      \u001b[0m\u001b[2;91;40msync_on_compute\u001b[0m\u001b[2;97;40m:\u001b[0m\u001b[2;97;40m \u001b[0m\u001b[2;40mfalse                                            \u001b[0m\n",
      "\u001b[2m    \u001b[0m\u001b[2m    \u001b[0m\u001b[2;91;40mlogger\u001b[0m\u001b[2;97;40m:\u001b[0m\u001b[2;40m                                                                 \u001b[0m\n",
      "\u001b[2m    \u001b[0m\u001b[2m    \u001b[0m\u001b[2;97;40m  \u001b[0m\u001b[2;91;40m_target_\u001b[0m\u001b[2;97;40m:\u001b[0m\u001b[2;97;40m \u001b[0m\u001b[2;40mlightning.fabric.loggers.TensorBoardLogger                  \u001b[0m\n",
      "\u001b[2m    \u001b[0m\u001b[2m    \u001b[0m\u001b[2;97;40m  \u001b[0m\u001b[2;91;40mname\u001b[0m\u001b[2;97;40m:\u001b[0m\u001b[2;97;40m \u001b[0m\u001b[2;40m2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5          \u001b[0m\n",
      "\u001b[2m    \u001b[0m\u001b[2m    \u001b[0m\u001b[2;97;40m  \u001b[0m\u001b[2;91;40mroot_dir\u001b[0m\u001b[2;97;40m:\u001b[0m\u001b[2;97;40m \u001b[0m\u001b[2;40mlogs/runs/dreamer_v3/isaac_sim_environment                  \u001b[0m\n",
      "\u001b[2m    \u001b[0m\u001b[2m    \u001b[0m\u001b[2;97;40m  \u001b[0m\u001b[2;91;40mversion\u001b[0m\u001b[2;97;40m:\u001b[0m\u001b[2;97;40m \u001b[0m\u001b[2;40mnull                                                         \u001b[0m\n",
      "\u001b[2m    \u001b[0m\u001b[2m    \u001b[0m\u001b[2;97;40m  \u001b[0m\u001b[2;91;40mdefault_hp_metric\u001b[0m\u001b[2;97;40m:\u001b[0m\u001b[2;97;40m \u001b[0m\u001b[2;40mtrue                                               \u001b[0m\n",
      "\u001b[2m    \u001b[0m\u001b[2m    \u001b[0m\u001b[2;97;40m  \u001b[0m\u001b[2;91;40mprefix\u001b[0m\u001b[2;97;40m:\u001b[0m\u001b[2;97;40m \u001b[0m\u001b[2;93;40m'\u001b[0m\u001b[2;93;40m'\u001b[0m\u001b[2;40m                                                            \u001b[0m\n",
      "\u001b[2m    \u001b[0m\u001b[2m    \u001b[0m\u001b[2;97;40m  \u001b[0m\u001b[2;91;40msub_dir\u001b[0m\u001b[2;97;40m:\u001b[0m\u001b[2;97;40m \u001b[0m\u001b[2;40mnull                                                         \u001b[0m\n",
      "\u001b[2m    \u001b[0m\u001b[2m    \u001b[0m\u001b[2;40m                                                                        \u001b[0m\n",
      "[2024-08-23 09:18:05,582][pytorch_lightning.utilities.rank_zero][INFO] - Using bfloat16 Automatic Mixed Precision (AMP)\n",
      "INFO: Seed set to 5\n",
      "[2024-08-23 09:18:05,595][lightning.fabric.utilities.seed][INFO] - Seed set to 5\n",
      "Log dir: logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "\u001b[1m\u001b[93m2024-08-23 08:18:08 [12,821ms] [Warning] [rtx.neuraylib.plugin] [MDLC:COMPILER]   1.0   MDLC   comp warn : file:/home/jianheng/omniverse/assets/Collected_MobileShopDesk_A01_Blue_01/Materials/Base/Metals/Metal_Rough_A.mdl?watch=000000002b034300(16,7): C109 'physical_name' has not been declared; did you mean 'display_name'?\n",
      "\u001b[0m\u001b[1m\u001b[93m2024-08-23 08:18:08 [12,822ms] [Warning] [rtx.neuraylib.plugin] [MDLC:COMPILER]   1.0   MDLC   comp warn : file:/home/jianheng/omniverse/assets/Collected_MobileShopDesk_A01_Blue_01/Materials/Base/Metals/Metal_Rough_A.mdl?watch=000000002b034300(16,7): C188 annotation '::anno::physical_name' will be ignored\n",
      "\u001b[0m\u001b[1m\u001b[93m2024-08-23 08:18:08 [12,823ms] [Warning] [rtx.neuraylib.plugin] [MDLC:COMPILER]   1.0   MDLC   comp warn : file:/home/jianheng/omniverse/assets/Collected_MobileShopDesk_A01_Blue_01/Materials/Base/Metals/Metal_Rough_A.mdl?watch=000000002b034300(16,7): C109 'physical_name' has not been declared; did you mean 'display_name'?\n",
      "\u001b[0m\u001b[1m\u001b[93m2024-08-23 08:18:08 [12,823ms] [Warning] [rtx.neuraylib.plugin] [MDLC:COMPILER]   1.0   MDLC   comp warn : file:/home/jianheng/omniverse/assets/Collected_MobileShopDesk_A01_Blue_01/Materials/Base/Metals/Metal_Rough_A.mdl?watch=000000002b034300(16,7): C188 annotation '::anno::physical_name' will be ignored\n",
      "\u001b[0m\u001b[1m\u001b[93m2024-08-23 08:18:08 [12,824ms] [Warning] [rtx.neuraylib.plugin] [MDLC:COMPILER]   1.0   MDLC   comp warn : file:/home/jianheng/omniverse/assets/Collected_MobileShopDesk_A01_Blue_01/Materials/Base/Metals/Metal_Rough_A.mdl?watch=000000002b034300(16,7): C109 'physical_name' has not been declared; did you mean 'display_name'?\n",
      "\u001b[0m\u001b[1m\u001b[93m2024-08-23 08:18:08 [12,824ms] [Warning] [rtx.neuraylib.plugin] [MDLC:COMPILER]   1.0   MDLC   comp warn : file:/home/jianheng/omniverse/assets/Collected_MobileShopDesk_A01_Blue_01/Materials/Base/Metals/Metal_Rough_A.mdl?watch=000000002b034300(16,7): C188 annotation '::anno::physical_name' will be ignored\n",
      "\u001b[0m\u001b[1m\u001b[93m2024-08-23 08:18:08 [12,824ms] [Warning] [rtx.neuraylib.plugin] [MDLC:COMPILER]   1.0   MDLC   comp warn : file:/home/jianheng/omniverse/assets/Collected_MobileShopDesk_A01_Blue_01/Materials/Base/Metals/Metal_Rough_A.mdl?watch=000000002b034300(16,7): C109 'physical_name' has not been declared; did you mean 'display_name'?\n",
      "\u001b[0m\u001b[1m\u001b[93m2024-08-23 08:18:08 [12,824ms] [Warning] [rtx.neuraylib.plugin] [MDLC:COMPILER]   1.0   MDLC   comp warn : file:/home/jianheng/omniverse/assets/Collected_MobileShopDesk_A01_Blue_01/Materials/Base/Metals/Metal_Rough_A.mdl?watch=000000002b034300(16,7): C188 annotation '::anno::physical_name' will be ignored\n",
      "\u001b[0m\u001b[1m\u001b[93m2024-08-23 08:18:08 [12,858ms] [Warning] [rtx.neuraylib.plugin] [MDLC:COMPILER]   1.0   MDLC   comp warn : file:/home/jianheng/omniverse/assets/Collected_MobileShopDesk_A01_Blue_01/Materials/Base/Metals/Metal_Painted_White_Glossy_A.mdl?watch=00007f95ca25ab10(16,7): C109 'physical_name' has not been declared; did you mean 'display_name'?\n",
      "\u001b[0m\u001b[1m\u001b[93m2024-08-23 08:18:08 [12,858ms] [Warning] [rtx.neuraylib.plugin] [MDLC:COMPILER]   1.0   MDLC   comp warn : file:/home/jianheng/omniverse/assets/Collected_MobileShopDesk_A01_Blue_01/Materials/Base/Metals/Metal_Painted_White_Glossy_A.mdl?watch=00007f95ca25ab10(16,7): C188 annotation '::anno::physical_name' will be ignored\n",
      "\u001b[0m\u001b[1m\u001b[93m2024-08-23 08:18:08 [12,859ms] [Warning] [rtx.neuraylib.plugin] [MDLC:COMPILER]   1.0   MDLC   comp warn : file:/home/jianheng/omniverse/assets/Collected_MobileShopDesk_A01_Blue_01/Materials/Base/Metals/Metal_Painted_White_Glossy_A.mdl?watch=00007f95ca25ab10(16,7): C109 'physical_name' has not been declared; did you mean 'display_name'?\n",
      "\u001b[0m\u001b[1m\u001b[93m2024-08-23 08:18:08 [12,859ms] [Warning] [rtx.neuraylib.plugin] [MDLC:COMPILER]   1.0   MDLC   comp warn : file:/home/jianheng/omniverse/assets/Collected_MobileShopDesk_A01_Blue_01/Materials/Base/Metals/Metal_Painted_White_Glossy_A.mdl?watch=00007f95ca25ab10(16,7): C188 annotation '::anno::physical_name' will be ignored\n",
      "\u001b[0m\u001b[1m\u001b[93m2024-08-23 08:18:08 [12,860ms] [Warning] [rtx.neuraylib.plugin] [MDLC:COMPILER]   1.0   MDLC   comp warn : file:/home/jianheng/omniverse/assets/Collected_MobileShopDesk_A01_Blue_01/Materials/Base/Metals/Metal_Painted_White_Glossy_A.mdl?watch=00007f95ca25ab10(16,7): C109 'physical_name' has not been declared; did you mean 'display_name'?\n",
      "\u001b[0m\u001b[1m\u001b[93m2024-08-23 08:18:08 [12,860ms] [Warning] [rtx.neuraylib.plugin] [MDLC:COMPILER]   1.0   MDLC   comp warn : file:/home/jianheng/omniverse/assets/Collected_MobileShopDesk_A01_Blue_01/Materials/Base/Metals/Metal_Painted_White_Glossy_A.mdl?watch=00007f95ca25ab10(16,7): C188 annotation '::anno::physical_name' will be ignored\n",
      "\u001b[0m\u001b[1m\u001b[93m2024-08-23 08:18:08 [12,860ms] [Warning] [rtx.neuraylib.plugin] [MDLC:COMPILER]   1.0   MDLC   comp warn : file:/home/jianheng/omniverse/assets/Collected_MobileShopDesk_A01_Blue_01/Materials/Base/Metals/Metal_Painted_White_Glossy_A.mdl?watch=00007f95ca25ab10(16,7): C109 'physical_name' has not been declared; did you mean 'display_name'?\n",
      "\u001b[0m\u001b[1m\u001b[93m2024-08-23 08:18:08 [12,860ms] [Warning] [rtx.neuraylib.plugin] [MDLC:COMPILER]   1.0   MDLC   comp warn : file:/home/jianheng/omniverse/assets/Collected_MobileShopDesk_A01_Blue_01/Materials/Base/Metals/Metal_Painted_White_Glossy_A.mdl?watch=00007f95ca25ab10(16,7): C188 annotation '::anno::physical_name' will be ignored\n",
      "\u001b[0m\u001b[1m\u001b[93m2024-08-23 08:18:08 [13,200ms] [Warning] [rtx.neuraylib.plugin] [MDLC:COMPILER]   1.0   MDLC   comp warn : file:/home/jianheng/omniverse/assets/Collected_MobileShopDesk_A01_Blue_01/Materials/Base/Metals/Metal_Glossy_A.mdl?watch=00007f95c9e961c0(16,7): C109 'physical_name' has not been declared; did you mean 'display_name'?\n",
      "\u001b[0m\u001b[1m\u001b[93m2024-08-23 08:18:08 [13,200ms] [Warning] [rtx.neuraylib.plugin] [MDLC:COMPILER]   1.0   MDLC   comp warn : file:/home/jianheng/omniverse/assets/Collected_MobileShopDesk_A01_Blue_01/Materials/Base/Metals/Metal_Glossy_A.mdl?watch=00007f95c9e961c0(16,7): C188 annotation '::anno::physical_name' will be ignored\n",
      "\u001b[0m\u001b[1m\u001b[93m2024-08-23 08:18:08 [13,201ms] [Warning] [rtx.neuraylib.plugin] [MDLC:COMPILER]   1.0   MDLC   comp warn : file:/home/jianheng/omniverse/assets/Collected_MobileShopDesk_A01_Blue_01/Materials/Base/Metals/Metal_Glossy_A.mdl?watch=00007f95c9e961c0(16,7): C109 'physical_name' has not been declared; did you mean 'display_name'?\n",
      "\u001b[0m\u001b[1m\u001b[93m2024-08-23 08:18:08 [13,201ms] [Warning] [rtx.neuraylib.plugin] [MDLC:COMPILER]   1.0   MDLC   comp warn : file:/home/jianheng/omniverse/assets/Collected_MobileShopDesk_A01_Blue_01/Materials/Base/Metals/Metal_Glossy_A.mdl?watch=00007f95c9e961c0(16,7): C188 annotation '::anno::physical_name' will be ignored\n",
      "\u001b[0m\u001b[1m\u001b[93m2024-08-23 08:18:08 [13,202ms] [Warning] [rtx.neuraylib.plugin] [MDLC:COMPILER]   1.0   MDLC   comp warn : file:/home/jianheng/omniverse/assets/Collected_MobileShopDesk_A01_Blue_01/Materials/Base/Metals/Metal_Glossy_A.mdl?watch=00007f95c9e961c0(16,7): C109 'physical_name' has not been declared; did you mean 'display_name'?\n",
      "\u001b[0m\u001b[1m\u001b[93m2024-08-23 08:18:08 [13,202ms] [Warning] [rtx.neuraylib.plugin] [MDLC:COMPILER]   1.0   MDLC   comp warn : file:/home/jianheng/omniverse/assets/Collected_MobileShopDesk_A01_Blue_01/Materials/Base/Metals/Metal_Glossy_A.mdl?watch=00007f95c9e961c0(16,7): C188 annotation '::anno::physical_name' will be ignored\n",
      "\u001b[0m\u001b[1m\u001b[93m2024-08-23 08:18:08 [13,202ms] [Warning] [rtx.neuraylib.plugin] [MDLC:COMPILER]   1.0   MDLC   comp warn : file:/home/jianheng/omniverse/assets/Collected_MobileShopDesk_A01_Blue_01/Materials/Base/Metals/Metal_Glossy_A.mdl?watch=00007f95c9e961c0(16,7): C109 'physical_name' has not been declared; did you mean 'display_name'?\n",
      "\u001b[0m\u001b[1m\u001b[93m2024-08-23 08:18:08 [13,202ms] [Warning] [rtx.neuraylib.plugin] [MDLC:COMPILER]   1.0   MDLC   comp warn : file:/home/jianheng/omniverse/assets/Collected_MobileShopDesk_A01_Blue_01/Materials/Base/Metals/Metal_Glossy_A.mdl?watch=00007f95c9e961c0(16,7): C188 annotation '::anno::physical_name' will be ignored\n",
      "\u001b[0m\u001b[1m\u001b[93m2024-08-23 08:18:08 [13,225ms] [Warning] [rtx.neuraylib.plugin] [MDLC:COMPILER]   1.0   MDLC   comp warn : file:/home/jianheng/omniverse/assets/Collected_IndustrialSteelShelving_A24_PR_NVD_01/Materials/Base/Metals/Metal_Painted_White_Rough_A.mdl?watch=00007f9615cefbd0(16,7): C109 'physical_name' has not been declared; did you mean 'display_name'?\n",
      "\u001b[0m\u001b[1m\u001b[93m2024-08-23 08:18:08 [13,225ms] [Warning] [rtx.neuraylib.plugin] [MDLC:COMPILER]   1.0   MDLC   comp warn : file:/home/jianheng/omniverse/assets/Collected_IndustrialSteelShelving_A24_PR_NVD_01/Materials/Base/Metals/Metal_Painted_White_Rough_A.mdl?watch=00007f9615cefbd0(16,7): C188 annotation '::anno::physical_name' will be ignored\n",
      "\u001b[0m\u001b[1m\u001b[93m2024-08-23 08:18:08 [13,227ms] [Warning] [rtx.neuraylib.plugin] [MDLC:COMPILER]   1.0   MDLC   comp warn : file:/home/jianheng/omniverse/assets/Collected_IndustrialSteelShelving_A24_PR_NVD_01/Materials/Base/Metals/Metal_Painted_White_Rough_A.mdl?watch=00007f9615cefbd0(16,7): C109 'physical_name' has not been declared; did you mean 'display_name'?\n",
      "\u001b[0m\u001b[1m\u001b[93m2024-08-23 08:18:08 [13,227ms] [Warning] [rtx.neuraylib.plugin] [MDLC:COMPILER]   1.0   MDLC   comp warn : file:/home/jianheng/omniverse/assets/Collected_IndustrialSteelShelving_A24_PR_NVD_01/Materials/Base/Metals/Metal_Painted_White_Rough_A.mdl?watch=00007f9615cefbd0(16,7): C188 annotation '::anno::physical_name' will be ignored\n",
      "\u001b[0m\u001b[1m\u001b[93m2024-08-23 08:18:08 [13,227ms] [Warning] [rtx.neuraylib.plugin] [MDLC:COMPILER]   1.0   MDLC   comp warn : file:/home/jianheng/omniverse/assets/Collected_IndustrialSteelShelving_A24_PR_NVD_01/Materials/Base/Metals/Metal_Painted_White_Rough_A.mdl?watch=00007f9615cefbd0(16,7): C109 'physical_name' has not been declared; did you mean 'display_name'?\n",
      "\u001b[0m\u001b[1m\u001b[93m2024-08-23 08:18:08 [13,227ms] [Warning] [rtx.neuraylib.plugin] [MDLC:COMPILER]   1.0   MDLC   comp warn : file:/home/jianheng/omniverse/assets/Collected_IndustrialSteelShelving_A24_PR_NVD_01/Materials/Base/Metals/Metal_Painted_White_Rough_A.mdl?watch=00007f9615cefbd0(16,7): C188 annotation '::anno::physical_name' will be ignored\n",
      "\u001b[0m\u001b[1m\u001b[93m2024-08-23 08:18:08 [13,227ms] [Warning] [rtx.neuraylib.plugin] [MDLC:COMPILER]   1.0   MDLC   comp warn : file:/home/jianheng/omniverse/assets/Collected_IndustrialSteelShelving_A24_PR_NVD_01/Materials/Base/Metals/Metal_Painted_White_Rough_A.mdl?watch=00007f9615cefbd0(16,7): C109 'physical_name' has not been declared; did you mean 'display_name'?\n",
      "\u001b[0m\u001b[1m\u001b[93m2024-08-23 08:18:08 [13,227ms] [Warning] [rtx.neuraylib.plugin] [MDLC:COMPILER]   1.0   MDLC   comp warn : file:/home/jianheng/omniverse/assets/Collected_IndustrialSteelShelving_A24_PR_NVD_01/Materials/Base/Metals/Metal_Painted_White_Rough_A.mdl?watch=00007f9615cefbd0(16,7): C188 annotation '::anno::physical_name' will be ignored\n",
      "\u001b[0m\u001b[1m\u001b[93m2024-08-23 08:18:08 [13,291ms] [Warning] [rtx.neuraylib.plugin] [MDLC:COMPILER]   1.0   MDLC   comp warn : file:/home/jianheng/omniverse/assets/Collected_DeluxePackingTable_A01_01/Materials/Base/Metals/Metal_Rough_A.mdl?watch=00007f95defe2630(16,7): C109 'physical_name' has not been declared; did you mean 'display_name'?\n",
      "\u001b[0m\u001b[1m\u001b[93m2024-08-23 08:18:08 [13,291ms] [Warning] [rtx.neuraylib.plugin] [MDLC:COMPILER]   1.0   MDLC   comp warn : file:/home/jianheng/omniverse/assets/Collected_DeluxePackingTable_A01_01/Materials/Base/Metals/Metal_Rough_A.mdl?watch=00007f95defe2630(16,7): C188 annotation '::anno::physical_name' will be ignored\n",
      "\u001b[0m\u001b[1m\u001b[93m2024-08-23 08:18:08 [13,292ms] [Warning] [rtx.neuraylib.plugin] [MDLC:COMPILER]   1.0   MDLC   comp warn : file:/home/jianheng/omniverse/assets/Collected_DeluxePackingTable_A01_01/Materials/Base/Metals/Metal_Rough_A.mdl?watch=00007f95defe2630(16,7): C109 'physical_name' has not been declared; did you mean 'display_name'?\n",
      "\u001b[0m\u001b[1m\u001b[93m2024-08-23 08:18:08 [13,292ms] [Warning] [rtx.neuraylib.plugin] [MDLC:COMPILER]   1.0   MDLC   comp warn : file:/home/jianheng/omniverse/assets/Collected_DeluxePackingTable_A01_01/Materials/Base/Metals/Metal_Rough_A.mdl?watch=00007f95defe2630(16,7): C188 annotation '::anno::physical_name' will be ignored\n",
      "\u001b[0m\u001b[1m\u001b[93m2024-08-23 08:18:08 [13,293ms] [Warning] [rtx.neuraylib.plugin] [MDLC:COMPILER]   1.0   MDLC   comp warn : file:/home/jianheng/omniverse/assets/Collected_DeluxePackingTable_A01_01/Materials/Base/Metals/Metal_Rough_A.mdl?watch=00007f95defe2630(16,7): C109 'physical_name' has not been declared; did you mean 'display_name'?\n",
      "\u001b[0m\u001b[1m\u001b[93m2024-08-23 08:18:08 [13,293ms] [Warning] [rtx.neuraylib.plugin] [MDLC:COMPILER]   1.0   MDLC   comp warn : file:/home/jianheng/omniverse/assets/Collected_DeluxePackingTable_A01_01/Materials/Base/Metals/Metal_Rough_A.mdl?watch=00007f95defe2630(16,7): C188 annotation '::anno::physical_name' will be ignored\n",
      "\u001b[0m\u001b[1m\u001b[93m2024-08-23 08:18:08 [13,293ms] [Warning] [rtx.neuraylib.plugin] [MDLC:COMPILER]   1.0   MDLC   comp warn : file:/home/jianheng/omniverse/assets/Collected_DeluxePackingTable_A01_01/Materials/Base/Metals/Metal_Rough_A.mdl?watch=00007f95defe2630(16,7): C109 'physical_name' has not been declared; did you mean 'display_name'?\n",
      "\u001b[0m\u001b[1m\u001b[93m2024-08-23 08:18:08 [13,293ms] [Warning] [rtx.neuraylib.plugin] [MDLC:COMPILER]   1.0   MDLC   comp warn : file:/home/jianheng/omniverse/assets/Collected_DeluxePackingTable_A01_01/Materials/Base/Metals/Metal_Rough_A.mdl?watch=00007f95defe2630(16,7): C188 annotation '::anno::physical_name' will be ignored\n",
      "\u001b[0m\u001b[1m\u001b[93m2024-08-23 08:18:09 [13,325ms] [Warning] [rtx.neuraylib.plugin] [MDLC:COMPILER]   1.0   MDLC   comp warn : file:/home/jianheng/omniverse/assets/Collected_DeluxePackingTable_A01_01/Materials/Base/Metals/Steel_A.mdl?watch=00007f95cdac8c10(16,7): C109 'physical_name' has not been declared; did you mean 'display_name'?\n",
      "\u001b[0m\u001b[1m\u001b[93m2024-08-23 08:18:09 [13,325ms] [Warning] [rtx.neuraylib.plugin] [MDLC:COMPILER]   1.0   MDLC   comp warn : file:/home/jianheng/omniverse/assets/Collected_DeluxePackingTable_A01_01/Materials/Base/Metals/Steel_A.mdl?watch=00007f95cdac8c10(16,7): C188 annotation '::anno::physical_name' will be ignored\n",
      "\u001b[0m\u001b[1m\u001b[93m2024-08-23 08:18:09 [13,326ms] [Warning] [rtx.neuraylib.plugin] [MDLC:COMPILER]   1.0   MDLC   comp warn : file:/home/jianheng/omniverse/assets/Collected_DeluxePackingTable_A01_01/Materials/Base/Metals/Steel_A.mdl?watch=00007f95cdac8c10(16,7): C109 'physical_name' has not been declared; did you mean 'display_name'?\n",
      "\u001b[0m\u001b[1m\u001b[93m2024-08-23 08:18:09 [13,326ms] [Warning] [rtx.neuraylib.plugin] [MDLC:COMPILER]   1.0   MDLC   comp warn : file:/home/jianheng/omniverse/assets/Collected_DeluxePackingTable_A01_01/Materials/Base/Metals/Steel_A.mdl?watch=00007f95cdac8c10(16,7): C188 annotation '::anno::physical_name' will be ignored\n",
      "\u001b[0m\u001b[1m\u001b[93m2024-08-23 08:18:09 [13,327ms] [Warning] [rtx.neuraylib.plugin] [MDLC:COMPILER]   1.0   MDLC   comp warn : file:/home/jianheng/omniverse/assets/Collected_DeluxePackingTable_A01_01/Materials/Base/Metals/Steel_A.mdl?watch=00007f95cdac8c10(16,7): C109 'physical_name' has not been declared; did you mean 'display_name'?\n",
      "\u001b[0m\u001b[1m\u001b[93m2024-08-23 08:18:09 [13,327ms] [Warning] [rtx.neuraylib.plugin] [MDLC:COMPILER]   1.0   MDLC   comp warn : file:/home/jianheng/omniverse/assets/Collected_DeluxePackingTable_A01_01/Materials/Base/Metals/Steel_A.mdl?watch=00007f95cdac8c10(16,7): C188 annotation '::anno::physical_name' will be ignored\n",
      "\u001b[0m\u001b[1m\u001b[93m2024-08-23 08:18:09 [13,327ms] [Warning] [rtx.neuraylib.plugin] [MDLC:COMPILER]   1.0   MDLC   comp warn : file:/home/jianheng/omniverse/assets/Collected_DeluxePackingTable_A01_01/Materials/Base/Metals/Steel_A.mdl?watch=00007f95cdac8c10(16,7): C109 'physical_name' has not been declared; did you mean 'display_name'?\n",
      "\u001b[0m\u001b[1m\u001b[93m2024-08-23 08:18:09 [13,327ms] [Warning] [rtx.neuraylib.plugin] [MDLC:COMPILER]   1.0   MDLC   comp warn : file:/home/jianheng/omniverse/assets/Collected_DeluxePackingTable_A01_01/Materials/Base/Metals/Steel_A.mdl?watch=00007f95cdac8c10(16,7): C188 annotation '::anno::physical_name' will be ignored\n",
      "\u001b[0m\u001b[1m\u001b[93m2024-08-23 08:18:09 [13,346ms] [Warning] [rtx.neuraylib.plugin] [MDLC:COMPILER]   1.0   MDLC   comp warn : file:/home/jianheng/omniverse/assets/Collected_DeluxePackingTable_A01_01/Materials/Base/Metals/Chrome_A.mdl?watch=000000002b0c5710(15,7): C109 'physical_name' has not been declared; did you mean 'display_name'?\n",
      "\u001b[0m\u001b[1m\u001b[93m2024-08-23 08:18:09 [13,346ms] [Warning] [rtx.neuraylib.plugin] [MDLC:COMPILER]   1.0   MDLC   comp warn : file:/home/jianheng/omniverse/assets/Collected_DeluxePackingTable_A01_01/Materials/Base/Metals/Chrome_A.mdl?watch=000000002b0c5710(15,7): C188 annotation '::anno::physical_name' will be ignored\n",
      "\u001b[0m\u001b[1m\u001b[93m2024-08-23 08:18:09 [13,347ms] [Warning] [rtx.neuraylib.plugin] [MDLC:COMPILER]   1.0   MDLC   comp warn : file:/home/jianheng/omniverse/assets/Collected_DeluxePackingTable_A01_01/Materials/Base/Metals/Chrome_A.mdl?watch=000000002b0c5710(15,7): C109 'physical_name' has not been declared; did you mean 'display_name'?\n",
      "\u001b[0m\u001b[1m\u001b[93m2024-08-23 08:18:09 [13,348ms] [Warning] [rtx.neuraylib.plugin] [MDLC:COMPILER]   1.0   MDLC   comp warn : file:/home/jianheng/omniverse/assets/Collected_DeluxePackingTable_A01_01/Materials/Base/Metals/Chrome_A.mdl?watch=000000002b0c5710(15,7): C188 annotation '::anno::physical_name' will be ignored\n",
      "\u001b[0m\u001b[1m\u001b[93m2024-08-23 08:18:09 [13,348ms] [Warning] [rtx.neuraylib.plugin] [MDLC:COMPILER]   1.0   MDLC   comp warn : file:/home/jianheng/omniverse/assets/Collected_DeluxePackingTable_A01_01/Materials/Base/Metals/Chrome_A.mdl?watch=000000002b0c5710(15,7): C109 'physical_name' has not been declared; did you mean 'display_name'?\n",
      "\u001b[0m\u001b[1m\u001b[93m2024-08-23 08:18:09 [13,348ms] [Warning] [rtx.neuraylib.plugin] [MDLC:COMPILER]   1.0   MDLC   comp warn : file:/home/jianheng/omniverse/assets/Collected_DeluxePackingTable_A01_01/Materials/Base/Metals/Chrome_A.mdl?watch=000000002b0c5710(15,7): C188 annotation '::anno::physical_name' will be ignored\n",
      "\u001b[0m\u001b[1m\u001b[93m2024-08-23 08:18:09 [13,348ms] [Warning] [rtx.neuraylib.plugin] [MDLC:COMPILER]   1.0   MDLC   comp warn : file:/home/jianheng/omniverse/assets/Collected_DeluxePackingTable_A01_01/Materials/Base/Metals/Chrome_A.mdl?watch=000000002b0c5710(15,7): C109 'physical_name' has not been declared; did you mean 'display_name'?\n",
      "\u001b[0m\u001b[1m\u001b[93m2024-08-23 08:18:09 [13,348ms] [Warning] [rtx.neuraylib.plugin] [MDLC:COMPILER]   1.0   MDLC   comp warn : file:/home/jianheng/omniverse/assets/Collected_DeluxePackingTable_A01_01/Materials/Base/Metals/Chrome_A.mdl?watch=000000002b0c5710(15,7): C188 annotation '::anno::physical_name' will be ignored\n",
      "\u001b[0m\u001b[1m\u001b[93m2024-08-23 08:18:09 [13,392ms] [Warning] [rtx.neuraylib.plugin] [MDLC:COMPILER]   1.0   MDLC   comp warn : file:/home/jianheng/omniverse/assets/Collected_DeluxePackingTable_A01_01/Materials/Base/Metals/Metal_Painted_White_Glossy_A.mdl?watch=00007f95f4327560(16,7): C109 'physical_name' has not been declared; did you mean 'display_name'?\n",
      "\u001b[0m\u001b[1m\u001b[93m2024-08-23 08:18:09 [13,392ms] [Warning] [rtx.neuraylib.plugin] [MDLC:COMPILER]   1.0   MDLC   comp warn : file:/home/jianheng/omniverse/assets/Collected_DeluxePackingTable_A01_01/Materials/Base/Metals/Metal_Painted_White_Glossy_A.mdl?watch=00007f95f4327560(16,7): C188 annotation '::anno::physical_name' will be ignored\n",
      "\u001b[0m\u001b[1m\u001b[93m2024-08-23 08:18:09 [13,393ms] [Warning] [rtx.neuraylib.plugin] [MDLC:COMPILER]   1.0   MDLC   comp warn : file:/home/jianheng/omniverse/assets/Collected_DeluxePackingTable_A01_01/Materials/Base/Metals/Metal_Painted_White_Glossy_A.mdl?watch=00007f95f4327560(16,7): C109 'physical_name' has not been declared; did you mean 'display_name'?\n",
      "\u001b[0m\u001b[1m\u001b[93m2024-08-23 08:18:09 [13,393ms] [Warning] [rtx.neuraylib.plugin] [MDLC:COMPILER]   1.0   MDLC   comp warn : file:/home/jianheng/omniverse/assets/Collected_DeluxePackingTable_A01_01/Materials/Base/Metals/Metal_Painted_White_Glossy_A.mdl?watch=00007f95f4327560(16,7): C188 annotation '::anno::physical_name' will be ignored\n",
      "\u001b[0m\u001b[1m\u001b[93m2024-08-23 08:18:09 [13,393ms] [Warning] [rtx.neuraylib.plugin] [MDLC:COMPILER]   1.0   MDLC   comp warn : file:/home/jianheng/omniverse/assets/Collected_DeluxePackingTable_A01_01/Materials/Base/Metals/Metal_Painted_White_Glossy_A.mdl?watch=00007f95f4327560(16,7): C109 'physical_name' has not been declared; did you mean 'display_name'?\n",
      "\u001b[0m\u001b[1m\u001b[93m2024-08-23 08:18:09 [13,393ms] [Warning] [rtx.neuraylib.plugin] [MDLC:COMPILER]   1.0   MDLC   comp warn : file:/home/jianheng/omniverse/assets/Collected_DeluxePackingTable_A01_01/Materials/Base/Metals/Metal_Painted_White_Glossy_A.mdl?watch=00007f95f4327560(16,7): C188 annotation '::anno::physical_name' will be ignored\n",
      "\u001b[0m\u001b[1m\u001b[93m2024-08-23 08:18:09 [13,393ms] [Warning] [rtx.neuraylib.plugin] [MDLC:COMPILER]   1.0   MDLC   comp warn : file:/home/jianheng/omniverse/assets/Collected_DeluxePackingTable_A01_01/Materials/Base/Metals/Metal_Painted_White_Glossy_A.mdl?watch=00007f95f4327560(16,7): C109 'physical_name' has not been declared; did you mean 'display_name'?\n",
      "\u001b[0m\u001b[1m\u001b[93m2024-08-23 08:18:09 [13,393ms] [Warning] [rtx.neuraylib.plugin] [MDLC:COMPILER]   1.0   MDLC   comp warn : file:/home/jianheng/omniverse/assets/Collected_DeluxePackingTable_A01_01/Materials/Base/Metals/Metal_Painted_White_Glossy_A.mdl?watch=00007f95f4327560(16,7): C188 annotation '::anno::physical_name' will be ignored\n",
      "\u001b[0m\u001b[1m\u001b[93m2024-08-23 08:18:09 [13,401ms] [Warning] [rtx.neuraylib.plugin] [MDLC:COMPILER]   1.0   MDLC   comp warn : file:/home/jianheng/omniverse/assets/Collected_DeluxePackingTable_A01_01/Materials/Base/Metals/Metal_Glossy_A.mdl?watch=00007f963ae87770(16,7): C109 'physical_name' has not been declared; did you mean 'display_name'?\n",
      "\u001b[0m\u001b[1m\u001b[93m2024-08-23 08:18:09 [13,401ms] [Warning] [rtx.neuraylib.plugin] [MDLC:COMPILER]   1.0   MDLC   comp warn : file:/home/jianheng/omniverse/assets/Collected_DeluxePackingTable_A01_01/Materials/Base/Metals/Metal_Glossy_A.mdl?watch=00007f963ae87770(16,7): C188 annotation '::anno::physical_name' will be ignored\n",
      "\u001b[0m\u001b[1m\u001b[93m2024-08-23 08:18:09 [13,402ms] [Warning] [rtx.neuraylib.plugin] [MDLC:COMPILER]   1.0   MDLC   comp warn : file:/home/jianheng/omniverse/assets/Collected_DeluxePackingTable_A01_01/Materials/Base/Metals/Metal_Glossy_A.mdl?watch=00007f963ae87770(16,7): C109 'physical_name' has not been declared; did you mean 'display_name'?\n",
      "\u001b[0m\u001b[1m\u001b[93m2024-08-23 08:18:09 [13,402ms] [Warning] [rtx.neuraylib.plugin] [MDLC:COMPILER]   1.0   MDLC   comp warn : file:/home/jianheng/omniverse/assets/Collected_DeluxePackingTable_A01_01/Materials/Base/Metals/Metal_Glossy_A.mdl?watch=00007f963ae87770(16,7): C188 annotation '::anno::physical_name' will be ignored\n",
      "\u001b[0m\u001b[1m\u001b[93m2024-08-23 08:18:09 [13,402ms] [Warning] [rtx.neuraylib.plugin] [MDLC:COMPILER]   1.0   MDLC   comp warn : file:/home/jianheng/omniverse/assets/Collected_DeluxePackingTable_A01_01/Materials/Base/Metals/Metal_Glossy_A.mdl?watch=00007f963ae87770(16,7): C109 'physical_name' has not been declared; did you mean 'display_name'?\n",
      "\u001b[0m\u001b[1m\u001b[93m2024-08-23 08:18:09 [13,402ms] [Warning] [rtx.neuraylib.plugin] [MDLC:COMPILER]   1.0   MDLC   comp warn : file:/home/jianheng/omniverse/assets/Collected_DeluxePackingTable_A01_01/Materials/Base/Metals/Metal_Glossy_A.mdl?watch=00007f963ae87770(16,7): C188 annotation '::anno::physical_name' will be ignored\n",
      "\u001b[0m\u001b[1m\u001b[93m2024-08-23 08:18:09 [13,402ms] [Warning] [rtx.neuraylib.plugin] [MDLC:COMPILER]   1.0   MDLC   comp warn : file:/home/jianheng/omniverse/assets/Collected_DeluxePackingTable_A01_01/Materials/Base/Metals/Metal_Glossy_A.mdl?watch=00007f963ae87770(16,7): C109 'physical_name' has not been declared; did you mean 'display_name'?\n",
      "\u001b[0m\u001b[1m\u001b[93m2024-08-23 08:18:09 [13,402ms] [Warning] [rtx.neuraylib.plugin] [MDLC:COMPILER]   1.0   MDLC   comp warn : file:/home/jianheng/omniverse/assets/Collected_DeluxePackingTable_A01_01/Materials/Base/Metals/Metal_Glossy_A.mdl?watch=00007f963ae87770(16,7): C188 annotation '::anno::physical_name' will be ignored\n",
      "\u001b[0m\u001b[1m\u001b[93m2024-08-23 08:18:10 [14,649ms] [Warning] [rtx.neuraylib.plugin] [MDLC:COMPILER]   1.0   MDLC   comp warn : file:/home/jianheng/omniverse/assets/Collected_full_warehouse_worker_benchmark_sdg/Environments/Simple_Warehouse/Materials/MaterialInstanceDynamic_1220.mdl?watch=000000002b3a10e0(36,21): C183 unused parameter 'MultiMap_Box'\n",
      "\u001b[0m\u001b[1m\u001b[93m2024-08-23 08:18:10 [14,650ms] [Warning] [rtx.neuraylib.plugin] [MDLC:COMPILER]   1.0   MDLC   comp warn : file:/home/jianheng/omniverse/assets/Collected_full_warehouse_worker_benchmark_sdg/Environments/Simple_Warehouse/Materials/MaterialInstanceDynamic_1220.mdl?watch=000000002b3a10e0(36,21): C183 unused parameter 'MultiMap_Box'\n",
      "\u001b[0m\u001b[1m\u001b[93m2024-08-23 08:18:10 [14,650ms] [Warning] [rtx.neuraylib.plugin] [MDLC:COMPILER]   1.0   MDLC   comp warn : file:/home/jianheng/omniverse/assets/Collected_full_warehouse_worker_benchmark_sdg/Environments/Simple_Warehouse/Materials/MaterialInstanceDynamic_1220.mdl?watch=000000002b3a10e0(36,21): C183 unused parameter 'MultiMap_Box'\n",
      "\u001b[0m\u001b[1m\u001b[93m2024-08-23 08:18:10 [14,650ms] [Warning] [rtx.neuraylib.plugin] [MDLC:COMPILER]   1.0   MDLC   comp warn : file:/home/jianheng/omniverse/assets/Collected_full_warehouse_worker_benchmark_sdg/Environments/Simple_Warehouse/Materials/MaterialInstanceDynamic_1220.mdl?watch=000000002b3a10e0(36,21): C183 unused parameter 'MultiMap_Box'\n",
      "\u001b[0m\u001b[1m\u001b[93m2024-08-23 08:18:34 [38,464ms] [Warning] [omni.physx.plugin] PhysX warning: Cooking::cookConvexMesh: GPU-compatible convex hull could not be built because of oblong shape. Will fall back to CPU collision, particles and deformables will not collide with this mesh!, FILE /builds/omniverse/physics/physx/source/geomutils/src/cooking/GuCookingConvexMesh.cpp, LINE 124\n",
      "\u001b[0m\u001b[1m\u001b[93m2024-08-23 08:18:34 [38,464ms] [Warning] [omni.physx.plugin] PhysX warning: Cooking::cookConvexMesh: GPU-compatible convex hull could not be built because of oblong shape. Will fall back to CPU collision, particles and deformables will not collide with this mesh!, FILE /builds/omniverse/physics/physx/source/geomutils/src/cooking/GuCookingConvexMesh.cpp, LINE 124\n",
      "\u001b[0m\u001b[1m\u001b[93m2024-08-23 08:18:34 [38,464ms] [Warning] [omni.physx.plugin] PhysX warning: Cooking::cookConvexMesh: GPU-compatible convex hull could not be built because of oblong shape. Will fall back to CPU collision, particles and deformables will not collide with this mesh!, FILE /builds/omniverse/physics/physx/source/geomutils/src/cooking/GuCookingConvexMesh.cpp, LINE 124\n",
      "\u001b[0m\u001b[1m\u001b[93m2024-08-23 08:18:34 [38,464ms] [Warning] [omni.physx.plugin] PhysX warning: Cooking::cookConvexMesh: GPU-compatible convex hull could not be built because of oblong shape. Will fall back to CPU collision, particles and deformables will not collide with this mesh!, FILE /builds/omniverse/physics/physx/source/geomutils/src/cooking/GuCookingConvexMesh.cpp, LINE 124\n",
      "\u001b[0m\u001b[1m\u001b[93m2024-08-23 08:18:34 [38,465ms] [Warning] [omni.physx.plugin] PhysX warning: Cooking::cookConvexMesh: GPU-compatible convex hull could not be built because of oblong shape. Will fall back to CPU collision, particles and deformables will not collide with this mesh!, FILE /builds/omniverse/physics/physx/source/geomutils/src/cooking/GuCookingConvexMesh.cpp, LINE 124\n",
      "\u001b[0m\u001b[1m\u001b[93m2024-08-23 08:18:34 [38,465ms] [Warning] [omni.physx.plugin] PhysX warning: Cooking::cookConvexMesh: GPU-compatible convex hull could not be built because of oblong shape. Will fall back to CPU collision, particles and deformables will not collide with this mesh!, FILE /builds/omniverse/physics/physx/source/geomutils/src/cooking/GuCookingConvexMesh.cpp, LINE 124\n",
      "\u001b[0m\u001b[1m\u001b[93m2024-08-23 08:18:34 [38,465ms] [Warning] [omni.physx.plugin] PhysX warning: Cooking::cookConvexMesh: GPU-compatible convex hull could not be built because of oblong shape. Will fall back to CPU collision, particles and deformables will not collide with this mesh!, FILE /builds/omniverse/physics/physx/source/geomutils/src/cooking/GuCookingConvexMesh.cpp, LINE 124\n",
      "\u001b[0m\u001b[1m\u001b[93m2024-08-23 08:18:34 [38,465ms] [Warning] [omni.physx.plugin] PhysX warning: Cooking::cookConvexMesh: GPU-compatible convex hull could not be built because of oblong shape. Will fall back to CPU collision, particles and deformables will not collide with this mesh!, FILE /builds/omniverse/physics/physx/source/geomutils/src/cooking/GuCookingConvexMesh.cpp, LINE 124\n",
      "\u001b[0m\u001b[1m\u001b[93m2024-08-23 08:18:34 [38,465ms] [Warning] [omni.physx.plugin] PhysX warning: Cooking::cookConvexMesh: GPU-compatible convex hull could not be built because of oblong shape. Will fall back to CPU collision, particles and deformables will not collide with this mesh!, FILE /builds/omniverse/physics/physx/source/geomutils/src/cooking/GuCookingConvexMesh.cpp, LINE 124\n",
      "\u001b[0m\u001b[1m\u001b[93m2024-08-23 08:18:34 [38,465ms] [Warning] [omni.physx.plugin] PhysX warning: Cooking::cookConvexMesh: GPU-compatible convex hull could not be built because of oblong shape. Will fall back to CPU collision, particles and deformables will not collide with this mesh!, FILE /builds/omniverse/physics/physx/source/geomutils/src/cooking/GuCookingConvexMesh.cpp, LINE 124\n",
      "\u001b[0m\u001b[1m\u001b[93m2024-08-23 08:18:34 [38,466ms] [Warning] [omni.physx.plugin] PhysX warning: Cooking::cookConvexMesh: GPU-compatible convex hull could not be built because of oblong shape. Will fall back to CPU collision, particles and deformables will not collide with this mesh!, FILE /builds/omniverse/physics/physx/source/geomutils/src/cooking/GuCookingConvexMesh.cpp, LINE 124\n",
      "\u001b[0m\u001b[1m\u001b[93m2024-08-23 08:18:34 [38,466ms] [Warning] [omni.physx.plugin] PhysX warning: Cooking::cookConvexMesh: GPU-compatible convex hull could not be built because of oblong shape. Will fall back to CPU collision, particles and deformables will not collide with this mesh!, FILE /builds/omniverse/physics/physx/source/geomutils/src/cooking/GuCookingConvexMesh.cpp, LINE 124\n",
      "\u001b[0m\u001b[1m\u001b[93m2024-08-23 08:18:34 [38,466ms] [Warning] [omni.physx.plugin] PhysX warning: Cooking::cookConvexMesh: GPU-compatible convex hull could not be built because of oblong shape. Will fall back to CPU collision, particles and deformables will not collide with this mesh!, FILE /builds/omniverse/physics/physx/source/geomutils/src/cooking/GuCookingConvexMesh.cpp, LINE 124\n",
      "\u001b[0m\u001b[1m\u001b[93m2024-08-23 08:18:34 [38,466ms] [Warning] [omni.physx.plugin] PhysX warning: Cooking::cookConvexMesh: GPU-compatible convex hull could not be built because of oblong shape. Will fall back to CPU collision, particles and deformables will not collide with this mesh!, FILE /builds/omniverse/physics/physx/source/geomutils/src/cooking/GuCookingConvexMesh.cpp, LINE 124\n",
      "\u001b[0m\u001b[1m\u001b[93m2024-08-23 08:18:34 [38,466ms] [Warning] [omni.physx.plugin] PhysX warning: Cooking::cookConvexMesh: GPU-compatible convex hull could not be built because of oblong shape. Will fall back to CPU collision, particles and deformables will not collide with this mesh!, FILE /builds/omniverse/physics/physx/source/geomutils/src/cooking/GuCookingConvexMesh.cpp, LINE 124\n",
      "\u001b[0m\u001b[1m\u001b[93m2024-08-23 08:18:34 [38,466ms] [Warning] [omni.physx.plugin] PhysX warning: Cooking::cookConvexMesh: GPU-compatible convex hull could not be built because of oblong shape. Will fall back to CPU collision, particles and deformables will not collide with this mesh!, FILE /builds/omniverse/physics/physx/source/geomutils/src/cooking/GuCookingConvexMesh.cpp, LINE 124\n",
      "\u001b[0m\u001b[1m\u001b[93m2024-08-23 08:18:34 [38,467ms] [Warning] [omni.physx.plugin] PhysX warning: Cooking::cookConvexMesh: GPU-compatible convex hull could not be built because of oblong shape. Will fall back to CPU collision, particles and deformables will not collide with this mesh!, FILE /builds/omniverse/physics/physx/source/geomutils/src/cooking/GuCookingConvexMesh.cpp, LINE 124\n",
      "\u001b[0m\u001b[1m\u001b[93m2024-08-23 08:18:34 [38,467ms] [Warning] [omni.physx.plugin] PhysX warning: Cooking::cookConvexMesh: GPU-compatible convex hull could not be built because of oblong shape. Will fall back to CPU collision, particles and deformables will not collide with this mesh!, FILE /builds/omniverse/physics/physx/source/geomutils/src/cooking/GuCookingConvexMesh.cpp, LINE 124\n",
      "\u001b[0m\u001b[1m\u001b[93m2024-08-23 08:18:34 [38,467ms] [Warning] [omni.physx.plugin] PhysX warning: Cooking::cookConvexMesh: GPU-compatible convex hull could not be built because of oblong shape. Will fall back to CPU collision, particles and deformables will not collide with this mesh!, FILE /builds/omniverse/physics/physx/source/geomutils/src/cooking/GuCookingConvexMesh.cpp, LINE 124\n",
      "\u001b[0m\u001b[1m\u001b[93m2024-08-23 08:18:34 [38,467ms] [Warning] [omni.physx.plugin] PhysX warning: Cooking::cookConvexMesh: GPU-compatible convex hull could not be built because of oblong shape. Will fall back to CPU collision, particles and deformables will not collide with this mesh!, FILE /builds/omniverse/physics/physx/source/geomutils/src/cooking/GuCookingConvexMesh.cpp, LINE 124\n",
      "\u001b[0m\u001b[1m\u001b[93m2024-08-23 08:18:34 [38,468ms] [Warning] [omni.physx.plugin] PhysX warning: Cooking::cookConvexMesh: GPU-compatible convex hull could not be built because of oblong shape. Will fall back to CPU collision, particles and deformables will not collide with this mesh!, FILE /builds/omniverse/physics/physx/source/geomutils/src/cooking/GuCookingConvexMesh.cpp, LINE 124\n",
      "\u001b[0m\u001b[1m\u001b[93m2024-08-23 08:18:34 [38,468ms] [Warning] [omni.physx.plugin] PhysX warning: Cooking::cookConvexMesh: GPU-compatible convex hull could not be built because of oblong shape. Will fall back to CPU collision, particles and deformables will not collide with this mesh!, FILE /builds/omniverse/physics/physx/source/geomutils/src/cooking/GuCookingConvexMesh.cpp, LINE 124\n",
      "\u001b[0m\u001b[1m\u001b[93m2024-08-23 08:18:34 [38,468ms] [Warning] [omni.physx.plugin] PhysX warning: Cooking::cookConvexMesh: GPU-compatible convex hull could not be built because of oblong shape. Will fall back to CPU collision, particles and deformables will not collide with this mesh!, FILE /builds/omniverse/physics/physx/source/geomutils/src/cooking/GuCookingConvexMesh.cpp, LINE 124\n",
      "\u001b[0m\u001b[1m\u001b[93m2024-08-23 08:18:34 [38,468ms] [Warning] [omni.physx.plugin] PhysX warning: Cooking::cookConvexMesh: GPU-compatible convex hull could not be built because of oblong shape. Will fall back to CPU collision, particles and deformables will not collide with this mesh!, FILE /builds/omniverse/physics/physx/source/geomutils/src/cooking/GuCookingConvexMesh.cpp, LINE 124\n",
      "\u001b[0m\u001b[1m\u001b[93m2024-08-23 08:18:34 [38,468ms] [Warning] [omni.physx.plugin] PhysX warning: Cooking::cookConvexMesh: GPU-compatible convex hull could not be built because of oblong shape. Will fall back to CPU collision, particles and deformables will not collide with this mesh!, FILE /builds/omniverse/physics/physx/source/geomutils/src/cooking/GuCookingConvexMesh.cpp, LINE 124\n",
      "\u001b[0m\u001b[1m\u001b[93m2024-08-23 08:18:34 [38,468ms] [Warning] [omni.physx.plugin] PhysX warning: Cooking::cookConvexMesh: GPU-compatible convex hull could not be built because of oblong shape. Will fall back to CPU collision, particles and deformables will not collide with this mesh!, FILE /builds/omniverse/physics/physx/source/geomutils/src/cooking/GuCookingConvexMesh.cpp, LINE 124\n",
      "\u001b[0m\u001b[1m\u001b[93m2024-08-23 08:18:34 [38,468ms] [Warning] [omni.physx.plugin] PhysX warning: Cooking::cookConvexMesh: GPU-compatible convex hull could not be built because of oblong shape. Will fall back to CPU collision, particles and deformables will not collide with this mesh!, FILE /builds/omniverse/physics/physx/source/geomutils/src/cooking/GuCookingConvexMesh.cpp, LINE 124\n",
      "\u001b[0m\u001b[1m\u001b[93m2024-08-23 08:18:34 [38,468ms] [Warning] [omni.physx.plugin] PhysX warning: Cooking::cookConvexMesh: GPU-compatible convex hull could not be built because of oblong shape. Will fall back to CPU collision, particles and deformables will not collide with this mesh!, FILE /builds/omniverse/physics/physx/source/geomutils/src/cooking/GuCookingConvexMesh.cpp, LINE 124\n",
      "\u001b[0m\u001b[1m\u001b[93m2024-08-23 08:18:34 [38,469ms] [Warning] [omni.physx.plugin] PhysX warning: Cooking::cookConvexMesh: GPU-compatible convex hull could not be built because of oblong shape. Will fall back to CPU collision, particles and deformables will not collide with this mesh!, FILE /builds/omniverse/physics/physx/source/geomutils/src/cooking/GuCookingConvexMesh.cpp, LINE 124\n",
      "\u001b[0m\u001b[1m\u001b[93m2024-08-23 08:18:34 [38,469ms] [Warning] [omni.physx.plugin] PhysX warning: Cooking::cookConvexMesh: GPU-compatible convex hull could not be built because of oblong shape. Will fall back to CPU collision, particles and deformables will not collide with this mesh!, FILE /builds/omniverse/physics/physx/source/geomutils/src/cooking/GuCookingConvexMesh.cpp, LINE 124\n",
      "\u001b[0m\u001b[1m\u001b[93m2024-08-23 08:18:34 [38,469ms] [Warning] [omni.physx.plugin] PhysX warning: Cooking::cookConvexMesh: GPU-compatible convex hull could not be built because of oblong shape. Will fall back to CPU collision, particles and deformables will not collide with this mesh!, FILE /builds/omniverse/physics/physx/source/geomutils/src/cooking/GuCookingConvexMesh.cpp, LINE 124\n",
      "\u001b[0m\u001b[1m\u001b[93m2024-08-23 08:18:34 [38,469ms] [Warning] [omni.physx.plugin] PhysX warning: Cooking::cookConvexMesh: GPU-compatible convex hull could not be built because of oblong shape. Will fall back to CPU collision, particles and deformables will not collide with this mesh!, FILE /builds/omniverse/physics/physx/source/geomutils/src/cooking/GuCookingConvexMesh.cpp, LINE 124\n",
      "\u001b[0m\u001b[1m\u001b[93m2024-08-23 08:18:34 [38,469ms] [Warning] [omni.physx.plugin] PhysX warning: Cooking::cookConvexMesh: GPU-compatible convex hull could not be built because of oblong shape. Will fall back to CPU collision, particles and deformables will not collide with this mesh!, FILE /builds/omniverse/physics/physx/source/geomutils/src/cooking/GuCookingConvexMesh.cpp, LINE 124\n",
      "\u001b[0m\u001b[1m\u001b[93m2024-08-23 08:18:34 [38,469ms] [Warning] [omni.physx.plugin] PhysX warning: Cooking::cookConvexMesh: GPU-compatible convex hull could not be built because of oblong shape. Will fall back to CPU collision, particles and deformables will not collide with this mesh!, FILE /builds/omniverse/physics/physx/source/geomutils/src/cooking/GuCookingConvexMesh.cpp, LINE 124\n",
      "\u001b[0m\u001b[1m\u001b[93m2024-08-23 08:18:34 [38,469ms] [Warning] [omni.physx.plugin] PhysX warning: Cooking::cookConvexMesh: GPU-compatible convex hull could not be built because of oblong shape. Will fall back to CPU collision, particles and deformables will not collide with this mesh!, FILE /builds/omniverse/physics/physx/source/geomutils/src/cooking/GuCookingConvexMesh.cpp, LINE 124\n",
      "\u001b[0m\u001b[1m\u001b[93m2024-08-23 08:18:34 [38,469ms] [Warning] [omni.physx.plugin] PhysX warning: Cooking::cookConvexMesh: GPU-compatible convex hull could not be built because of oblong shape. Will fall back to CPU collision, particles and deformables will not collide with this mesh!, FILE /builds/omniverse/physics/physx/source/geomutils/src/cooking/GuCookingConvexMesh.cpp, LINE 124\n",
      "\u001b[0mPOIs:  [(3.457227959721856, 28.55171920702834), (-17.50996538432892, 28.7017717030231), (-18.351572359530305, 11.91346049371595), (-24.67431268577434, 6.1455887316484334), (3.340493732802214, 6.6371289050365005), (-2.6796329984608374, 12.831770584318631), (-10.320236846468212, 1.7342467361793465), (-23.14637486804602, 18.716575154962015), (-12.72258949270193, 17.427116784195196), (-7.825032070223778, 9.220468119169944)]\n",
      "Usd.Prim(</World/Worker_1>).GetAttribute('xformOp:translate')\n",
      "\u001b[1m\u001b[91m2024-08-23 08:18:35 [39,725ms] [Error] [omni.isaac.dynamic_control.plugin] Failed to register rigid body at '/World/Worker_1'\n",
      "\u001b[0m\u001b[1m\u001b[91m2024-08-23 08:18:35 [39,866ms] [Error] [omni.isaac.dynamic_control.plugin] Failed to register rigid body at '/World/Worker_1'\n",
      "\u001b[0m\u001b[1m\u001b[91m2024-08-23 08:18:35 [39,866ms] [Error] [omni.isaac.dynamic_control.plugin] DcSetRigidBodyLinearVelocity: Invalid or expired body handle\n",
      "\u001b[0m\u001b[1m\u001b[91m2024-08-23 08:18:35 [39,866ms] [Error] [omni.isaac.dynamic_control.plugin] DcSetRigidBodyAngularVelocity: Invalid or expired body handle\n",
      "\u001b[0mUsd.Prim(</World/Worker_2>).GetAttribute('xformOp:translate')\n",
      "\u001b[1m\u001b[91m2024-08-23 08:18:35 [39,876ms] [Error] [omni.isaac.dynamic_control.plugin] Failed to register rigid body at '/World/Worker_2'\n",
      "\u001b[0m\u001b[1m\u001b[91m2024-08-23 08:18:36 [40,409ms] [Error] [omni.isaac.dynamic_control.plugin] Failed to register rigid body at '/World/Worker_2'\n",
      "\u001b[0m\u001b[1m\u001b[91m2024-08-23 08:18:36 [40,409ms] [Error] [omni.isaac.dynamic_control.plugin] DcSetRigidBodyLinearVelocity: Invalid or expired body handle\n",
      "\u001b[0m\u001b[1m\u001b[91m2024-08-23 08:18:36 [40,409ms] [Error] [omni.isaac.dynamic_control.plugin] DcSetRigidBodyAngularVelocity: Invalid or expired body handle\n",
      "\u001b[0mUsd.Prim(</World/Worker_3>).GetAttribute('xformOp:translate')\n",
      "\u001b[1m\u001b[91m2024-08-23 08:18:36 [40,419ms] [Error] [omni.isaac.dynamic_control.plugin] Failed to register rigid body at '/World/Worker_3'\n",
      "\u001b[0m\u001b[1m\u001b[91m2024-08-23 08:18:36 [40,796ms] [Error] [omni.isaac.dynamic_control.plugin] Failed to register rigid body at '/World/Worker_3'\n",
      "\u001b[0m\u001b[1m\u001b[91m2024-08-23 08:18:36 [40,796ms] [Error] [omni.isaac.dynamic_control.plugin] DcSetRigidBodyLinearVelocity: Invalid or expired body handle\n",
      "\u001b[0m\u001b[1m\u001b[91m2024-08-23 08:18:36 [40,796ms] [Error] [omni.isaac.dynamic_control.plugin] DcSetRigidBodyAngularVelocity: Invalid or expired body handle\n",
      "\u001b[0mUsd.Prim(</World/Worker_4>).GetAttribute('xformOp:translate')\n",
      "\u001b[1m\u001b[91m2024-08-23 08:18:36 [40,805ms] [Error] [omni.isaac.dynamic_control.plugin] Failed to register rigid body at '/World/Worker_4'\n",
      "\u001b[0m\u001b[1m\u001b[91m2024-08-23 08:18:36 [40,948ms] [Error] [omni.isaac.dynamic_control.plugin] Failed to register rigid body at '/World/Worker_4'\n",
      "\u001b[0m\u001b[1m\u001b[91m2024-08-23 08:18:36 [40,948ms] [Error] [omni.isaac.dynamic_control.plugin] DcSetRigidBodyLinearVelocity: Invalid or expired body handle\n",
      "\u001b[0m\u001b[1m\u001b[91m2024-08-23 08:18:36 [40,948ms] [Error] [omni.isaac.dynamic_control.plugin] DcSetRigidBodyAngularVelocity: Invalid or expired body handle\n",
      "\u001b[0m[2024-08-23 09:18:36,702][rospy.topics][INFO] - topicmanager initialized\n",
      "Encoder CNN keys: ['map', 'depth']\n",
      "Encoder MLP keys: ['goal', 'heading', 'orientation', 'distance']\n",
      "Decoder CNN keys: ['map']\n",
      "Decoder MLP keys: ['goal', 'heading', 'orientation', 'distance']\n",
      "/home/jianheng/miniconda3/envs/sheeprl/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.get_global_map to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.get_global_map` for environment variables or `env.get_wrapper_attr('get_global_map')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/jianheng/miniconda3/envs/sheeprl/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.get_env_idx to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.get_env_idx` for environment variables or `env.get_wrapper_attr('get_env_idx')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/jianheng/miniconda3/envs/sheeprl/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.init_camera to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.init_camera` for environment variables or `env.get_wrapper_attr('init_camera')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "Usd.Prim(</World/Camera_1>).GetAttribute('xformOp:translate')\n",
      "\u001b[1m\u001b[91m2024-08-23 08:18:38 [42,328ms] [Error] [omni.isaac.dynamic_control.plugin] Failed to register rigid body at '/World/Camera_1'\n",
      "\u001b[0m[SensorRig]  Loading sensor rig from file at /home/jianheng/SyntheticToolkit/sensors.json.\n",
      "{'CAMERA': {'instances': {'1': {'name': 'camera', 'focal_length': 18.14756, 'focus_distance': 400.0, 'f_stop': 0.0, 'horizontal_aperture': 20.955, 'horizontal_aperture_offset': 0, 'vertical_aperture_offset': 0, 'clipping_range': [0.5, 1000000.0], 'resolution': [720, 720], 'position': [0.0, 0.0, 0.0], 'rotation': [0.0, 0.0, 180.0]}}}}\n",
      "\u001b[1m\u001b[93m2024-08-23 08:18:38 [42,350ms] [Warning] [omni.fabric.plugin] No source has valid data array=0x310d3d20 usdValid=0 cpuValid=0\n",
      "\u001b[0m\u001b[1m\u001b[93m2024-08-23 08:18:38 [42,437ms] [Warning] [omni.fabric.plugin] No source has valid data array=0x30d71fc0 usdValid=0 cpuValid=0\n",
      "\u001b[0mUsd.Prim(</World/Camera_2>).GetAttribute('xformOp:translate')\n",
      "\u001b[1m\u001b[91m2024-08-23 08:18:38 [42,551ms] [Error] [omni.isaac.dynamic_control.plugin] Failed to register rigid body at '/World/Camera_2'\n",
      "\u001b[0m[SensorRig]  Loading sensor rig from file at /home/jianheng/SyntheticToolkit/sensors.json.\n",
      "{'CAMERA': {'instances': {'1': {'name': 'camera', 'focal_length': 18.14756, 'focus_distance': 400.0, 'f_stop': 0.0, 'horizontal_aperture': 20.955, 'horizontal_aperture_offset': 0, 'vertical_aperture_offset': 0, 'clipping_range': [0.5, 1000000.0], 'resolution': [720, 720], 'position': [0.0, 0.0, 0.0], 'rotation': [0.0, 0.0, 180.0]}}}}\n",
      "Usd.Prim(</World/Camera_3>).GetAttribute('xformOp:translate')\n",
      "\u001b[1m\u001b[91m2024-08-23 08:18:38 [42,776ms] [Error] [omni.isaac.dynamic_control.plugin] Failed to register rigid body at '/World/Camera_3'\n",
      "\u001b[0m[SensorRig]  Loading sensor rig from file at /home/jianheng/SyntheticToolkit/sensors.json.\n",
      "{'CAMERA': {'instances': {'1': {'name': 'camera', 'focal_length': 18.14756, 'focus_distance': 400.0, 'f_stop': 0.0, 'horizontal_aperture': 20.955, 'horizontal_aperture_offset': 0, 'vertical_aperture_offset': 0, 'clipping_range': [0.5, 1000000.0], 'resolution': [720, 720], 'position': [0.0, 0.0, 0.0], 'rotation': [0.0, 0.0, 180.0]}}}}\n",
      "Usd.Prim(</World/Camera_4>).GetAttribute('xformOp:translate')\n",
      "\u001b[1m\u001b[91m2024-08-23 08:18:38 [43,059ms] [Error] [omni.isaac.dynamic_control.plugin] Failed to register rigid body at '/World/Camera_4'\n",
      "\u001b[0m[SensorRig]  Loading sensor rig from file at /home/jianheng/SyntheticToolkit/sensors.json.\n",
      "{'CAMERA': {'instances': {'1': {'name': 'camera', 'focal_length': 18.14756, 'focus_distance': 400.0, 'f_stop': 0.0, 'horizontal_aperture': 20.955, 'horizontal_aperture_offset': 0, 'vertical_aperture_offset': 0, 'clipping_range': [0.5, 1000000.0], 'resolution': [720, 720], 'position': [0.0, 0.0, 0.0], 'rotation': [0.0, 0.0, 180.0]}}}}\n",
      "Usd.Prim(</World/Camera_5>).GetAttribute('xformOp:translate')\n",
      "\u001b[1m\u001b[91m2024-08-23 08:18:39 [43,389ms] [Error] [omni.isaac.dynamic_control.plugin] Failed to register rigid body at '/World/Camera_5'\n",
      "\u001b[0m[SensorRig]  Loading sensor rig from file at /home/jianheng/SyntheticToolkit/sensors.json.\n",
      "{'CAMERA': {'instances': {'1': {'name': 'camera', 'focal_length': 18.14756, 'focus_distance': 400.0, 'f_stop': 0.0, 'horizontal_aperture': 20.955, 'horizontal_aperture_offset': 0, 'vertical_aperture_offset': 0, 'clipping_range': [0.5, 1000000.0], 'resolution': [720, 720], 'position': [0.0, 0.0, 0.0], 'rotation': [0.0, 0.0, 180.0]}}}}\n",
      "Usd.Prim(</World/Camera_6>).GetAttribute('xformOp:translate')\n",
      "\u001b[1m\u001b[91m2024-08-23 08:18:39 [43,765ms] [Error] [omni.isaac.dynamic_control.plugin] Failed to register rigid body at '/World/Camera_6'\n",
      "\u001b[0m[SensorRig]  Loading sensor rig from file at /home/jianheng/SyntheticToolkit/sensors.json.\n",
      "{'CAMERA': {'instances': {'1': {'name': 'camera', 'focal_length': 18.14756, 'focus_distance': 400.0, 'f_stop': 0.0, 'horizontal_aperture': 20.955, 'horizontal_aperture_offset': 0, 'vertical_aperture_offset': 0, 'clipping_range': [0.5, 1000000.0], 'resolution': [720, 720], 'position': [0.0, 0.0, 0.0], 'rotation': [0.0, 0.0, 180.0]}}}}\n",
      "Usd.Prim(</World/Camera_7>).GetAttribute('xformOp:translate')\n",
      "\u001b[1m\u001b[91m2024-08-23 08:18:39 [44,224ms] [Error] [omni.isaac.dynamic_control.plugin] Failed to register rigid body at '/World/Camera_7'\n",
      "\u001b[0m[SensorRig]  Loading sensor rig from file at /home/jianheng/SyntheticToolkit/sensors.json.\n",
      "{'CAMERA': {'instances': {'1': {'name': 'camera', 'focal_length': 18.14756, 'focus_distance': 400.0, 'f_stop': 0.0, 'horizontal_aperture': 20.955, 'horizontal_aperture_offset': 0, 'vertical_aperture_offset': 0, 'clipping_range': [0.5, 1000000.0], 'resolution': [720, 720], 'position': [0.0, 0.0, 0.0], 'rotation': [0.0, 0.0, 180.0]}}}}\n",
      "Usd.Prim(</World/Camera_8>).GetAttribute('xformOp:translate')\n",
      "\u001b[1m\u001b[91m2024-08-23 08:18:40 [44,755ms] [Error] [omni.isaac.dynamic_control.plugin] Failed to register rigid body at '/World/Camera_8'\n",
      "\u001b[0m[SensorRig]  Loading sensor rig from file at /home/jianheng/SyntheticToolkit/sensors.json.\n",
      "{'CAMERA': {'instances': {'1': {'name': 'camera', 'focal_length': 18.14756, 'focus_distance': 400.0, 'f_stop': 0.0, 'horizontal_aperture': 20.955, 'horizontal_aperture_offset': 0, 'vertical_aperture_offset': 0, 'clipping_range': [0.5, 1000000.0], 'resolution': [720, 720], 'position': [0.0, 0.0, 0.0], 'rotation': [0.0, 0.0, 180.0]}}}}\n",
      "\u001b[1m\u001b[93m2024-08-23 08:18:40 [44,989ms] [Warning] [carb] Client omni.hydratexture.plugin has acquired [carb::settings::ISettings v1.0] 100 times. Consider accessing this interface with carb::getCachedInterface() (Performance warning)\n",
      "\u001b[0m\u001b[1m\u001b[93m2024-08-23 08:18:41 [45,432ms] [Warning] [carb] Client omni.stageupdate.plugin has acquired [omni::hydra::IOmniHydra v2.0] 100 times. Consider accessing this interface with carb::getCachedInterface() (Performance warning)\n",
      "\u001b[0m\u001b[1m\u001b[91m2024-08-23 08:18:46 [50,960ms] [Error] [omni.physicsschema.plugin] Rigid Body of (/World/Worker_1/obj/Cube) missing xformstack reset when child of rigid body (/World/Worker_1) in hierarchy. Simulation of multiple RigidBodyAPI's in a hierarchy will cause unpredicted results. Please fix the hierarchy or use XformStack reset.\n",
      "\u001b[0m\u001b[1m\u001b[91m2024-08-23 08:18:46 [50,960ms] [Error] [omni.physicsschema.plugin] Rigid Body of (/World/Worker_2/obj/Cube) missing xformstack reset when child of rigid body (/World/Worker_2) in hierarchy. Simulation of multiple RigidBodyAPI's in a hierarchy will cause unpredicted results. Please fix the hierarchy or use XformStack reset.\n",
      "\u001b[0m\u001b[1m\u001b[91m2024-08-23 08:18:46 [50,960ms] [Error] [omni.physicsschema.plugin] Rigid Body of (/World/Worker_3/obj/Cube) missing xformstack reset when child of rigid body (/World/Worker_3) in hierarchy. Simulation of multiple RigidBodyAPI's in a hierarchy will cause unpredicted results. Please fix the hierarchy or use XformStack reset.\n",
      "\u001b[0m\u001b[1m\u001b[91m2024-08-23 08:18:46 [50,961ms] [Error] [omni.physicsschema.plugin] Rigid Body of (/World/Worker_4/obj/Cube) missing xformstack reset when child of rigid body (/World/Worker_4) in hierarchy. Simulation of multiple RigidBodyAPI's in a hierarchy will cause unpredicted results. Please fix the hierarchy or use XformStack reset.\n",
      "\u001b[0m\u001b[1m\u001b[93m2024-08-23 08:18:46 [50,962ms] [Warning] [omni.physx.plugin] PhysX warning: Cooking::cookConvexMesh: GPU-compatible convex hull could not be built because of oblong shape. Will fall back to CPU collision, particles and deformables will not collide with this mesh!, FILE /builds/omniverse/physics/physx/source/geomutils/src/cooking/GuCookingConvexMesh.cpp, LINE 124\n",
      "\u001b[0m\u001b[1m\u001b[93m2024-08-23 08:18:46 [50,962ms] [Warning] [omni.physx.plugin] PhysX warning: Cooking::cookConvexMesh: GPU-compatible convex hull could not be built because of oblong shape. Will fall back to CPU collision, particles and deformables will not collide with this mesh!, FILE /builds/omniverse/physics/physx/source/geomutils/src/cooking/GuCookingConvexMesh.cpp, LINE 124\n",
      "\u001b[0m\u001b[1m\u001b[93m2024-08-23 08:18:46 [50,962ms] [Warning] [omni.physx.plugin] PhysX warning: Cooking::cookConvexMesh: GPU-compatible convex hull could not be built because of oblong shape. Will fall back to CPU collision, particles and deformables will not collide with this mesh!, FILE /builds/omniverse/physics/physx/source/geomutils/src/cooking/GuCookingConvexMesh.cpp, LINE 124\n",
      "\u001b[0m\u001b[1m\u001b[93m2024-08-23 08:18:46 [50,962ms] [Warning] [omni.physx.plugin] PhysX warning: Cooking::cookConvexMesh: GPU-compatible convex hull could not be built because of oblong shape. Will fall back to CPU collision, particles and deformables will not collide with this mesh!, FILE /builds/omniverse/physics/physx/source/geomutils/src/cooking/GuCookingConvexMesh.cpp, LINE 124\n",
      "\u001b[0m\u001b[1m\u001b[93m2024-08-23 08:18:46 [50,962ms] [Warning] [omni.physx.plugin] PhysX warning: Cooking::cookConvexMesh: GPU-compatible convex hull could not be built because of oblong shape. Will fall back to CPU collision, particles and deformables will not collide with this mesh!, FILE /builds/omniverse/physics/physx/source/geomutils/src/cooking/GuCookingConvexMesh.cpp, LINE 124\n",
      "\u001b[0m\u001b[1m\u001b[93m2024-08-23 08:18:46 [50,962ms] [Warning] [omni.physx.plugin] PhysX warning: Cooking::cookConvexMesh: GPU-compatible convex hull could not be built because of oblong shape. Will fall back to CPU collision, particles and deformables will not collide with this mesh!, FILE /builds/omniverse/physics/physx/source/geomutils/src/cooking/GuCookingConvexMesh.cpp, LINE 124\n",
      "\u001b[0m\u001b[1m\u001b[93m2024-08-23 08:18:46 [50,963ms] [Warning] [omni.physx.plugin] PhysX warning: Cooking::cookConvexMesh: GPU-compatible convex hull could not be built because of oblong shape. Will fall back to CPU collision, particles and deformables will not collide with this mesh!, FILE /builds/omniverse/physics/physx/source/geomutils/src/cooking/GuCookingConvexMesh.cpp, LINE 124\n",
      "\u001b[0m\u001b[1m\u001b[93m2024-08-23 08:18:46 [50,963ms] [Warning] [omni.physx.plugin] PhysX warning: Cooking::cookConvexMesh: GPU-compatible convex hull could not be built because of oblong shape. Will fall back to CPU collision, particles and deformables will not collide with this mesh!, FILE /builds/omniverse/physics/physx/source/geomutils/src/cooking/GuCookingConvexMesh.cpp, LINE 124\n",
      "\u001b[0m\u001b[1m\u001b[93m2024-08-23 08:18:46 [50,963ms] [Warning] [omni.physx.plugin] PhysX warning: Cooking::cookConvexMesh: GPU-compatible convex hull could not be built because of oblong shape. Will fall back to CPU collision, particles and deformables will not collide with this mesh!, FILE /builds/omniverse/physics/physx/source/geomutils/src/cooking/GuCookingConvexMesh.cpp, LINE 124\n",
      "\u001b[0m\u001b[1m\u001b[93m2024-08-23 08:18:46 [50,963ms] [Warning] [omni.physx.plugin] PhysX warning: Cooking::cookConvexMesh: GPU-compatible convex hull could not be built because of oblong shape. Will fall back to CPU collision, particles and deformables will not collide with this mesh!, FILE /builds/omniverse/physics/physx/source/geomutils/src/cooking/GuCookingConvexMesh.cpp, LINE 124\n",
      "\u001b[0m\u001b[1m\u001b[93m2024-08-23 08:18:46 [50,963ms] [Warning] [omni.physx.plugin] PhysX warning: Cooking::cookConvexMesh: GPU-compatible convex hull could not be built because of oblong shape. Will fall back to CPU collision, particles and deformables will not collide with this mesh!, FILE /builds/omniverse/physics/physx/source/geomutils/src/cooking/GuCookingConvexMesh.cpp, LINE 124\n",
      "\u001b[0m\u001b[1m\u001b[93m2024-08-23 08:18:46 [50,963ms] [Warning] [omni.physx.plugin] PhysX warning: Cooking::cookConvexMesh: GPU-compatible convex hull could not be built because of oblong shape. Will fall back to CPU collision, particles and deformables will not collide with this mesh!, FILE /builds/omniverse/physics/physx/source/geomutils/src/cooking/GuCookingConvexMesh.cpp, LINE 124\n",
      "\u001b[0m\u001b[1m\u001b[93m2024-08-23 08:18:46 [50,964ms] [Warning] [omni.physx.plugin] PhysX warning: Cooking::cookConvexMesh: GPU-compatible convex hull could not be built because of oblong shape. Will fall back to CPU collision, particles and deformables will not collide with this mesh!, FILE /builds/omniverse/physics/physx/source/geomutils/src/cooking/GuCookingConvexMesh.cpp, LINE 124\n",
      "\u001b[0m\u001b[1m\u001b[93m2024-08-23 08:18:46 [50,964ms] [Warning] [omni.physx.plugin] PhysX warning: Cooking::cookConvexMesh: GPU-compatible convex hull could not be built because of oblong shape. Will fall back to CPU collision, particles and deformables will not collide with this mesh!, FILE /builds/omniverse/physics/physx/source/geomutils/src/cooking/GuCookingConvexMesh.cpp, LINE 124\n",
      "\u001b[0m\u001b[1m\u001b[93m2024-08-23 08:18:46 [50,964ms] [Warning] [omni.physx.plugin] PhysX warning: Cooking::cookConvexMesh: GPU-compatible convex hull could not be built because of oblong shape. Will fall back to CPU collision, particles and deformables will not collide with this mesh!, FILE /builds/omniverse/physics/physx/source/geomutils/src/cooking/GuCookingConvexMesh.cpp, LINE 124\n",
      "\u001b[0m\u001b[1m\u001b[93m2024-08-23 08:18:46 [50,964ms] [Warning] [omni.physx.plugin] PhysX warning: Cooking::cookConvexMesh: GPU-compatible convex hull could not be built because of oblong shape. Will fall back to CPU collision, particles and deformables will not collide with this mesh!, FILE /builds/omniverse/physics/physx/source/geomutils/src/cooking/GuCookingConvexMesh.cpp, LINE 124\n",
      "\u001b[0m\u001b[1m\u001b[93m2024-08-23 08:18:46 [50,964ms] [Warning] [omni.physx.plugin] PhysX warning: Cooking::cookConvexMesh: GPU-compatible convex hull could not be built because of oblong shape. Will fall back to CPU collision, particles and deformables will not collide with this mesh!, FILE /builds/omniverse/physics/physx/source/geomutils/src/cooking/GuCookingConvexMesh.cpp, LINE 124\n",
      "\u001b[0m\u001b[1m\u001b[93m2024-08-23 08:18:46 [50,964ms] [Warning] [omni.physx.plugin] PhysX warning: Cooking::cookConvexMesh: GPU-compatible convex hull could not be built because of oblong shape. Will fall back to CPU collision, particles and deformables will not collide with this mesh!, FILE /builds/omniverse/physics/physx/source/geomutils/src/cooking/GuCookingConvexMesh.cpp, LINE 124\n",
      "\u001b[0m\u001b[1m\u001b[93m2024-08-23 08:18:46 [50,965ms] [Warning] [omni.physx.plugin] PhysX warning: Cooking::cookConvexMesh: GPU-compatible convex hull could not be built because of oblong shape. Will fall back to CPU collision, particles and deformables will not collide with this mesh!, FILE /builds/omniverse/physics/physx/source/geomutils/src/cooking/GuCookingConvexMesh.cpp, LINE 124\n",
      "\u001b[0m\u001b[1m\u001b[93m2024-08-23 08:18:46 [50,965ms] [Warning] [omni.physx.plugin] PhysX warning: Cooking::cookConvexMesh: GPU-compatible convex hull could not be built because of oblong shape. Will fall back to CPU collision, particles and deformables will not collide with this mesh!, FILE /builds/omniverse/physics/physx/source/geomutils/src/cooking/GuCookingConvexMesh.cpp, LINE 124\n",
      "\u001b[0m\u001b[1m\u001b[93m2024-08-23 08:18:46 [50,965ms] [Warning] [omni.physx.plugin] PhysX warning: Cooking::cookConvexMesh: GPU-compatible convex hull could not be built because of oblong shape. Will fall back to CPU collision, particles and deformables will not collide with this mesh!, FILE /builds/omniverse/physics/physx/source/geomutils/src/cooking/GuCookingConvexMesh.cpp, LINE 124\n",
      "\u001b[0m\u001b[1m\u001b[93m2024-08-23 08:18:46 [50,965ms] [Warning] [omni.physx.plugin] PhysX warning: Cooking::cookConvexMesh: GPU-compatible convex hull could not be built because of oblong shape. Will fall back to CPU collision, particles and deformables will not collide with this mesh!, FILE /builds/omniverse/physics/physx/source/geomutils/src/cooking/GuCookingConvexMesh.cpp, LINE 124\n",
      "\u001b[0m\u001b[1m\u001b[93m2024-08-23 08:18:46 [50,965ms] [Warning] [omni.physx.plugin] PhysX warning: Cooking::cookConvexMesh: GPU-compatible convex hull could not be built because of oblong shape. Will fall back to CPU collision, particles and deformables will not collide with this mesh!, FILE /builds/omniverse/physics/physx/source/geomutils/src/cooking/GuCookingConvexMesh.cpp, LINE 124\n",
      "\u001b[0m\u001b[1m\u001b[93m2024-08-23 08:18:46 [50,966ms] [Warning] [omni.physx.plugin] PhysX warning: Cooking::cookConvexMesh: GPU-compatible convex hull could not be built because of oblong shape. Will fall back to CPU collision, particles and deformables will not collide with this mesh!, FILE /builds/omniverse/physics/physx/source/geomutils/src/cooking/GuCookingConvexMesh.cpp, LINE 124\n",
      "\u001b[0m\u001b[1m\u001b[93m2024-08-23 08:18:46 [50,966ms] [Warning] [omni.physx.plugin] PhysX warning: Cooking::cookConvexMesh: GPU-compatible convex hull could not be built because of oblong shape. Will fall back to CPU collision, particles and deformables will not collide with this mesh!, FILE /builds/omniverse/physics/physx/source/geomutils/src/cooking/GuCookingConvexMesh.cpp, LINE 124\n",
      "\u001b[0m\u001b[1m\u001b[93m2024-08-23 08:18:46 [50,966ms] [Warning] [omni.physx.plugin] PhysX warning: Cooking::cookConvexMesh: GPU-compatible convex hull could not be built because of oblong shape. Will fall back to CPU collision, particles and deformables will not collide with this mesh!, FILE /builds/omniverse/physics/physx/source/geomutils/src/cooking/GuCookingConvexMesh.cpp, LINE 124\n",
      "\u001b[0m\u001b[1m\u001b[93m2024-08-23 08:18:46 [50,966ms] [Warning] [omni.physx.plugin] PhysX warning: Cooking::cookConvexMesh: GPU-compatible convex hull could not be built because of oblong shape. Will fall back to CPU collision, particles and deformables will not collide with this mesh!, FILE /builds/omniverse/physics/physx/source/geomutils/src/cooking/GuCookingConvexMesh.cpp, LINE 124\n",
      "\u001b[0m\u001b[1m\u001b[93m2024-08-23 08:18:46 [50,966ms] [Warning] [omni.physx.plugin] PhysX warning: Cooking::cookConvexMesh: GPU-compatible convex hull could not be built because of oblong shape. Will fall back to CPU collision, particles and deformables will not collide with this mesh!, FILE /builds/omniverse/physics/physx/source/geomutils/src/cooking/GuCookingConvexMesh.cpp, LINE 124\n",
      "\u001b[0m\u001b[1m\u001b[93m2024-08-23 08:18:46 [50,966ms] [Warning] [omni.physx.plugin] PhysX warning: Cooking::cookConvexMesh: GPU-compatible convex hull could not be built because of oblong shape. Will fall back to CPU collision, particles and deformables will not collide with this mesh!, FILE /builds/omniverse/physics/physx/source/geomutils/src/cooking/GuCookingConvexMesh.cpp, LINE 124\n",
      "\u001b[0m\u001b[1m\u001b[93m2024-08-23 08:18:46 [50,966ms] [Warning] [omni.physx.plugin] PhysX warning: Cooking::cookConvexMesh: GPU-compatible convex hull could not be built because of oblong shape. Will fall back to CPU collision, particles and deformables will not collide with this mesh!, FILE /builds/omniverse/physics/physx/source/geomutils/src/cooking/GuCookingConvexMesh.cpp, LINE 124\n",
      "\u001b[0m\u001b[1m\u001b[93m2024-08-23 08:18:46 [50,967ms] [Warning] [omni.physx.plugin] PhysX warning: Cooking::cookConvexMesh: GPU-compatible convex hull could not be built because of oblong shape. Will fall back to CPU collision, particles and deformables will not collide with this mesh!, FILE /builds/omniverse/physics/physx/source/geomutils/src/cooking/GuCookingConvexMesh.cpp, LINE 124\n",
      "\u001b[0m\u001b[1m\u001b[93m2024-08-23 08:18:46 [50,967ms] [Warning] [omni.physx.plugin] PhysX warning: Cooking::cookConvexMesh: GPU-compatible convex hull could not be built because of oblong shape. Will fall back to CPU collision, particles and deformables will not collide with this mesh!, FILE /builds/omniverse/physics/physx/source/geomutils/src/cooking/GuCookingConvexMesh.cpp, LINE 124\n",
      "\u001b[0m\u001b[1m\u001b[93m2024-08-23 08:18:46 [50,967ms] [Warning] [omni.physx.plugin] PhysX warning: Cooking::cookConvexMesh: GPU-compatible convex hull could not be built because of oblong shape. Will fall back to CPU collision, particles and deformables will not collide with this mesh!, FILE /builds/omniverse/physics/physx/source/geomutils/src/cooking/GuCookingConvexMesh.cpp, LINE 124\n",
      "\u001b[0m\u001b[1m\u001b[93m2024-08-23 08:18:46 [50,967ms] [Warning] [omni.physx.plugin] PhysX warning: Cooking::cookConvexMesh: GPU-compatible convex hull could not be built because of oblong shape. Will fall back to CPU collision, particles and deformables will not collide with this mesh!, FILE /builds/omniverse/physics/physx/source/geomutils/src/cooking/GuCookingConvexMesh.cpp, LINE 124\n",
      "\u001b[0m\u001b[1m\u001b[93m2024-08-23 08:18:46 [50,967ms] [Warning] [omni.physx.plugin] PhysX warning: Cooking::cookConvexMesh: GPU-compatible convex hull could not be built because of oblong shape. Will fall back to CPU collision, particles and deformables will not collide with this mesh!, FILE /builds/omniverse/physics/physx/source/geomutils/src/cooking/GuCookingConvexMesh.cpp, LINE 124\n",
      "\u001b[0m\u001b[1m\u001b[93m2024-08-23 08:18:46 [50,967ms] [Warning] [omni.physx.plugin] PhysX warning: Cooking::cookConvexMesh: GPU-compatible convex hull could not be built because of oblong shape. Will fall back to CPU collision, particles and deformables will not collide with this mesh!, FILE /builds/omniverse/physics/physx/source/geomutils/src/cooking/GuCookingConvexMesh.cpp, LINE 124\n",
      "\u001b[0m\u001b[1m\u001b[93m2024-08-23 08:18:47 [52,062ms] [Warning] [omni.syntheticdata.plugin] OgnSdPostRenderVarToHost : rendervar copy from texture directly to host buffer is counter-performant. Please use copy from texture to device buffer first.\n",
      "\u001b[0mModule omni.replicator.core.ogn.python._impl.nodes.OgnSemanticSegmentation load on device 'cuda:0' took 1.73 ms\n",
      "Resetting Agent:  1\n",
      "Resetting Agent:  2\n",
      "Resetting Agent:  3\n",
      "Resetting Agent:  4\n",
      "Resetting Agent:  5\n",
      "Resetting Agent:  6\n",
      "Resetting Agent:  7\n",
      "Resetting Agent:  8\n",
      "/home/jianheng/miniconda3/envs/sheeprl/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.compute_obs to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.compute_obs` for environment variables or `env.get_wrapper_attr('compute_obs')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/jianheng/SyntheticToolkit/SyntheticToolkit/core/dynamic_objects.py:117: RuntimeWarning: invalid value encountered in divide\n",
      "  direction_vector = direction_vector / np.linalg.norm(\n",
      "/home/jianheng/miniconda3/envs/sheeprl/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.pre_step to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.pre_step` for environment variables or `env.get_wrapper_attr('pre_step')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/jianheng/miniconda3/envs/sheeprl/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.post_step to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.post_step` for environment variables or `env.get_wrapper_attr('post_step')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "Collide! - agent:  4  - stage:  1\n",
      "Resetting Agent:  4\n",
      "Rank-0: policy_step=200, average_reward_env_0=-68.63890007948098\n",
      "/home/jianheng/sheeprl/sheeprl/algos/dreamer_v3/agent.py:658: UserWarning: Use of index_put_ on expanded tensors is deprecated. Please clone() the tensor before performing this operation. This also applies to advanced indexing e.g. tensor[indices] = tensor (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:713.)\n",
      "  self.recurrent_state[:, reset_envs], stochastic_state = self.rssm.get_initial_states((1, len(reset_envs)))\n",
      "Collide! - agent:  4  - stage:  1\n",
      "Resetting Agent:  4\n",
      "Collide! - agent:  6  - stage:  1\n",
      "Resetting Agent:  6\n",
      "Rank-0: policy_step=360, average_reward_env_0=-92.21484506173366\n",
      "Collide! - agent:  6  - stage:  1\n",
      "Resetting Agent:  6\n",
      "Collide! - agent:  5  - stage:  1\n",
      "Resetting Agent:  5\n",
      "Rank-0: policy_step=400, average_reward_env_0=-105.96348592672067\n",
      "Collide! - agent:  5  - stage:  1\n",
      "Resetting Agent:  5\n",
      "Collide! - agent:  3  - stage:  1\n",
      "Resetting Agent:  3\n",
      "Collide! - agent:  8  - stage:  1\n",
      "Resetting Agent:  8\n",
      "Rank-0: policy_step=480, average_reward_env_0=-109.05248726488504\n",
      "Rank-0: policy_step=480, average_reward_env_1=-169.42414062543781\n",
      "Collide! - agent:  3  - stage:  1\n",
      "Resetting Agent:  3\n",
      "Collide! - agent:  8  - stage:  1\n",
      "Resetting Agent:  8\n",
      "Collide! - agent:  3  - stage:  1\n",
      "Resetting Agent:  3\n",
      "Collision detected between /World/Worker_4 and /World/Worker_3\n",
      "Collision detected between /World/Worker_4 and /World/Worker_3\n",
      "/home/jianheng/miniconda3/envs/sheeprl/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.camera_rig to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.camera_rig` for environment variables or `env.get_wrapper_attr('camera_rig')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "Collision detected between /World/Worker_4 and /World/Worker_3\n",
      "Collision detected between /World/Worker_4 and /World/Worker_3\n",
      "Collide! - agent:  5  - stage:  1\n",
      "Resetting Agent:  5\n",
      "Rank-0: policy_step=632, average_reward_env_0=-98.70893942000536\n",
      "Collide! - agent:  5  - stage:  1\n",
      "Resetting Agent:  5\n",
      "Collide! - agent:  1  - stage:  1\n",
      "Resetting Agent:  1\n",
      "Rank-0: policy_step=656, average_reward_env_0=-159.84340613899082\n",
      "Collide! - agent:  1  - stage:  1\n",
      "Resetting Agent:  1\n",
      "Collide! - agent:  1  - stage:  1\n",
      "Resetting Agent:  1\n",
      "Collision detected between /World/Worker_3 and /World/Warehouse/Forklift_B01_PR_V_NVD_01\n",
      "Collide! - agent:  3  - stage:  1\n",
      "Resetting Agent:  3\n",
      "Rank-0: policy_step=856, average_reward_env_0=-119.91968188679095\n",
      "Collide! - agent:  3  - stage:  1\n",
      "Resetting Agent:  3\n",
      "Collide! - agent:  5  - stage:  1\n",
      "Resetting Agent:  5\n",
      "Rank-0: policy_step=880, average_reward_env_0=-94.07940897403294\n",
      "Collide! - agent:  5  - stage:  1\n",
      "Resetting Agent:  5\n",
      "Collide! - agent:  4  - stage:  1\n",
      "Resetting Agent:  4\n",
      "Rank-0: policy_step=912, average_reward_env_0=-95.8682363236936\n",
      "Collide! - agent:  4  - stage:  1\n",
      "Resetting Agent:  4\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "[2024-08-23 09:19:34,642][py.warnings][WARNING] - /home/jianheng/miniconda3/envs/sheeprl/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.pre_step to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.pre_step` for environment variables or `env.get_wrapper_attr('pre_step')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "\n",
      "[2024-08-23 09:19:34,727][py.warnings][WARNING] - /home/jianheng/miniconda3/envs/sheeprl/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.get_global_map to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.get_global_map` for environment variables or `env.get_wrapper_attr('get_global_map')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "\n",
      "[2024-08-23 09:19:34,728][py.warnings][WARNING] - /home/jianheng/miniconda3/envs/sheeprl/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.post_step to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.post_step` for environment variables or `env.get_wrapper_attr('post_step')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "[2024-08-23 09:19:46,735][py.warnings][WARNING] - /home/jianheng/miniconda3/envs/sheeprl/lib/python3.10/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: The ``compute`` method of metric MeanMetric was called before the ``update`` method which may lead to errors, as metric states have not yet been updated.\n",
      "  warnings.warn(*args, **kwargs)  # noqa: B028\n",
      "\n",
      "Collide! - agent:  6  - stage:  1\n",
      "Resetting Agent:  6\n",
      "Arrive at the goal! - agent:  7  - stage:  1\n",
      "Resetting Agent:  7\n",
      "Rank-0: policy_step=1008, average_reward_env_0=-75.61679477402936\n",
      "[2024-08-23 09:19:47,488][py.warnings][WARNING] - /home/jianheng/sheeprl/sheeprl/algos/dreamer_v3/agent.py:658: UserWarning: Use of index_put_ on expanded tensors is deprecated. Please clone() the tensor before performing this operation. This also applies to advanced indexing e.g. tensor[indices] = tensor (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:713.)\n",
      "  self.recurrent_state[:, reset_envs], stochastic_state = self.rssm.get_initial_states((1, len(reset_envs)))\n",
      "\n",
      "Collide! - agent:  6  - stage:  1\n",
      "Resetting Agent:  6\n",
      "Collide! - agent:  5  - stage:  1\n",
      "Resetting Agent:  5\n",
      "Rank-0: policy_step=1024, average_reward_env_0=-84.66954498524777\n",
      "Collide! - agent:  5  - stage:  1\n",
      "Resetting Agent:  5\n",
      "Collide! - agent:  2  - stage:  1\n",
      "Resetting Agent:  2\n",
      "Rank-0: policy_step=1080, average_reward_env_0=-309.37886524688463\n",
      "Collide! - agent:  2  - stage:  1\n",
      "Resetting Agent:  2\n",
      "Collide! - agent:  4  - stage:  1\n",
      "Resetting Agent:  4\n",
      "Rank-0: policy_step=1168, average_reward_env_0=-107.75341518503429\n",
      "Collide! - agent:  4  - stage:  1\n",
      "Resetting Agent:  4\n",
      "Collide! - agent:  6  - stage:  1\n",
      "Resetting Agent:  6\n",
      "Rank-0: policy_step=1208, average_reward_env_0=-82.23190962491745\n",
      "Collision detected between /World/Worker_2 and /World/Warehouse/Warehouse/SM_Rackshield_2984/SM_Rackshield_02\n",
      "Collide! - agent:  6  - stage:  1\n",
      "Resetting Agent:  6\n",
      "[2024-08-23 09:19:53,867][py.warnings][WARNING] - /home/jianheng/miniconda3/envs/sheeprl/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.camera_rig to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.camera_rig` for environment variables or `env.get_wrapper_attr('camera_rig')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "\n",
      "[2024-08-23 09:19:54,196][py.warnings][WARNING] - /home/jianheng/SyntheticToolkit/SyntheticToolkit/core/dynamic_objects.py:117: RuntimeWarning: invalid value encountered in divide\n",
      "  direction_vector = direction_vector / np.linalg.norm(\n",
      "\n",
      "Collision detected between /World/Worker_2 and /World/Warehouse/Warehouse/SM_Rackshield_2984/SM_Rackshield_02\n",
      "Collide! - agent:  1  - stage:  1\n",
      "Resetting Agent:  1\n",
      "Rank-0: policy_step=1368, average_reward_env_0=-268.31835919116446\n",
      "Collide! - agent:  1  - stage:  1\n",
      "Resetting Agent:  1\n",
      "Collide! - agent:  5  - stage:  1\n",
      "Resetting Agent:  5\n",
      "Rank-0: policy_step=1552, average_reward_env_0=-85.41371758876532\n",
      "Collide! - agent:  5  - stage:  1\n",
      "Resetting Agent:  5\n",
      "Max step number - agent:  7\n",
      "Resetting Agent:  7\n",
      "Rank-0: policy_step=1600, average_reward_env_0=77.23017551648583\n",
      "Collide! - agent:  3  - stage:  1\n",
      "Resetting Agent:  3\n",
      "Rank-0: policy_step=1648, average_reward_env_0=-121.94678803349217\n",
      "Collide! - agent:  3  - stage:  1\n",
      "Resetting Agent:  3\n",
      "Collision detected between /World/Worker_4 and /World/Warehouse/Warehouse/SM_FloorDecal_StripeFull_4m66_1161/SM_FloorDecal_StripeFull_4m\n",
      "Arrive at the goal! - agent:  2  - stage:  1\n",
      "Resetting Agent:  2\n",
      "Collide! - agent:  1  - stage:  1\n",
      "Resetting Agent:  1\n",
      "Rank-0: policy_step=1832, average_reward_env_0=-209.39396847302748\n",
      "Collide! - agent:  1  - stage:  1\n",
      "Resetting Agent:  1\n",
      "Collide! - agent:  3  - stage:  1\n",
      "Resetting Agent:  3\n",
      "Rank-0: policy_step=1880, average_reward_env_0=-117.44353610238551\n",
      "Collide! - agent:  3  - stage:  1\n",
      "Resetting Agent:  3\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collide! - agent:  2  - stage:  1\n",
      "Resetting Agent:  2\n",
      "Rank-0: policy_step=2024, average_reward_env_0=-170.90495542734044\n",
      "Collide! - agent:  2  - stage:  1\n",
      "Resetting Agent:  2\n",
      "Max step number - agent:  8\n",
      "Resetting Agent:  8\n",
      "Rank-0: policy_step=2088, average_reward_env_0=-148.90518039558373\n",
      "Collide! - agent:  8  - stage:  1\n",
      "Resetting Agent:  8\n",
      "Collide! - agent:  4  - stage:  1\n",
      "Resetting Agent:  4\n",
      "Rank-0: policy_step=2136, average_reward_env_0=-146.6099722627606\n",
      "Collide! - agent:  4  - stage:  1\n",
      "Resetting Agent:  4\n",
      "Collide! - agent:  3  - stage:  1\n",
      "Resetting Agent:  3\n",
      "Rank-0: policy_step=2272, average_reward_env_0=-103.33392820962959\n",
      "Collide! - agent:  3  - stage:  1\n",
      "Resetting Agent:  3\n",
      "Arrive at the goal! - agent:  5  - stage:  1\n",
      "Resetting Agent:  5\n",
      "Collision detected between /World/Worker_4 and /World/Warehouse/Table_02\n",
      "Collision detected between /World/Worker_4 and /World/Warehouse/Table_02\n",
      "Collide! - agent:  7  - stage:  1\n",
      "Resetting Agent:  7\n",
      "Rank-0: policy_step=2480, average_reward_env_0=-201.7135468875681\n",
      "Collide! - agent:  3  - stage:  1\n",
      "Resetting Agent:  3\n",
      "Arrive at the goal! - agent:  5  - stage:  1\n",
      "Resetting Agent:  5\n",
      "Collide! - agent:  7  - stage:  1\n",
      "Resetting Agent:  7\n",
      "Rank-0: policy_step=2488, average_reward_env_0=-96.17536255214564\n",
      "Collide! - agent:  1  - stage:  1\n",
      "Resetting Agent:  1\n",
      "Collide! - agent:  3  - stage:  1\n",
      "Resetting Agent:  3\n",
      "Rank-0: policy_step=2496, average_reward_env_0=-173.68854502382953\n",
      "Collide! - agent:  1  - stage:  1\n",
      "Resetting Agent:  1\n",
      "Collide! - agent:  2  - stage:  1\n",
      "Resetting Agent:  2\n",
      "Rank-0: policy_step=2552, average_reward_env_0=-137.81055489119188\n",
      "Arrive at the goal! - agent:  5  - stage:  1\n",
      "Resetting Agent:  5\n",
      "Collide! - agent:  2  - stage:  1\n",
      "Resetting Agent:  2\n",
      "Rank-0: policy_step=2760, average_reward_env_0=-116.68734026095522\n",
      "Collide! - agent:  2  - stage:  1\n",
      "Resetting Agent:  2\n",
      "Collide! - agent:  6  - stage:  1\n",
      "Resetting Agent:  6\n",
      "Rank-0: policy_step=2776, average_reward_env_0=-100.39977990245819\n",
      "Collide! - agent:  6  - stage:  1\n",
      "Resetting Agent:  6\n",
      "Collide! - agent:  7  - stage:  1\n",
      "Resetting Agent:  7\n",
      "Rank-0: policy_step=2800, average_reward_env_0=-159.70217430239245\n",
      "Collision detected between /World/Worker_1 and /World/Warehouse/Table_12\n",
      "Collision detected between /World/Worker_1 and /World/Warehouse/Warehouse/SM_Rackshield_43/SM_Rackshield_02\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collide! - agent:  2  - stage:  1\n",
      "Resetting Agent:  2\n",
      "Rank-0: policy_step=3016, average_reward_env_0=-113.41350718280128\n",
      "Collide! - agent:  2  - stage:  1\n",
      "Resetting Agent:  2\n",
      "Collide! - agent:  8  - stage:  1\n",
      "Resetting Agent:  8\n",
      "Rank-0: policy_step=3064, average_reward_env_0=-143.48410046044827\n",
      "Collide! - agent:  8  - stage:  1\n",
      "Resetting Agent:  8\n",
      "Collide! - agent:  7  - stage:  1\n",
      "Resetting Agent:  7\n",
      "Rank-0: policy_step=3080, average_reward_env_0=-138.44574825331733\n",
      "Collide! - agent:  7  - stage:  1\n",
      "Resetting Agent:  7\n",
      "Max step number - agent:  5\n",
      "Resetting Agent:  5\n",
      "Rank-0: policy_step=3160, average_reward_env_0=-32.22771689692325\n",
      "Collide! - agent:  3  - stage:  1\n",
      "Resetting Agent:  3\n",
      "Rank-0: policy_step=3168, average_reward_env_0=-96.08709172502061\n",
      "Collide! - agent:  3  - stage:  1\n",
      "Resetting Agent:  3\n",
      "Collide! - agent:  7  - stage:  1\n",
      "Resetting Agent:  7\n",
      "Rank-0: policy_step=3184, average_reward_env_0=-121.51757966675265\n",
      "Collide! - agent:  7  - stage:  1\n",
      "Resetting Agent:  7\n",
      "Collide! - agent:  4  - stage:  1\n",
      "Resetting Agent:  4\n",
      "Rank-0: policy_step=3288, average_reward_env_0=-148.06066538517766\n",
      "Collide! - agent:  4  - stage:  1\n",
      "Resetting Agent:  4\n",
      "Collide! - agent:  7  - stage:  1\n",
      "Resetting Agent:  7\n",
      "Rank-0: policy_step=3344, average_reward_env_0=-112.84877230296858\n",
      "Collide! - agent:  7  - stage:  1\n",
      "Resetting Agent:  7\n",
      "Arrive at the goal! - agent:  8  - stage:  1\n",
      "Resetting Agent:  8\n",
      "Collide! - agent:  3  - stage:  1\n",
      "Resetting Agent:  3\n",
      "Rank-0: policy_step=3424, average_reward_env_0=-90.5677378580642\n",
      "Collide! - agent:  3  - stage:  1\n",
      "Resetting Agent:  3\n",
      "Collide! - agent:  4  - stage:  1\n",
      "Resetting Agent:  4\n",
      "Rank-0: policy_step=3528, average_reward_env_0=-135.13707920532303\n",
      "Collide! - agent:  4  - stage:  1\n",
      "Resetting Agent:  4\n",
      "Arrive at the goal! - agent:  5  - stage:  1\n",
      "Resetting Agent:  5\n",
      "Arrive at the goal! - agent:  3  - stage:  1\n",
      "Resetting Agent:  3\n",
      "Arrive at the goal! - agent:  8  - stage:  1\n",
      "Resetting Agent:  8\n",
      "Collide! - agent:  1  - stage:  1\n",
      "Resetting Agent:  1\n",
      "Rank-0: policy_step=3720, average_reward_env_0=-164.9170932176136\n",
      "Collide! - agent:  1  - stage:  1\n",
      "Resetting Agent:  1\n",
      "Collide! - agent:  6  - stage:  1\n",
      "Resetting Agent:  6\n",
      "Rank-0: policy_step=3800, average_reward_env_0=-101.5890184123036\n",
      "Collide! - agent:  6  - stage:  1\n",
      "Resetting Agent:  6\n",
      "Collide! - agent:  4  - stage:  1\n",
      "Resetting Agent:  4\n",
      "Rank-0: policy_step=3824, average_reward_env_0=-123.66935319159958\n",
      "Collide! - agent:  8  - stage:  1\n",
      "Resetting Agent:  8\n",
      "Rank-0: policy_step=3832, average_reward_env_0=-73.08010828872672\n",
      "Collide! - agent:  8  - stage:  1\n",
      "Resetting Agent:  8\n",
      "Collide! - agent:  8  - stage:  1\n",
      "Resetting Agent:  8\n",
      "Collide! - agent:  3  - stage:  1\n",
      "Resetting Agent:  3\n",
      "Rank-0: policy_step=3920, average_reward_env_0=-79.71380440325576\n",
      "Collide! - agent:  3  - stage:  1\n",
      "Resetting Agent:  3\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collision detected between /World/Worker_1 and /World/Warehouse/Warehouse/SM_Rackshield_1074/SM_Rackshield_02\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collide! - agent:  8  - stage:  1\n",
      "Resetting Agent:  8\n",
      "Rank-0: policy_step=4128, average_reward_env_0=-93.03840406478972\n",
      "Collide! - agent:  8  - stage:  1\n",
      "Resetting Agent:  8\n",
      "Arrive at the goal! - agent:  4  - stage:  1\n",
      "Resetting Agent:  4\n",
      "Collide! - agent:  7  - stage:  1\n",
      "Resetting Agent:  7\n",
      "Rank-0: policy_step=4336, average_reward_env_0=-115.11934809821574\n",
      "Collide! - agent:  7  - stage:  1\n",
      "Resetting Agent:  7\n",
      "Collide! - agent:  2  - stage:  1\n",
      "Resetting Agent:  2\n",
      "Rank-0: policy_step=4456, average_reward_env_0=-164.2887066446509\n",
      "Collide! - agent:  2  - stage:  1\n",
      "Resetting Agent:  2\n",
      "Collide! - agent:  5  - stage:  1\n",
      "Resetting Agent:  5\n",
      "Rank-0: policy_step=4472, average_reward_env_0=-26.327097848115255\n",
      "Collide! - agent:  5  - stage:  1\n",
      "Resetting Agent:  5\n",
      "Arrive at the goal! - agent:  1  - stage:  1\n",
      "Resetting Agent:  1\n",
      "Collide! - agent:  2  - stage:  1\n",
      "Resetting Agent:  2\n",
      "Rank-0: policy_step=4600, average_reward_env_0=-152.42747036720212\n",
      "Collide! - agent:  2  - stage:  1\n",
      "Resetting Agent:  2\n",
      "Arrive at the goal! - agent:  1  - stage:  1\n",
      "Resetting Agent:  1\n",
      "Collide! - agent:  3  - stage:  1\n",
      "Resetting Agent:  3\n",
      "Rank-0: policy_step=4696, average_reward_env_0=-102.53984382391404\n",
      "Collide! - agent:  3  - stage:  1\n",
      "Resetting Agent:  3\n",
      "Arrive at the goal! - agent:  1  - stage:  1\n",
      "Resetting Agent:  1\n",
      "Arrive at the goal! - agent:  2  - stage:  1\n",
      "Resetting Agent:  2\n",
      "Collide! - agent:  4  - stage:  1\n",
      "Resetting Agent:  4\n",
      "Rank-0: policy_step=4880, average_reward_env_0=-102.95293264507345\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collision detected between /World/Worker_2 and /World/Worker_1\n",
      "Collision detected between /World/Worker_2 and /World/Worker_1\n",
      "Collision detected between /World/Worker_2 and /World/Worker_1\n",
      "Collision detected between /World/Worker_2 and /World/Worker_1\n",
      "Collide! - agent:  3  - stage:  1\n",
      "Resetting Agent:  3\n",
      "Collide! - agent:  5  - stage:  1\n",
      "Resetting Agent:  5\n",
      "Rank-0: policy_step=5144, average_reward_env_0=-100.35767282532156\n",
      "Rank-0: policy_step=5144, average_reward_env_1=-34.691812044368255\n",
      "Collide! - agent:  3  - stage:  1\n",
      "Resetting Agent:  3\n",
      "Collide! - agent:  5  - stage:  1\n",
      "Resetting Agent:  5\n",
      "Collide! - agent:  6  - stage:  1\n",
      "Resetting Agent:  6\n",
      "Rank-0: policy_step=5168, average_reward_env_0=-111.44423895841975\n",
      "Collide! - agent:  6  - stage:  1\n",
      "Resetting Agent:  6\n",
      "Collide! - agent:  8  - stage:  1\n",
      "Resetting Agent:  8\n",
      "Rank-0: policy_step=5224, average_reward_env_0=-108.85068768218365\n",
      "Collide! - agent:  8  - stage:  1\n",
      "Resetting Agent:  8\n",
      "Collide! - agent:  6  - stage:  1\n",
      "Resetting Agent:  6\n",
      "Rank-0: policy_step=5320, average_reward_env_0=-103.01448223207198\n",
      "Max step number - agent:  1\n",
      "Resetting Agent:  1\n",
      "Collide! - agent:  6  - stage:  1\n",
      "Resetting Agent:  6\n",
      "Rank-0: policy_step=5328, average_reward_env_0=-86.07626688495304\n",
      "Arrive at the goal! - agent:  2  - stage:  1\n",
      "Resetting Agent:  2\n",
      "Collision detected between /World/Worker_1 and /World/Warehouse/Table_02\n",
      "Collision detected between /World/Worker_1 and /World/Warehouse/Table_02\n",
      "Collide! - agent:  5  - stage:  1\n",
      "Resetting Agent:  5\n",
      "Rank-0: policy_step=5544, average_reward_env_0=-58.696836380464966\n",
      "Collision detected between /World/Worker_1 and /World/Warehouse/Warehouse/SM_RackShelf_179/SM_RackShelf_01/SM_RackShelf_01/Section2\n",
      "Arrive at the goal! - agent:  8  - stage:  1\n",
      "Resetting Agent:  8\n",
      "Collide! - agent:  4  - stage:  1\n",
      "Resetting Agent:  4\n",
      "Rank-0: policy_step=5616, average_reward_env_0=-97.68713681698057\n",
      "Collide! - agent:  4  - stage:  1\n",
      "Resetting Agent:  4\n",
      "Collide! - agent:  5  - stage:  1\n",
      "Resetting Agent:  5\n",
      "Rank-0: policy_step=5824, average_reward_env_0=-64.99195449346247\n",
      "Collide! - agent:  5  - stage:  1\n",
      "Resetting Agent:  5\n",
      "Collide! - agent:  1  - stage:  1\n",
      "Resetting Agent:  1\n",
      "Max step number - agent:  7\n",
      "Resetting Agent:  7\n",
      "Rank-0: policy_step=5944, average_reward_env_0=-104.43553149820514\n",
      "Rank-0: policy_step=5944, average_reward_env_1=-110.35493230337373\n",
      "Collide! - agent:  1  - stage:  1\n",
      "Resetting Agent:  1\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collide! - agent:  4  - stage:  1\n",
      "Resetting Agent:  4\n",
      "Rank-0: policy_step=6032, average_reward_env_0=-96.28268503785266\n",
      "Collide! - agent:  4  - stage:  1\n",
      "Resetting Agent:  4\n",
      "Collide! - agent:  5  - stage:  1\n",
      "Resetting Agent:  5\n",
      "Rank-0: policy_step=6096, average_reward_env_0=-65.41272973864221\n",
      "Collide! - agent:  5  - stage:  1\n",
      "Resetting Agent:  5\n",
      "Max step number - agent:  2\n",
      "Resetting Agent:  2\n",
      "Rank-0: policy_step=6208, average_reward_env_0=-120.63621044091262\n",
      "Collide! - agent:  2  - stage:  1\n",
      "Resetting Agent:  2\n",
      "Collide! - agent:  3  - stage:  1\n",
      "Resetting Agent:  3\n",
      "Rank-0: policy_step=6328, average_reward_env_0=-100.1284729377724\n",
      "Collide! - agent:  3  - stage:  1\n",
      "Resetting Agent:  3\n",
      "Collide! - agent:  6  - stage:  1\n",
      "Resetting Agent:  6\n",
      "Rank-0: policy_step=6344, average_reward_env_0=-101.78995777316696\n",
      "Collide! - agent:  6  - stage:  1\n",
      "Resetting Agent:  6\n",
      "Collide! - agent:  5  - stage:  1\n",
      "Resetting Agent:  5\n",
      "Rank-0: policy_step=6416, average_reward_env_0=-68.4564534293721\n",
      "Collide! - agent:  4  - stage:  1\n",
      "Resetting Agent:  4\n",
      "Rank-0: policy_step=6560, average_reward_env_0=-97.47024749312186\n",
      "Collide! - agent:  4  - stage:  1\n",
      "Resetting Agent:  4\n",
      "Collide! - agent:  7  - stage:  1\n",
      "Resetting Agent:  7\n",
      "Rank-0: policy_step=6568, average_reward_env_0=-112.17029072262311\n",
      "Collide! - agent:  7  - stage:  1\n",
      "Resetting Agent:  7\n",
      "Arrive at the goal! - agent:  5  - stage:  1\n",
      "Resetting Agent:  5\n",
      "Arrive at the goal! - agent:  2  - stage:  1\n",
      "Resetting Agent:  2\n",
      "Collide! - agent:  4  - stage:  1\n",
      "Resetting Agent:  4\n",
      "Rank-0: policy_step=6760, average_reward_env_0=-97.09656067453062\n",
      "Collide! - agent:  4  - stage:  1\n",
      "Resetting Agent:  4\n",
      "Max step number - agent:  8\n",
      "Resetting Agent:  8\n",
      "Rank-0: policy_step=6832, average_reward_env_0=-83.94737038426995\n",
      "Collide! - agent:  1  - stage:  1\n",
      "Resetting Agent:  1\n",
      "Rank-0: policy_step=6912, average_reward_env_0=-110.24366305452611\n",
      "Collide! - agent:  1  - stage:  1\n",
      "Resetting Agent:  1\n",
      "Collide! - agent:  2  - stage:  1\n",
      "Resetting Agent:  2\n",
      "Rank-0: policy_step=6960, average_reward_env_0=-102.66072182680801\n",
      "Collide! - agent:  2  - stage:  1\n",
      "Resetting Agent:  2\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collision detected between /World/Worker_2 and /World/Warehouse/Warehouse/SM_Rackshield_29/SM_Rackshield_02\n",
      "Collide! - agent:  5  - stage:  1\n",
      "Resetting Agent:  5\n",
      "Collide! - agent:  8  - stage:  1\n",
      "Resetting Agent:  8\n",
      "Rank-0: policy_step=7160, average_reward_env_0=-61.38730608614503\n",
      "Rank-0: policy_step=7160, average_reward_env_1=-91.37851371599925\n",
      "Collide! - agent:  5  - stage:  1\n",
      "Resetting Agent:  5\n",
      "Collide! - agent:  8  - stage:  1\n",
      "Resetting Agent:  8\n",
      "Collide! - agent:  7  - stage:  1\n",
      "Resetting Agent:  7\n",
      "Rank-0: policy_step=7256, average_reward_env_0=-109.11298334481678\n",
      "Collide! - agent:  7  - stage:  1\n",
      "Resetting Agent:  7\n",
      "Collision detected between /World/Worker_2 and /World/Warehouse/Forklift_B01_PR_V_NVD_01\n",
      "Collision detected between /World/Worker_2 and /World/Warehouse/Forklift_B01_PR_V_NVD_01\n",
      "Collide! - agent:  4  - stage:  1\n",
      "Resetting Agent:  4\n",
      "Rank-0: policy_step=7352, average_reward_env_0=-103.63223533627318\n",
      "Collide! - agent:  4  - stage:  1\n",
      "Resetting Agent:  4\n",
      "Collide! - agent:  3  - stage:  1\n",
      "Resetting Agent:  3\n",
      "Rank-0: policy_step=7432, average_reward_env_0=-107.79565568258657\n",
      "Collide! - agent:  3  - stage:  1\n",
      "Resetting Agent:  3\n",
      "Collide! - agent:  7  - stage:  1\n",
      "Resetting Agent:  7\n",
      "Rank-0: policy_step=7488, average_reward_env_0=-106.17227394818525\n",
      "Collide! - agent:  7  - stage:  1\n",
      "Resetting Agent:  7\n",
      "Collide! - agent:  6  - stage:  1\n",
      "Resetting Agent:  6\n",
      "Rank-0: policy_step=7568, average_reward_env_0=-101.64422055680642\n",
      "Collide! - agent:  6  - stage:  1\n",
      "Resetting Agent:  6\n",
      "Collide! - agent:  1  - stage:  1\n",
      "Resetting Agent:  1\n",
      "Rank-0: policy_step=7600, average_reward_env_0=-112.56550017325733\n",
      "Collide! - agent:  1  - stage:  1\n",
      "Resetting Agent:  1\n",
      "Collide! - agent:  7  - stage:  1\n",
      "Resetting Agent:  7\n",
      "Rank-0: policy_step=7632, average_reward_env_0=-101.63418113411497\n",
      "Collide! - agent:  7  - stage:  1\n",
      "Resetting Agent:  7\n",
      "Collide! - agent:  6  - stage:  1\n",
      "Resetting Agent:  6\n",
      "Rank-0: policy_step=7688, average_reward_env_0=-96.61447876396\n",
      "Collide! - agent:  6  - stage:  1\n",
      "Resetting Agent:  6\n",
      "Collide! - agent:  4  - stage:  1\n",
      "Resetting Agent:  4\n",
      "Collide! - agent:  8  - stage:  1\n",
      "Resetting Agent:  8\n",
      "Rank-0: policy_step=7832, average_reward_env_0=-116.25484081067574\n",
      "Rank-0: policy_step=7832, average_reward_env_1=-88.48954590044055\n",
      "Collide! - agent:  4  - stage:  1\n",
      "Resetting Agent:  4\n",
      "Collide! - agent:  8  - stage:  1\n",
      "Resetting Agent:  8\n",
      "Collide! - agent:  1  - stage:  1\n",
      "Resetting Agent:  1\n",
      "Rank-0: policy_step=7872, average_reward_env_0=-111.42905067488725\n",
      "Collide! - agent:  1  - stage:  1\n",
      "Resetting Agent:  1\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collide! - agent:  1  - stage:  1\n",
      "Resetting Agent:  1\n",
      "Rank-0: policy_step=8024, average_reward_env_0=-107.89623465232286\n",
      "Collide! - agent:  1  - stage:  1\n",
      "Resetting Agent:  1\n",
      "Arrive at the goal! - agent:  3  - stage:  1\n",
      "Resetting Agent:  3\n",
      "Collide! - agent:  2  - stage:  1\n",
      "Resetting Agent:  2\n",
      "Rank-0: policy_step=8184, average_reward_env_0=-101.35543892392101\n",
      "Collide! - agent:  4  - stage:  1\n",
      "Resetting Agent:  4\n",
      "Rank-0: policy_step=8344, average_reward_env_0=-112.69073885647336\n",
      "Collide! - agent:  4  - stage:  1\n",
      "Resetting Agent:  4\n",
      "Collide! - agent:  7  - stage:  1\n",
      "Resetting Agent:  7\n",
      "Rank-0: policy_step=8440, average_reward_env_0=-99.3376317842504\n",
      "Arrive at the goal! - agent:  1  - stage:  1\n",
      "Resetting Agent:  1\n",
      "Collide! - agent:  3  - stage:  1\n",
      "Resetting Agent:  3\n",
      "Rank-0: policy_step=8504, average_reward_env_0=-107.67012551392436\n",
      "Collide! - agent:  3  - stage:  1\n",
      "Resetting Agent:  3\n",
      "Collide! - agent:  2  - stage:  1\n",
      "Resetting Agent:  2\n",
      "Rank-0: policy_step=8536, average_reward_env_0=-103.84824595959945\n",
      "Collide! - agent:  2  - stage:  1\n",
      "Resetting Agent:  2\n",
      "Collide! - agent:  5  - stage:  1\n",
      "Resetting Agent:  5\n",
      "Rank-0: policy_step=8600, average_reward_env_0=-70.99196028238832\n",
      "Collide! - agent:  5  - stage:  1\n",
      "Resetting Agent:  5\n",
      "Collide! - agent:  3  - stage:  1\n",
      "Resetting Agent:  3\n",
      "Rank-0: policy_step=8704, average_reward_env_0=-104.0089260953974\n",
      "Collide! - agent:  3  - stage:  1\n",
      "Resetting Agent:  3\n",
      "Collision detected between /World/Worker_1 and /World/Warehouse/Table_08\n",
      "Collide! - agent:  8  - stage:  1\n",
      "Resetting Agent:  8\n",
      "Rank-0: policy_step=8800, average_reward_env_0=-84.22722961180874\n",
      "Collide! - agent:  8  - stage:  1\n",
      "Resetting Agent:  8\n",
      "Collide! - agent:  2  - stage:  1\n",
      "Resetting Agent:  2\n",
      "Rank-0: policy_step=8896, average_reward_env_0=-104.68230814594507\n",
      "Collide! - agent:  2  - stage:  1\n",
      "Resetting Agent:  2\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collide! - agent:  2  - stage:  1\n",
      "Resetting Agent:  2\n",
      "Rank-0: policy_step=8984, average_reward_env_0=-101.75442802446668\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collide! - agent:  4  - stage:  1\n",
      "Resetting Agent:  4\n",
      "Collide! - agent:  7  - stage:  1\n",
      "Resetting Agent:  7\n",
      "Rank-0: policy_step=9016, average_reward_env_0=-110.64302955816736\n",
      "Rank-0: policy_step=9016, average_reward_env_1=-106.76425621639775\n",
      "Collide! - agent:  4  - stage:  1\n",
      "Resetting Agent:  4\n",
      "Collide! - agent:  7  - stage:  1\n",
      "Resetting Agent:  7\n",
      "Collide! - agent:  3  - stage:  1\n",
      "Resetting Agent:  3\n",
      "Rank-0: policy_step=9104, average_reward_env_0=-104.38872219226621\n",
      "Collide! - agent:  3  - stage:  1\n",
      "Resetting Agent:  3\n",
      "Collide! - agent:  6  - stage:  1\n",
      "Resetting Agent:  6\n",
      "Rank-0: policy_step=9144, average_reward_env_0=-97.9547353989829\n",
      "Collide! - agent:  6  - stage:  1\n",
      "Resetting Agent:  6\n",
      "Collide! - agent:  1  - stage:  1\n",
      "Resetting Agent:  1\n",
      "Rank-0: policy_step=9160, average_reward_env_0=-108.80588919195124\n",
      "Collide! - agent:  1  - stage:  1\n",
      "Resetting Agent:  1\n",
      "Collide! - agent:  6  - stage:  1\n",
      "Resetting Agent:  6\n",
      "Rank-0: policy_step=9192, average_reward_env_0=-95.67370600307065\n",
      "Collide! - agent:  6  - stage:  1\n",
      "Resetting Agent:  6\n",
      "Arrive at the goal! - agent:  4  - stage:  1\n",
      "Resetting Agent:  4\n",
      "Collide! - agent:  8  - stage:  1\n",
      "Resetting Agent:  8\n",
      "Rank-0: policy_step=9400, average_reward_env_0=-82.76749484359438\n",
      "Collide! - agent:  8  - stage:  1\n",
      "Resetting Agent:  8\n",
      "Collide! - agent:  7  - stage:  1\n",
      "Resetting Agent:  7\n",
      "Rank-0: policy_step=9512, average_reward_env_0=-107.51542465214028\n",
      "Collide! - agent:  7  - stage:  1\n",
      "Resetting Agent:  7\n",
      "Collide! - agent:  1  - stage:  1\n",
      "Resetting Agent:  1\n",
      "Rank-0: policy_step=9544, average_reward_env_0=-107.93881195479504\n",
      "Collide! - agent:  1  - stage:  1\n",
      "Resetting Agent:  1\n",
      "Arrive at the goal! - agent:  6  - stage:  1\n",
      "Resetting Agent:  6\n",
      "Collide! - agent:  2  - stage:  1\n",
      "Resetting Agent:  2\n",
      "Collide! - agent:  5  - stage:  1\n",
      "Resetting Agent:  5\n",
      "Rank-0: policy_step=9696, average_reward_env_0=-101.79881404853676\n",
      "Rank-0: policy_step=9696, average_reward_env_1=-75.40267298292747\n",
      "Collide! - agent:  2  - stage:  1\n",
      "Resetting Agent:  2\n",
      "Collide! - agent:  5  - stage:  1\n",
      "Resetting Agent:  5\n",
      "Collide! - agent:  2  - stage:  1\n",
      "Resetting Agent:  2\n",
      "Collide! - agent:  4  - stage:  1\n",
      "Resetting Agent:  4\n",
      "Rank-0: policy_step=9728, average_reward_env_0=-103.6614073279515\n",
      "Collide! - agent:  6  - stage:  1\n",
      "Resetting Agent:  6\n",
      "Rank-0: policy_step=9920, average_reward_env_0=-89.34206053381227\n",
      "Collide! - agent:  6  - stage:  1\n",
      "Resetting Agent:  6\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collide! - agent:  3  - stage:  1\n",
      "Resetting Agent:  3\n",
      "Rank-0: policy_step=9984, average_reward_env_0=-116.38895621246732\n",
      "Collide! - agent:  3  - stage:  1\n",
      "Resetting Agent:  3\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collide! - agent:  7  - stage:  1\n",
      "Resetting Agent:  7\n",
      "Rank-0: policy_step=10016, average_reward_env_0=-110.82893893707217\n",
      "Collide! - agent:  7  - stage:  1\n",
      "Resetting Agent:  7\n",
      "Collision detected between /World/Worker_4 and /World/Warehouse/Rack_02\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Max step number - agent:  8\n",
      "Resetting Agent:  8\n",
      "Rank-0: policy_step=11008, average_reward_env_0=-85.30917547516906\n",
      "Collide! - agent:  8  - stage:  1\n",
      "Resetting Agent:  8\n",
      "Rank-0: policy_step=11040, average_reward_env_0=-82.55910529154418\n",
      "Collide! - agent:  4  - stage:  1\n",
      "Resetting Agent:  4\n",
      "Rank-0: policy_step=11112, average_reward_env_0=-106.40234860543124\n",
      "Max step number - agent:  1\n",
      "Resetting Agent:  1\n",
      "Rank-0: policy_step=11152, average_reward_env_0=-108.02205771938345\n",
      "Max step number - agent:  5\n",
      "Resetting Agent:  5\n",
      "Rank-0: policy_step=11304, average_reward_env_0=-203.36413787575916\n",
      "Max step number - agent:  2\n",
      "Resetting Agent:  2\n",
      "Collide! - agent:  5  - stage:  1\n",
      "Resetting Agent:  5\n",
      "Rank-0: policy_step=11312, average_reward_env_0=-102.19561202283482\n",
      "Collision detected between /World/Worker_4 and /World/Worker_1\n",
      "Collision detected between /World/Worker_4 and /World/Worker_1\n",
      "Collision detected between /World/Worker_4 and /World/Worker_1\n",
      "Collision detected between /World/Worker_4 and /World/Worker_1\n",
      "Collide! - agent:  8  - stage:  1\n",
      "Resetting Agent:  8\n",
      "Rank-0: policy_step=11360, average_reward_env_0=-82.42093790458132\n",
      "Collide! - agent:  8  - stage:  1\n",
      "Resetting Agent:  8\n",
      "Max step number - agent:  6\n",
      "Resetting Agent:  6\n",
      "Rank-0: policy_step=11528, average_reward_env_0=-92.51472839802253\n",
      "Collide! - agent:  5  - stage:  1\n",
      "Resetting Agent:  5\n",
      "Rank-0: policy_step=11576, average_reward_env_0=-195.45847563464008\n",
      "Collide! - agent:  5  - stage:  1\n",
      "Resetting Agent:  5\n",
      "Max step number - agent:  3\n",
      "Resetting Agent:  3\n",
      "Rank-0: policy_step=11592, average_reward_env_0=-117.34056857307256\n",
      "Collide! - agent:  1  - stage:  1\n",
      "Resetting Agent:  1\n",
      "Rank-0: policy_step=11600, average_reward_env_0=-108.82891805161945\n",
      "Collide! - agent:  1  - stage:  1\n",
      "Resetting Agent:  1\n",
      "Collide! - agent:  6  - stage:  1\n",
      "Resetting Agent:  6\n",
      "Collide! - agent:  8  - stage:  1\n",
      "Resetting Agent:  8\n",
      "Rank-0: policy_step=11616, average_reward_env_0=-92.82573995205712\n",
      "Rank-0: policy_step=11616, average_reward_env_1=-82.68585169602956\n",
      "Collide! - agent:  6  - stage:  1\n",
      "Resetting Agent:  6\n",
      "Max step number - agent:  7\n",
      "Resetting Agent:  7\n",
      "Collide! - agent:  8  - stage:  1\n",
      "Resetting Agent:  8\n",
      "Rank-0: policy_step=11624, average_reward_env_0=-111.1250648392431\n",
      "Collide! - agent:  4  - stage:  1\n",
      "Resetting Agent:  4\n",
      "Rank-0: policy_step=11640, average_reward_env_0=-105.4348904357472\n",
      "Collide! - agent:  4  - stage:  1\n",
      "Resetting Agent:  4\n",
      "Collide! - agent:  6  - stage:  1\n",
      "Resetting Agent:  6\n",
      "Rank-0: policy_step=11688, average_reward_env_0=-91.4789069104912\n",
      "Collide! - agent:  6  - stage:  1\n",
      "Resetting Agent:  6\n",
      "Collide! - agent:  7  - stage:  1\n",
      "Resetting Agent:  7\n",
      "Rank-0: policy_step=11704, average_reward_env_0=-109.76381056332424\n",
      "Collide! - agent:  3  - stage:  1\n",
      "Resetting Agent:  3\n",
      "Collide! - agent:  7  - stage:  1\n",
      "Resetting Agent:  7\n",
      "Rank-0: policy_step=11712, average_reward_env_0=-115.72147666398507\n",
      "Collide! - agent:  3  - stage:  1\n",
      "Resetting Agent:  3\n",
      "Collide! - agent:  4  - stage:  1\n",
      "Resetting Agent:  4\n",
      "Rank-0: policy_step=11720, average_reward_env_0=-103.89020519893602\n",
      "Collide! - agent:  4  - stage:  1\n",
      "Resetting Agent:  4\n",
      "Collide! - agent:  7  - stage:  1\n",
      "Resetting Agent:  7\n",
      "Rank-0: policy_step=11792, average_reward_env_0=-106.95828018006542\n",
      "Collide! - agent:  7  - stage:  1\n",
      "Resetting Agent:  7\n",
      "Arrive at the goal! - agent:  4  - stage:  1\n",
      "Resetting Agent:  4\n",
      "Collide! - agent:  2  - stage:  1\n",
      "Resetting Agent:  2\n",
      "Collide! - agent:  8  - stage:  1\n",
      "Resetting Agent:  8\n",
      "Rank-0: policy_step=11816, average_reward_env_0=-99.88929291152088\n",
      "Rank-0: policy_step=11816, average_reward_env_1=-80.56958949799947\n",
      "Collide! - agent:  2  - stage:  1\n",
      "Resetting Agent:  2\n",
      "Collide! - agent:  5  - stage:  1\n",
      "Resetting Agent:  5\n",
      "Collide! - agent:  6  - stage:  1\n",
      "Resetting Agent:  6\n",
      "Rank-0: policy_step=11832, average_reward_env_0=-188.52218327390267\n",
      "Rank-0: policy_step=11832, average_reward_env_1=-89.8712783029975\n",
      "Collide! - agent:  5  - stage:  1\n",
      "Resetting Agent:  5\n",
      "Collide! - agent:  6  - stage:  1\n",
      "Resetting Agent:  6\n",
      "Collide! - agent:  1  - stage:  1\n",
      "Resetting Agent:  1\n",
      "Rank-0: policy_step=11848, average_reward_env_0=-107.39273032914892\n",
      "Collide! - agent:  1  - stage:  1\n",
      "Resetting Agent:  1\n",
      "Collide! - agent:  8  - stage:  1\n",
      "Resetting Agent:  8\n",
      "Rank-0: policy_step=11872, average_reward_env_0=-80.15366955809287\n",
      "Collide! - agent:  8  - stage:  1\n",
      "Resetting Agent:  8\n",
      "Collide! - agent:  4  - stage:  1\n",
      "Resetting Agent:  4\n",
      "Collide! - agent:  7  - stage:  1\n",
      "Resetting Agent:  7\n",
      "Rank-0: policy_step=11904, average_reward_env_0=-96.65296126605882\n",
      "Rank-0: policy_step=11904, average_reward_env_1=-105.04083650930374\n",
      "Collide! - agent:  4  - stage:  1\n",
      "Resetting Agent:  4\n",
      "Collide! - agent:  7  - stage:  1\n",
      "Resetting Agent:  7\n",
      "Collide! - agent:  3  - stage:  1\n",
      "Resetting Agent:  3\n",
      "Arrive at the goal! - agent:  5  - stage:  1\n",
      "Resetting Agent:  5\n",
      "Rank-0: policy_step=11928, average_reward_env_0=-112.5949961403406\n",
      "Collide! - agent:  3  - stage:  1\n",
      "Resetting Agent:  3\n",
      "Collide! - agent:  6  - stage:  1\n",
      "Resetting Agent:  6\n",
      "Rank-0: policy_step=11936, average_reward_env_0=-88.44582652028144\n",
      "Collide! - agent:  6  - stage:  1\n",
      "Resetting Agent:  6\n",
      "Arrive at the goal! - agent:  5  - stage:  1\n",
      "Resetting Agent:  5\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collide! - agent:  6  - stage:  1\n",
      "Resetting Agent:  6\n",
      "Rank-0: policy_step=11976, average_reward_env_0=-86.5954951468391\n",
      "Collide! - agent:  6  - stage:  1\n",
      "Resetting Agent:  6\n",
      "Collide! - agent:  6  - stage:  1\n",
      "Resetting Agent:  6\n",
      "Arrive at the goal! - agent:  7  - stage:  1\n",
      "Resetting Agent:  7\n",
      "Collide! - agent:  8  - stage:  1\n",
      "Resetting Agent:  8\n",
      "Rank-0: policy_step=11992, average_reward_env_1=-82.43682344145152\n",
      "Collide! - agent:  8  - stage:  1\n",
      "Resetting Agent:  8\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collide! - agent:  2  - stage:  1\n",
      "Resetting Agent:  2\n",
      "Rank-0: policy_step=12040, average_reward_env_0=-101.00703351935601\n",
      "Collide! - agent:  2  - stage:  1\n",
      "Resetting Agent:  2\n",
      "Arrive at the goal! - agent:  5  - stage:  1\n",
      "Resetting Agent:  5\n",
      "Arrive at the goal! - agent:  2  - stage:  1\n",
      "Resetting Agent:  2\n",
      "Collide! - agent:  3  - stage:  1\n",
      "Resetting Agent:  3\n",
      "Rank-0: policy_step=12088, average_reward_env_1=-109.96932561311795\n",
      "Collide! - agent:  3  - stage:  1\n",
      "Resetting Agent:  3\n",
      "Collide! - agent:  4  - stage:  1\n",
      "Resetting Agent:  4\n",
      "Collide! - agent:  7  - stage:  1\n",
      "Resetting Agent:  7\n",
      "Rank-0: policy_step=12136, average_reward_env_0=-95.49093132739203\n",
      "Rank-0: policy_step=12136, average_reward_env_1=-96.52486233454795\n",
      "Collide! - agent:  3  - stage:  1\n",
      "Resetting Agent:  3\n",
      "Collide! - agent:  4  - stage:  1\n",
      "Resetting Agent:  4\n",
      "Rank-0: policy_step=12144, average_reward_env_0=-107.62945244726093\n",
      "Collide! - agent:  3  - stage:  1\n",
      "Resetting Agent:  3\n",
      "Collide! - agent:  6  - stage:  1\n",
      "Resetting Agent:  6\n",
      "Collide! - agent:  7  - stage:  1\n",
      "Resetting Agent:  7\n",
      "Collide! - agent:  8  - stage:  1\n",
      "Resetting Agent:  8\n",
      "Rank-0: policy_step=12168, average_reward_env_0=-85.7270465282541\n",
      "Rank-0: policy_step=12168, average_reward_env_1=-94.25938399858356\n",
      "Rank-0: policy_step=12168, average_reward_env_2=-86.22642553237486\n",
      "Collide! - agent:  6  - stage:  1\n",
      "Resetting Agent:  6\n",
      "Collide! - agent:  8  - stage:  1\n",
      "Resetting Agent:  8\n",
      "Arrive at the goal! - agent:  4  - stage:  1\n",
      "Resetting Agent:  4\n",
      "Collide! - agent:  2  - stage:  1\n",
      "Resetting Agent:  2\n",
      "Rank-0: policy_step=12208, average_reward_env_0=-92.01629859030459\n",
      "Collide! - agent:  2  - stage:  1\n",
      "Resetting Agent:  2\n",
      "Collide! - agent:  3  - stage:  1\n",
      "Resetting Agent:  3\n",
      "Rank-0: policy_step=12232, average_reward_env_0=-107.04300216150659\n",
      "Collide! - agent:  3  - stage:  1\n",
      "Resetting Agent:  3\n",
      "Collide! - agent:  8  - stage:  1\n",
      "Resetting Agent:  8\n",
      "Rank-0: policy_step=12264, average_reward_env_0=-84.51415717098595\n",
      "Collide! - agent:  8  - stage:  1\n",
      "Resetting Agent:  8\n",
      "Collide! - agent:  1  - stage:  1\n",
      "Resetting Agent:  1\n",
      "Rank-0: policy_step=12280, average_reward_env_0=-105.29593113548499\n",
      "Collide! - agent:  1  - stage:  1\n",
      "Resetting Agent:  1\n",
      "Collide! - agent:  4  - stage:  1\n",
      "Resetting Agent:  4\n",
      "Rank-0: policy_step=12304, average_reward_env_0=-91.37230381207533\n",
      "Collide! - agent:  4  - stage:  1\n",
      "Resetting Agent:  4\n",
      "Collide! - agent:  2  - stage:  1\n",
      "Resetting Agent:  2\n",
      "Rank-0: policy_step=12328, average_reward_env_0=-90.35160868259553\n",
      "Collide! - agent:  2  - stage:  1\n",
      "Resetting Agent:  2\n",
      "Collide! - agent:  8  - stage:  1\n",
      "Resetting Agent:  8\n",
      "Rank-0: policy_step=12336, average_reward_env_0=-83.02347989731635\n",
      "Collide! - agent:  8  - stage:  1\n",
      "Resetting Agent:  8\n",
      "Collide! - agent:  1  - stage:  1\n",
      "Resetting Agent:  1\n",
      "Rank-0: policy_step=12352, average_reward_env_0=-103.1354437599865\n",
      "Collide! - agent:  1  - stage:  1\n",
      "Resetting Agent:  1\n",
      "Collide! - agent:  1  - stage:  1\n",
      "Resetting Agent:  1\n",
      "Collide! - agent:  6  - stage:  1\n",
      "Resetting Agent:  6\n",
      "Rank-0: policy_step=12416, average_reward_env_0=-101.05492497656543\n",
      "Rank-0: policy_step=12416, average_reward_env_1=-84.97886151333952\n",
      "Collide! - agent:  1  - stage:  1\n",
      "Resetting Agent:  1\n",
      "Collide! - agent:  2  - stage:  1\n",
      "Resetting Agent:  2\n",
      "Collide! - agent:  6  - stage:  1\n",
      "Resetting Agent:  6\n",
      "Rank-0: policy_step=12424, average_reward_env_0=-88.91753329227184\n",
      "Collide! - agent:  2  - stage:  1\n",
      "Resetting Agent:  2\n",
      "Collide! - agent:  5  - stage:  1\n",
      "Resetting Agent:  5\n",
      "Rank-0: policy_step=12464, average_reward_env_0=-163.07717960035606\n",
      "Collide! - agent:  8  - stage:  1\n",
      "Resetting Agent:  8\n",
      "Rank-0: policy_step=12504, average_reward_env_0=-84.77444756497303\n",
      "Collide! - agent:  8  - stage:  1\n",
      "Resetting Agent:  8\n",
      "Collide! - agent:  1  - stage:  1\n",
      "Resetting Agent:  1\n",
      "Rank-0: policy_step=12568, average_reward_env_0=-100.54812362215544\n",
      "Collide! - agent:  1  - stage:  1\n",
      "Resetting Agent:  1\n",
      "Collide! - agent:  4  - stage:  1\n",
      "Resetting Agent:  4\n",
      "Collide! - agent:  6  - stage:  1\n",
      "Resetting Agent:  6\n",
      "Rank-0: policy_step=12584, average_reward_env_0=-90.55428867614017\n",
      "Rank-0: policy_step=12584, average_reward_env_1=-86.16005482134734\n",
      "Collide! - agent:  4  - stage:  1\n",
      "Resetting Agent:  4\n",
      "Collide! - agent:  6  - stage:  1\n",
      "Resetting Agent:  6\n",
      "Collide! - agent:  5  - stage:  1\n",
      "Resetting Agent:  5\n",
      "Rank-0: policy_step=12600, average_reward_env_0=-157.54420481398057\n",
      "Collide! - agent:  2  - stage:  1\n",
      "Resetting Agent:  2\n",
      "Collide! - agent:  5  - stage:  1\n",
      "Resetting Agent:  5\n",
      "Collide! - agent:  8  - stage:  1\n",
      "Resetting Agent:  8\n",
      "Rank-0: policy_step=12608, average_reward_env_0=-88.08984567266432\n",
      "Rank-0: policy_step=12608, average_reward_env_1=-83.77088253274064\n",
      "Collide! - agent:  2  - stage:  1\n",
      "Resetting Agent:  2\n",
      "Collide! - agent:  7  - stage:  1\n",
      "Resetting Agent:  7\n",
      "Collide! - agent:  8  - stage:  1\n",
      "Resetting Agent:  8\n",
      "Rank-0: policy_step=12616, average_reward_env_0=-92.62580542454207\n",
      "Collide! - agent:  7  - stage:  1\n",
      "Resetting Agent:  7\n",
      "Collide! - agent:  6  - stage:  1\n",
      "Resetting Agent:  6\n",
      "Rank-0: policy_step=12632, average_reward_env_0=-84.81956635482248\n",
      "Collide! - agent:  6  - stage:  1\n",
      "Resetting Agent:  6\n",
      "Arrive at the goal! - agent:  1  - stage:  1\n",
      "Resetting Agent:  1\n",
      "Collide! - agent:  4  - stage:  1\n",
      "Resetting Agent:  4\n",
      "Collide! - agent:  7  - stage:  1\n",
      "Resetting Agent:  7\n",
      "Rank-0: policy_step=12664, average_reward_env_1=-90.43851828212189\n",
      "Rank-0: policy_step=12664, average_reward_env_2=-91.07615495230135\n",
      "Collide! - agent:  4  - stage:  1\n",
      "Resetting Agent:  4\n",
      "Collide! - agent:  7  - stage:  1\n",
      "Resetting Agent:  7\n",
      "Collide! - agent:  5  - stage:  1\n",
      "Resetting Agent:  5\n",
      "Collide! - agent:  8  - stage:  1\n",
      "Resetting Agent:  8\n",
      "Rank-0: policy_step=12696, average_reward_env_0=-154.67707174179304\n",
      "Rank-0: policy_step=12696, average_reward_env_1=-83.97276172615008\n",
      "Collide! - agent:  5  - stage:  1\n",
      "Resetting Agent:  5\n",
      "Collide! - agent:  8  - stage:  1\n",
      "Resetting Agent:  8\n",
      "Collide! - agent:  2  - stage:  1\n",
      "Resetting Agent:  2\n",
      "Rank-0: policy_step=12752, average_reward_env_0=-88.23058938699158\n",
      "Collide! - agent:  2  - stage:  1\n",
      "Resetting Agent:  2\n",
      "Collide! - agent:  6  - stage:  1\n",
      "Resetting Agent:  6\n",
      "Rank-0: policy_step=12800, average_reward_env_0=-87.1419154711599\n",
      "Collide! - agent:  6  - stage:  1\n",
      "Resetting Agent:  6\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collide! - agent:  1  - stage:  1\n",
      "Resetting Agent:  1\n",
      "Rank-0: policy_step=13016, average_reward_env_0=-92.6118764453793\n",
      "Collide! - agent:  1  - stage:  1\n",
      "Resetting Agent:  1\n",
      "Collide! - agent:  4  - stage:  1\n",
      "Resetting Agent:  4\n",
      "Rank-0: policy_step=13048, average_reward_env_0=-89.68532807735937\n",
      "Collide! - agent:  4  - stage:  1\n",
      "Resetting Agent:  4\n",
      "Arrive at the goal! - agent:  8  - stage:  1\n",
      "Resetting Agent:  8\n",
      "Arrive at the goal! - agent:  2  - stage:  1\n",
      "Resetting Agent:  2\n",
      "Collide! - agent:  2  - stage:  1\n",
      "Resetting Agent:  2\n",
      "Rank-0: policy_step=13112, average_reward_env_0=-82.43764145511125\n",
      "Collide! - agent:  3  - stage:  1\n",
      "Resetting Agent:  3\n",
      "Arrive at the goal! - agent:  6  - stage:  1\n",
      "Resetting Agent:  6\n",
      "Rank-0: policy_step=13184, average_reward_env_0=-104.75728310872289\n",
      "Collide! - agent:  3  - stage:  1\n",
      "Resetting Agent:  3\n",
      "Arrive at the goal! - agent:  4  - stage:  1\n",
      "Resetting Agent:  4\n",
      "Arrive at the goal! - agent:  5  - stage:  1\n",
      "Resetting Agent:  5\n",
      "Arrive at the goal! - agent:  1  - stage:  1\n",
      "Resetting Agent:  1\n",
      "Arrive at the goal! - agent:  8  - stage:  1\n",
      "Resetting Agent:  8\n",
      "Arrive at the goal! - agent:  5  - stage:  1\n",
      "Resetting Agent:  5\n",
      "Arrive at the goal! - agent:  3  - stage:  1\n",
      "Resetting Agent:  3\n",
      "Collide! - agent:  7  - stage:  1\n",
      "Resetting Agent:  7\n",
      "Rank-0: policy_step=13528, average_reward_env_0=-88.81970385814118\n",
      "Collide! - agent:  7  - stage:  1\n",
      "Resetting Agent:  7\n",
      "Arrive at the goal! - agent:  1  - stage:  1\n",
      "Resetting Agent:  1\n",
      "Collide! - agent:  2  - stage:  1\n",
      "Resetting Agent:  2\n",
      "Collide! - agent:  6  - stage:  1\n",
      "Resetting Agent:  6\n",
      "Rank-0: policy_step=13568, average_reward_env_0=-80.83549568935582\n",
      "Rank-0: policy_step=13568, average_reward_env_1=-81.01725600760378\n",
      "Collide! - agent:  2  - stage:  1\n",
      "Resetting Agent:  2\n",
      "Collide! - agent:  6  - stage:  1\n",
      "Resetting Agent:  6\n",
      "Arrive at the goal! - agent:  2  - stage:  1\n",
      "Resetting Agent:  2\n",
      "Arrive at the goal! - agent:  4  - stage:  1\n",
      "Resetting Agent:  4\n",
      "Collide! - agent:  5  - stage:  1\n",
      "Resetting Agent:  5\n",
      "Arrive at the goal! - agent:  7  - stage:  1\n",
      "Resetting Agent:  7\n",
      "Rank-0: policy_step=13648, average_reward_env_0=-138.99761865852392\n",
      "Collide! - agent:  5  - stage:  1\n",
      "Resetting Agent:  5\n",
      "Collision detected between /World/Worker_3 and /World/Worker_2\n",
      "Collision detected between /World/Worker_3 and /World/Worker_2\n",
      "Collision detected between /World/Worker_3 and /World/Worker_2\n",
      "Collision detected between /World/Worker_3 and /World/Worker_2\n",
      "Arrive at the goal! - agent:  8  - stage:  1\n",
      "Resetting Agent:  8\n",
      "Collide! - agent:  6  - stage:  1\n",
      "Resetting Agent:  6\n",
      "Rank-0: policy_step=13776, average_reward_env_0=-79.52620749969917\n",
      "Collide! - agent:  6  - stage:  1\n",
      "Resetting Agent:  6\n",
      "Arrive at the goal! - agent:  4  - stage:  1\n",
      "Resetting Agent:  4\n",
      "Arrive at the goal! - agent:  1  - stage:  1\n",
      "Resetting Agent:  1\n",
      "Arrive at the goal! - agent:  7  - stage:  1\n",
      "Resetting Agent:  7\n",
      "Arrive at the goal! - agent:  3  - stage:  1\n",
      "Resetting Agent:  3\n",
      "Collide! - agent:  4  - stage:  1\n",
      "Resetting Agent:  4\n",
      "Rank-0: policy_step=13944, average_reward_env_0=-72.79068218975237\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Arrive at the goal! - agent:  4  - stage:  1\n",
      "Resetting Agent:  4\n",
      "Collide! - agent:  8  - stage:  1\n",
      "Resetting Agent:  8\n",
      "Rank-0: policy_step=14000, average_reward_env_0=-68.20217852874312\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collide! - agent:  8  - stage:  1\n",
      "Resetting Agent:  8\n",
      "Arrive at the goal! - agent:  3  - stage:  1\n",
      "Resetting Agent:  3\n",
      "Collide! - agent:  2  - stage:  1\n",
      "Resetting Agent:  2\n",
      "Rank-0: policy_step=14088, average_reward_env_0=-75.07751998549868\n",
      "Collide! - agent:  2  - stage:  1\n",
      "Resetting Agent:  2\n",
      "Arrive at the goal! - agent:  3  - stage:  1\n",
      "Collide! - agent:  3  - stage:  1\n",
      "Resetting Agent:  3\n",
      "Rank-0: policy_step=14104, average_reward_env_0=-84.68393669939113\n",
      "Collide! - agent:  3  - stage:  1\n",
      "Resetting Agent:  3\n",
      "Rank-0: policy_step=14112, average_reward_env_0=-66.38224687745372\n",
      "Collide! - agent:  3  - stage:  1\n",
      "Resetting Agent:  3\n",
      "Collide! - agent:  3  - stage:  1\n",
      "Resetting Agent:  3\n",
      "Arrive at the goal! - agent:  4  - stage:  1\n",
      "Resetting Agent:  4\n",
      "Collide! - agent:  5  - stage:  1\n",
      "Resetting Agent:  5\n",
      "Rank-0: policy_step=14168, average_reward_env_1=-134.32659623701423\n",
      "Collide! - agent:  5  - stage:  1\n",
      "Resetting Agent:  5\n",
      "Collide! - agent:  3  - stage:  1\n",
      "Resetting Agent:  3\n",
      "Arrive at the goal! - agent:  6  - stage:  1\n",
      "Resetting Agent:  6\n",
      "Rank-0: policy_step=14200, average_reward_env_0=-63.443123933225905\n",
      "Collide! - agent:  3  - stage:  1\n",
      "Resetting Agent:  3\n",
      "Collide! - agent:  4  - stage:  1\n",
      "Resetting Agent:  4\n",
      "Rank-0: policy_step=14232, average_reward_env_0=-60.851481955388415\n",
      "Arrive at the goal! - agent:  2  - stage:  1\n",
      "Resetting Agent:  2\n",
      "Collide! - agent:  8  - stage:  1\n",
      "Resetting Agent:  8\n",
      "Rank-0: policy_step=14288, average_reward_env_0=-64.02935472483315\n",
      "Collide! - agent:  8  - stage:  1\n",
      "Resetting Agent:  8\n",
      "Collide! - agent:  4  - stage:  1\n",
      "Resetting Agent:  4\n",
      "Rank-0: policy_step=14312, average_reward_env_0=-52.42492527847019\n",
      "Arrive at the goal! - agent:  7  - stage:  1\n",
      "Resetting Agent:  7\n",
      "Arrive at the goal! - agent:  7  - stage:  1\n",
      "Resetting Agent:  7\n",
      "Collide! - agent:  6  - stage:  1\n",
      "Resetting Agent:  6\n",
      "Rank-0: policy_step=14400, average_reward_env_0=-76.71387423370572\n",
      "Collide! - agent:  6  - stage:  1\n",
      "Resetting Agent:  6\n",
      "Collision detected between /World/Worker_4 and /World/Warehouse/Rack_04\n",
      "Arrive at the goal! - agent:  5  - stage:  1\n",
      "Resetting Agent:  5\n",
      "Arrive at the goal! - agent:  1  - stage:  1\n",
      "Resetting Agent:  1\n",
      "Arrive at the goal! - agent:  2  - stage:  1\n",
      "Resetting Agent:  2\n",
      "Collide! - agent:  3  - stage:  1\n",
      "Resetting Agent:  3\n",
      "Rank-0: policy_step=14584, average_reward_env_0=-60.02479712373538\n",
      "Collide! - agent:  3  - stage:  1\n",
      "Resetting Agent:  3\n",
      "Collide! - agent:  5  - stage:  1\n",
      "Resetting Agent:  5\n",
      "Rank-0: policy_step=14600, average_reward_env_0=-125.53330800260188\n",
      "Collide! - agent:  5  - stage:  1\n",
      "Resetting Agent:  5\n",
      "Collide! - agent:  8  - stage:  1\n",
      "Resetting Agent:  8\n",
      "Rank-0: policy_step=14608, average_reward_env_0=-60.306482230556774\n",
      "Collide! - agent:  8  - stage:  1\n",
      "Resetting Agent:  8\n",
      "Max step number - agent:  1\n",
      "Resetting Agent:  1\n",
      "Arrive at the goal! - agent:  6  - stage:  1\n",
      "Resetting Agent:  6\n",
      "Rank-0: policy_step=14624, average_reward_env_0=-65.93385915327558\n",
      "Arrive at the goal! - agent:  4  - stage:  1\n",
      "Resetting Agent:  4\n",
      "Collide! - agent:  5  - stage:  1\n",
      "Resetting Agent:  5\n",
      "Rank-0: policy_step=14664, average_reward_env_0=-122.78496411025972\n",
      "Collide! - agent:  5  - stage:  1\n",
      "Resetting Agent:  5\n",
      "Arrive at the goal! - agent:  2  - stage:  1\n",
      "Resetting Agent:  2\n",
      "Arrive at the goal! - agent:  6  - stage:  1\n",
      "Resetting Agent:  6\n",
      "Arrive at the goal! - agent:  4  - stage:  1\n",
      "Resetting Agent:  4\n",
      "Collide! - agent:  7  - stage:  1\n",
      "Resetting Agent:  7\n",
      "Rank-0: policy_step=14792, average_reward_env_0=-74.89875128796517\n",
      "Arrive at the goal! - agent:  6  - stage:  1\n",
      "Resetting Agent:  6\n",
      "Collide! - agent:  7  - stage:  1\n",
      "Resetting Agent:  7\n",
      "Collide! - agent:  1  - stage:  1\n",
      "Resetting Agent:  1\n",
      "Rank-0: policy_step=14824, average_reward_env_0=-65.23953385656587\n",
      "Collide! - agent:  1  - stage:  1\n",
      "Resetting Agent:  1\n",
      "Arrive at the goal! - agent:  5  - stage:  1\n",
      "Resetting Agent:  5\n",
      "Arrive at the goal! - agent:  7  - stage:  1\n",
      "Resetting Agent:  7\n",
      "Arrive at the goal! - agent:  8  - stage:  1\n",
      "Resetting Agent:  8\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Arrive at the goal! - agent:  7  - stage:  1\n",
      "Resetting Agent:  7\n",
      "Collide! - agent:  6  - stage:  1\n",
      "Resetting Agent:  6\n",
      "Rank-0: policy_step=14992, average_reward_env_0=-61.22762137865966\n",
      "Collide! - agent:  6  - stage:  1\n",
      "Resetting Agent:  6\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Arrive at the goal! - agent:  1  - stage:  1\n",
      "Resetting Agent:  1\n",
      "Collide! - agent:  4  - stage:  1\n",
      "Resetting Agent:  4\n",
      "Rank-0: policy_step=15040, average_reward_env_0=-38.91896465521846\n",
      "Arrive at the goal! - agent:  2  - stage:  1\n",
      "Resetting Agent:  2\n",
      "Arrive at the goal! - agent:  3  - stage:  1\n",
      "Resetting Agent:  3\n",
      "Arrive at the goal! - agent:  1  - stage:  1\n",
      "Resetting Agent:  1\n",
      "Collide! - agent:  8  - stage:  1\n",
      "Resetting Agent:  8\n",
      "Rank-0: policy_step=15088, average_reward_env_0=-52.86509631274891\n",
      "Arrive at the goal! - agent:  5  - stage:  1\n",
      "Resetting Agent:  5\n",
      "Collide! - agent:  8  - stage:  1\n",
      "Resetting Agent:  8\n",
      "Arrive at the goal! - agent:  1  - stage:  1\n",
      "Resetting Agent:  1\n",
      "Arrive at the goal! - agent:  7  - stage:  1\n",
      "Resetting Agent:  7\n",
      "Arrive at the goal! - agent:  3  - stage:  1\n",
      "Resetting Agent:  3\n",
      "Arrive at the goal! - agent:  2  - stage:  1\n",
      "Resetting Agent:  2\n",
      "Collide! - agent:  1  - stage:  1\n",
      "Resetting Agent:  1\n",
      "Arrive at the goal! - agent:  6  - stage:  1\n",
      "Resetting Agent:  6\n",
      "Rank-0: policy_step=15336, average_reward_env_0=-50.73623676895273\n",
      "Collide! - agent:  1  - stage:  1\n",
      "Resetting Agent:  1\n",
      "Arrive at the goal! - agent:  6  - stage:  1\n",
      "Resetting Agent:  6\n",
      "Arrive at the goal! - agent:  8  - stage:  1\n",
      "Resetting Agent:  8\n",
      "Arrive at the goal! - agent:  5  - stage:  1\n",
      "Resetting Agent:  5\n",
      "Arrive at the goal! - agent:  7  - stage:  1\n",
      "Resetting Agent:  7\n",
      "Arrive at the goal! - agent:  4  - stage:  1\n",
      "Resetting Agent:  4\n",
      "Collide! - agent:  3  - stage:  1\n",
      "Resetting Agent:  3\n",
      "Rank-0: policy_step=15456, average_reward_env_0=-47.41158161483326\n",
      "Collide! - agent:  3  - stage:  1\n",
      "Resetting Agent:  3\n",
      "Arrive at the goal! - agent:  2  - stage:  1\n",
      "Resetting Agent:  2\n",
      "Arrive at the goal! - agent:  8  - stage:  1\n",
      "Resetting Agent:  8\n",
      "Arrive at the goal! - agent:  5  - stage:  1\n",
      "Resetting Agent:  5\n",
      "Collide! - agent:  4  - stage:  1\n",
      "Resetting Agent:  4\n",
      "Arrive at the goal! - agent:  6  - stage:  1\n",
      "Resetting Agent:  6\n",
      "Rank-0: policy_step=15608, average_reward_env_0=-34.72661401615035\n",
      "Collide! - agent:  4  - stage:  1\n",
      "Resetting Agent:  4\n",
      "Arrive at the goal! - agent:  2  - stage:  1\n",
      "Resetting Agent:  2\n",
      "Arrive at the goal! - agent:  8  - stage:  1\n",
      "Resetting Agent:  8\n",
      "Arrive at the goal! - agent:  1  - stage:  1\n",
      "Resetting Agent:  1\n",
      "Max step number - agent:  2\n",
      "Resetting Agent:  2\n",
      "Arrive at the goal! - agent:  5  - stage:  1\n",
      "Resetting Agent:  5\n",
      "Rank-0: policy_step=15696, average_reward_env_0=-30.322268536023373\n",
      "Arrive at the goal! - agent:  4  - stage:  1\n",
      "Resetting Agent:  4\n",
      "Arrive at the goal! - agent:  1  - stage:  1\n",
      "Resetting Agent:  1\n",
      "Arrive at the goal! - agent:  3  - stage:  1\n",
      "Resetting Agent:  3\n",
      "Arrive at the goal! - agent:  6  - stage:  1\n",
      "Resetting Agent:  6\n",
      "Arrive at the goal! - agent:  3  - stage:  1\n",
      "Resetting Agent:  3\n",
      "Arrive at the goal! - agent:  8  - stage:  1\n",
      "Resetting Agent:  8\n",
      "Arrive at the goal! - agent:  3  - stage:  1\n",
      "Resetting Agent:  3\n",
      "Arrive at the goal! - agent:  5  - stage:  1\n",
      "Resetting Agent:  5\n",
      "Arrive at the goal! - agent:  2  - stage:  1\n",
      "Resetting Agent:  2\n",
      "Arrive at the goal! - agent:  1  - stage:  1\n",
      "Resetting Agent:  1\n",
      "Collide! - agent:  8  - stage:  1\n",
      "Resetting Agent:  8\n",
      "Rank-0: policy_step=15952, average_reward_env_0=-41.435679536186356\n",
      "Collide! - agent:  8  - stage:  1\n",
      "Resetting Agent:  8\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Arrive at the goal! - agent:  7  - stage:  1\n",
      "Resetting Agent:  7\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Arrive at the goal! - agent:  1  - stage:  1\n",
      "Resetting Agent:  1\n",
      "Arrive at the goal! - agent:  8  - stage:  1\n",
      "Resetting Agent:  8\n",
      "Arrive at the goal! - agent:  5  - stage:  1\n",
      "Resetting Agent:  5\n",
      "Arrive at the goal! - agent:  3  - stage:  1\n",
      "Resetting Agent:  3\n",
      "Arrive at the goal! - agent:  1  - stage:  1\n",
      "Resetting Agent:  1\n",
      "Arrive at the goal! - agent:  4  - stage:  1\n",
      "Resetting Agent:  4\n",
      "Arrive at the goal! - agent:  7  - stage:  1\n",
      "Resetting Agent:  7\n",
      "Collide! - agent:  8  - stage:  1\n",
      "Resetting Agent:  8\n",
      "Rank-0: policy_step=16224, average_reward_env_0=-32.2355500743998\n",
      "Collide! - agent:  8  - stage:  1\n",
      "Resetting Agent:  8\n",
      "Arrive at the goal! - agent:  1  - stage:  1\n",
      "Resetting Agent:  1\n",
      "Arrive at the goal! - agent:  2  - stage:  1\n",
      "Resetting Agent:  2\n",
      "Collide! - agent:  6  - stage:  1\n",
      "Resetting Agent:  6\n",
      "Rank-0: policy_step=16256, average_reward_env_0=-37.69830405601309\n",
      "Collide! - agent:  6  - stage:  1\n",
      "Resetting Agent:  6\n",
      "Max step number - agent:  5\n",
      "Resetting Agent:  5\n",
      "Rank-0: policy_step=16272, average_reward_env_0=-85.44047674190668\n",
      "Arrive at the goal! - agent:  1  - stage:  1\n",
      "Resetting Agent:  1\n",
      "Collide! - agent:  8  - stage:  1\n",
      "Resetting Agent:  8\n",
      "Rank-0: policy_step=16320, average_reward_env_0=-26.92408159113371\n",
      "Collide! - agent:  8  - stage:  1\n",
      "Resetting Agent:  8\n",
      "Arrive at the goal! - agent:  7  - stage:  1\n",
      "Resetting Agent:  7\n",
      "Arrive at the goal! - agent:  8  - stage:  1\n",
      "Resetting Agent:  8\n",
      "Arrive at the goal! - agent:  6  - stage:  1\n",
      "Resetting Agent:  6\n",
      "Max step number - agent:  7\n",
      "Resetting Agent:  7\n",
      "Rank-0: policy_step=16400, average_reward_env_0=-23.519040581783223\n",
      "Arrive at the goal! - agent:  1  - stage:  1\n",
      "Resetting Agent:  1\n",
      "Arrive at the goal! - agent:  3  - stage:  1\n",
      "Resetting Agent:  3\n",
      "Arrive at the goal! - agent:  5  - stage:  1\n",
      "Resetting Agent:  5\n",
      "Arrive at the goal! - agent:  5  - stage:  1\n",
      "Resetting Agent:  5\n",
      "Arrive at the goal! - agent:  6  - stage:  1\n",
      "Resetting Agent:  6\n",
      "Arrive at the goal! - agent:  8  - stage:  1\n",
      "Resetting Agent:  8\n",
      "Arrive at the goal! - agent:  2  - stage:  1\n",
      "Resetting Agent:  2\n",
      "Arrive at the goal! - agent:  7  - stage:  1\n",
      "Resetting Agent:  7\n",
      "Arrive at the goal! - agent:  3  - stage:  1\n",
      "Resetting Agent:  3\n",
      "Arrive at the goal! - agent:  8  - stage:  1\n",
      "Resetting Agent:  8\n",
      "Arrive at the goal! - agent:  4  - stage:  1\n",
      "Resetting Agent:  4\n",
      "Arrive at the goal! - agent:  7  - stage:  1\n",
      "Resetting Agent:  7\n",
      "Arrive at the goal! - agent:  5  - stage:  1\n",
      "Resetting Agent:  5\n",
      "Arrive at the goal! - agent:  6  - stage:  1\n",
      "Resetting Agent:  6\n",
      "Max step number - agent:  1\n",
      "Resetting Agent:  1\n",
      "Rank-0: policy_step=16944, average_reward_env_0=-10.809844780968564\n",
      "Arrive at the goal! - agent:  8  - stage:  1\n",
      "Resetting Agent:  8\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Arrive at the goal! - agent:  4  - stage:  1\n",
      "Resetting Agent:  4\n",
      "Arrive at the goal! - agent:  7  - stage:  1\n",
      "Resetting Agent:  7\n",
      "Max step number - agent:  3\n",
      "Resetting Agent:  3\n",
      "Arrive at the goal! - agent:  5  - stage:  1\n",
      "Resetting Agent:  5\n",
      "Rank-0: policy_step=17064, average_reward_env_0=-17.655386177120473\n",
      "Arrive at the goal! - agent:  8  - stage:  1\n",
      "Resetting Agent:  8\n",
      "Arrive at the goal! - agent:  2  - stage:  1\n",
      "Resetting Agent:  2\n",
      "Collide! - agent:  6  - stage:  1\n",
      "Resetting Agent:  6\n",
      "Arrive at the goal! - agent:  7  - stage:  1\n",
      "Resetting Agent:  7\n",
      "Rank-0: policy_step=17152, average_reward_env_1=-21.283780926371772\n",
      "Arrive at the goal! - agent:  1  - stage:  1\n",
      "Resetting Agent:  1\n",
      "Arrive at the goal! - agent:  5  - stage:  1\n",
      "Resetting Agent:  5\n",
      "Arrive at the goal! - agent:  4  - stage:  1\n",
      "Resetting Agent:  4\n",
      "Max step number - agent:  4\n",
      "Resetting Agent:  4\n",
      "Rank-0: policy_step=17216, average_reward_env_0=-9.988589018707263\n",
      "Arrive at the goal! - agent:  4  - stage:  1\n",
      "Resetting Agent:  4\n",
      "Max step number - agent:  2\n",
      "Resetting Agent:  2\n",
      "Arrive at the goal! - agent:  3  - stage:  1\n",
      "Resetting Agent:  3\n",
      "Arrive at the goal! - agent:  5  - stage:  1\n",
      "Resetting Agent:  5\n",
      "Arrive at the goal! - agent:  6  - stage:  1\n",
      "Resetting Agent:  6\n",
      "Rank-0: policy_step=17296, average_reward_env_0=-9.749601593096765\n",
      "Arrive at the goal! - agent:  6  - stage:  1\n",
      "Resetting Agent:  6\n",
      "Arrive at the goal! - agent:  8  - stage:  1\n",
      "Resetting Agent:  8\n",
      "Arrive at the goal! - agent:  1  - stage:  1\n",
      "Resetting Agent:  1\n",
      "Collide! - agent:  3  - stage:  1\n",
      "Resetting Agent:  3\n",
      "Rank-0: policy_step=17416, average_reward_env_0=-12.616375639990157\n",
      "Collide! - agent:  3  - stage:  1\n",
      "Resetting Agent:  3\n",
      "Arrive at the goal! - agent:  7  - stage:  1\n",
      "Resetting Agent:  7\n",
      "Arrive at the goal! - agent:  6  - stage:  1\n",
      "Resetting Agent:  6\n",
      "Arrive at the goal! - agent:  4  - stage:  1\n",
      "Resetting Agent:  4\n",
      "Arrive at the goal! - agent:  5  - stage:  1\n",
      "Resetting Agent:  5\n",
      "Arrive at the goal! - agent:  8  - stage:  1\n",
      "Resetting Agent:  8\n",
      "Arrive at the goal! - agent:  4  - stage:  1\n",
      "Resetting Agent:  4\n",
      "Collide! - agent:  5  - stage:  1\n",
      "Resetting Agent:  5\n",
      "Rank-0: policy_step=17608, average_reward_env_0=-50.97292316401777\n",
      "Arrive at the goal! - agent:  2  - stage:  1\n",
      "Resetting Agent:  2\n",
      "Arrive at the goal! - agent:  6  - stage:  1\n",
      "Resetting Agent:  6\n",
      "Arrive at the goal! - agent:  1  - stage:  1\n",
      "Resetting Agent:  1\n",
      "Collide! - agent:  6  - stage:  1\n",
      "Resetting Agent:  6\n",
      "Rank-0: policy_step=17696, average_reward_env_0=1.0221902407740187\n",
      "Arrive at the goal! - agent:  3  - stage:  1\n",
      "Resetting Agent:  3\n",
      "Arrive at the goal! - agent:  2  - stage:  1\n",
      "Resetting Agent:  2\n",
      "Collide! - agent:  7  - stage:  1\n",
      "Resetting Agent:  7\n",
      "Rank-0: policy_step=17800, average_reward_env_0=1.390427177189481\n",
      "Arrive at the goal! - agent:  5  - stage:  1\n",
      "Resetting Agent:  5\n",
      "Arrive at the goal! - agent:  6  - stage:  1\n",
      "Resetting Agent:  6\n",
      "Arrive at the goal! - agent:  1  - stage:  1\n",
      "Resetting Agent:  1\n",
      "Max step number - agent:  8\n",
      "Resetting Agent:  8\n",
      "Rank-0: policy_step=17928, average_reward_env_0=2.9926424926968456\n",
      "Arrive at the goal! - agent:  7  - stage:  1\n",
      "Resetting Agent:  7\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Judging Increasing\n",
      ">>>>>>>>>>>>>>>Increasing Difficulty>>>>>>>>>>>>>>>>>>>\n",
      "[2024-08-23 09:52:29,995][py.warnings][WARNING] - /home/jianheng/miniconda3/envs/sheeprl/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.increase_difficulty to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.increase_difficulty` for environment variables or `env.get_wrapper_attr('increase_difficulty')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Arrive at the goal! - agent:  4  - stage:  2\n",
      "Resetting Agent:  4\n",
      "Collision detected between /World/Worker_2 and /World/Worker_1\n",
      "Collision detected between /World/Worker_2 and /World/Worker_1\n",
      "Arrive at the goal! - agent:  5  - stage:  2\n",
      "Resetting Agent:  5\n",
      "Arrive at the goal! - agent:  2  - stage:  2\n",
      "Resetting Agent:  2\n",
      "Collide! - agent:  1  - stage:  2\n",
      "Resetting Agent:  1\n",
      "Rank-0: policy_step=18080, average_reward_env_0=421.5349706077018\n",
      "Arrive at the goal! - agent:  8  - stage:  2\n",
      "Resetting Agent:  8\n",
      "Arrive at the goal! - agent:  7  - stage:  2\n",
      "Resetting Agent:  7\n",
      "Arrive at the goal! - agent:  5  - stage:  2\n",
      "Resetting Agent:  5\n",
      "Arrive at the goal! - agent:  3  - stage:  2\n",
      "Resetting Agent:  3\n",
      "Arrive at the goal! - agent:  6  - stage:  2\n",
      "Resetting Agent:  6\n",
      "Arrive at the goal! - agent:  4  - stage:  2\n",
      "Resetting Agent:  4\n",
      "Arrive at the goal! - agent:  3  - stage:  2\n",
      "Resetting Agent:  3\n",
      "Arrive at the goal! - agent:  2  - stage:  2\n",
      "Resetting Agent:  2\n",
      "Arrive at the goal! - agent:  8  - stage:  2\n",
      "Resetting Agent:  8\n",
      "Arrive at the goal! - agent:  5  - stage:  2\n",
      "Resetting Agent:  5\n",
      "Collision detected between /World/Worker_2 and /World/Warehouse/Rack_07\n",
      "Arrive at the goal! - agent:  4  - stage:  2\n",
      "Resetting Agent:  4\n",
      "Arrive at the goal! - agent:  6  - stage:  2\n",
      "Resetting Agent:  6\n",
      "Arrive at the goal! - agent:  7  - stage:  2\n",
      "Resetting Agent:  7\n",
      "Arrive at the goal! - agent:  3  - stage:  2\n",
      "Resetting Agent:  3\n",
      "Collide! - agent:  4  - stage:  2\n",
      "Resetting Agent:  4\n",
      "Rank-0: policy_step=18920, average_reward_env_0=616.324887620552\n",
      "Collide! - agent:  4  - stage:  2\n",
      "Resetting Agent:  4\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Arrive at the goal! - agent:  2  - stage:  2\n",
      "Collide! - agent:  2  - stage:  2\n",
      "Resetting Agent:  2\n",
      "Rank-0: policy_step=19120, average_reward_env_0=549.5244367546197\n",
      "Arrive at the goal! - agent:  1  - stage:  2\n",
      "Resetting Agent:  1\n",
      "Collide! - agent:  7  - stage:  2\n",
      "Resetting Agent:  7\n",
      "Rank-0: policy_step=19128, average_reward_env_1=337.9927586834402\n",
      "Collide! - agent:  7  - stage:  2\n",
      "Resetting Agent:  7\n",
      "Collide! - agent:  7  - stage:  2\n",
      "Resetting Agent:  7\n",
      "Arrive at the goal! - agent:  8  - stage:  2\n",
      "Resetting Agent:  8\n",
      "Collide! - agent:  6  - stage:  2\n",
      "Resetting Agent:  6\n",
      "Rank-0: policy_step=19200, average_reward_env_0=329.864009347091\n",
      "Collide! - agent:  6  - stage:  2\n",
      "Resetting Agent:  6\n",
      "Collide! - agent:  5  - stage:  2\n",
      "Resetting Agent:  5\n",
      "Rank-0: policy_step=19280, average_reward_env_0=462.13737076673476\n",
      "Collide! - agent:  5  - stage:  2\n",
      "Resetting Agent:  5\n",
      "Collide! - agent:  7  - stage:  2\n",
      "Resetting Agent:  7\n",
      "Rank-0: policy_step=19288, average_reward_env_0=149.63902468265385\n",
      "Max step number - agent:  2\n",
      "Resetting Agent:  2\n",
      "Rank-0: policy_step=19296, average_reward_env_0=556.5068393140036\n",
      "Arrive at the goal! - agent:  4  - stage:  2\n",
      "Resetting Agent:  4\n",
      "Arrive at the goal! - agent:  3  - stage:  2\n",
      "Resetting Agent:  3\n",
      "Collide! - agent:  4  - stage:  2\n",
      "Resetting Agent:  4\n",
      "Rank-0: policy_step=19376, average_reward_env_0=337.68136822272294\n",
      "Collide! - agent:  4  - stage:  2\n",
      "Resetting Agent:  4\n",
      "Max step number - agent:  3\n",
      "Resetting Agent:  3\n",
      "Rank-0: policy_step=19424, average_reward_env_0=675.6511012503796\n",
      "Arrive at the goal! - agent:  1  - stage:  2\n",
      "Resetting Agent:  1\n",
      "Collide! - agent:  7  - stage:  2\n",
      "Resetting Agent:  7\n",
      "Rank-0: policy_step=19496, average_reward_env_0=57.91543123340523\n",
      "Collide! - agent:  7  - stage:  2\n",
      "Resetting Agent:  7\n",
      "Arrive at the goal! - agent:  8  - stage:  2\n",
      "Resetting Agent:  8\n",
      "Collide! - agent:  1  - stage:  2\n",
      "Resetting Agent:  1\n",
      "Rank-0: policy_step=19544, average_reward_env_0=326.4885219168615\n",
      "Collide! - agent:  1  - stage:  2\n",
      "Resetting Agent:  1\n",
      "Collision detected between /World/Worker_1 and /World/Warehouse/Warehouse/SM_Rackshield_3841/SM_Rackshield_02\n",
      "Collide! - agent:  4  - stage:  2\n",
      "Resetting Agent:  4\n",
      "Rank-0: policy_step=19680, average_reward_env_0=213.80389799448997\n",
      "Collide! - agent:  4  - stage:  2\n",
      "Resetting Agent:  4\n",
      "Collision detected between /World/Worker_1 and /World/Warehouse/Forklift_B01_PR_V_NVD_01\n",
      "Collision detected between /World/Worker_1 and /World/Warehouse/Forklift_B01_PR_V_NVD_01\n",
      "Collide! - agent:  2  - stage:  2\n",
      "Resetting Agent:  2\n",
      "Rank-0: policy_step=19744, average_reward_env_0=362.52405266352\n",
      "Collide! - agent:  2  - stage:  2\n",
      "Resetting Agent:  2\n",
      "Arrive at the goal! - agent:  7  - stage:  2\n",
      "Resetting Agent:  7\n",
      "Arrive at the goal! - agent:  6  - stage:  2\n",
      "Resetting Agent:  6\n",
      "Max step number - agent:  8\n",
      "Resetting Agent:  8\n",
      "Rank-0: policy_step=19928, average_reward_env_0=516.1496371418734\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collide! - agent:  5  - stage:  2\n",
      "Resetting Agent:  5\n",
      "Rank-0: policy_step=19976, average_reward_env_0=213.89387834703254\n",
      "Collide! - agent:  5  - stage:  2\n",
      "Resetting Agent:  5\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Arrive at the goal! - agent:  4  - stage:  2\n",
      "Resetting Agent:  4\n",
      "Collision detected between /World/Worker_1 and /World/Warehouse/Forklift_B01_PR_V_NVD_01\n",
      "Collision detected between /World/Worker_2 and /World/Warehouse/Rack_04\n",
      "Collide! - agent:  2  - stage:  2\n",
      "Resetting Agent:  2\n",
      "Rank-0: policy_step=20312, average_reward_env_0=257.244989504181\n",
      "Collide! - agent:  2  - stage:  2\n",
      "Resetting Agent:  2\n",
      "Arrive at the goal! - agent:  1  - stage:  2\n",
      "Resetting Agent:  1\n",
      "Arrive at the goal! - agent:  8  - stage:  2\n",
      "Resetting Agent:  8\n",
      "Collide! - agent:  5  - stage:  2\n",
      "Resetting Agent:  5\n",
      "Rank-0: policy_step=20416, average_reward_env_0=136.02820481918994\n",
      "Collide! - agent:  5  - stage:  2\n",
      "Resetting Agent:  5\n",
      "Arrive at the goal! - agent:  6  - stage:  2\n",
      "Resetting Agent:  6\n",
      "Collide! - agent:  7  - stage:  2\n",
      "Resetting Agent:  7\n",
      "Rank-0: policy_step=20440, average_reward_env_0=68.99640095470417\n",
      "Collide! - agent:  7  - stage:  2\n",
      "Resetting Agent:  7\n",
      "Arrive at the goal! - agent:  4  - stage:  2\n",
      "Resetting Agent:  4\n",
      "Arrive at the goal! - agent:  3  - stage:  2\n",
      "Resetting Agent:  3\n",
      "Collision detected between /World/Worker_1 and /World/Warehouse/Warehouse/SM_Rackshield_2984/SM_Rackshield_02\n",
      "Collision detected between /World/Worker_1 and /World/Warehouse/Table_02\n",
      "Collide! - agent:  6  - stage:  2\n",
      "Resetting Agent:  6\n",
      "Collision detected between /World/Worker_1 and /World/Warehouse/Table_02\n",
      "Rank-0: policy_step=20576, average_reward_env_0=274.31826232535474\n",
      "Collide! - agent:  6  - stage:  2\n",
      "Resetting Agent:  6\n",
      "Collide! - agent:  8  - stage:  2\n",
      "Resetting Agent:  8\n",
      "Rank-0: policy_step=20824, average_reward_env_0=303.41176391630455\n",
      "Arrive at the goal! - agent:  2  - stage:  2\n",
      "Resetting Agent:  2\n",
      "Collide! - agent:  5  - stage:  2\n",
      "Resetting Agent:  5\n",
      "Rank-0: policy_step=20912, average_reward_env_0=86.98704034959538\n",
      "Arrive at the goal! - agent:  4  - stage:  2\n",
      "Resetting Agent:  4\n",
      "Collide! - agent:  6  - stage:  2\n",
      "Resetting Agent:  6\n",
      "Rank-0: policy_step=20968, average_reward_env_0=166.6485332671706\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collide! - agent:  6  - stage:  2\n",
      "Resetting Agent:  6\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collide! - agent:  6  - stage:  2\n",
      "Resetting Agent:  6\n",
      "Rank-0: policy_step=21008, average_reward_env_0=111.92332009443817\n",
      "Arrive at the goal! - agent:  3  - stage:  2\n",
      "Resetting Agent:  3\n",
      "Collide! - agent:  3  - stage:  2\n",
      "Resetting Agent:  3\n",
      "Rank-0: policy_step=21160, average_reward_env_0=441.0108112515479\n",
      "Collide! - agent:  8  - stage:  2\n",
      "Resetting Agent:  8\n",
      "Rank-0: policy_step=21184, average_reward_env_0=170.38412481502067\n",
      "Collide! - agent:  8  - stage:  2\n",
      "Resetting Agent:  8\n",
      "Arrive at the goal! - agent:  7  - stage:  2\n",
      "Resetting Agent:  7\n",
      "Collide! - agent:  2  - stage:  2\n",
      "Resetting Agent:  2\n",
      "Rank-0: policy_step=21288, average_reward_env_0=225.27055702604784\n",
      "Collide! - agent:  2  - stage:  2\n",
      "Resetting Agent:  2\n",
      "Arrive at the goal! - agent:  6  - stage:  2\n",
      "Resetting Agent:  6\n",
      "Arrive at the goal! - agent:  1  - stage:  2\n",
      "Resetting Agent:  1\n",
      "Arrive at the goal! - agent:  4  - stage:  2\n",
      "Resetting Agent:  4\n",
      "Collide! - agent:  2  - stage:  2\n",
      "Resetting Agent:  2\n",
      "Collide! - agent:  5  - stage:  2\n",
      "Resetting Agent:  5\n",
      "Rank-0: policy_step=21448, average_reward_env_0=181.0685760838774\n",
      "Rank-0: policy_step=21448, average_reward_env_1=67.25861860046527\n",
      "Collide! - agent:  2  - stage:  2\n",
      "Resetting Agent:  2\n",
      "Collide! - agent:  5  - stage:  2\n",
      "Resetting Agent:  5\n",
      "Collide! - agent:  5  - stage:  2\n",
      "Resetting Agent:  5\n",
      "Collision detected between /World/Worker_1 and /World/Warehouse/Warehouse/SM_Rackshield_3841/SM_Rackshield_02\n",
      "Arrive at the goal! - agent:  3  - stage:  2\n",
      "Resetting Agent:  3\n",
      "Collide! - agent:  5  - stage:  2\n",
      "Resetting Agent:  5\n",
      "Collide! - agent:  7  - stage:  2\n",
      "Resetting Agent:  7\n",
      "Rank-0: policy_step=21480, average_reward_env_0=70.16829868542514\n",
      "Collide! - agent:  7  - stage:  2\n",
      "Resetting Agent:  7\n",
      "Collision detected between /World/Worker_2 and /World/Worker_1\n",
      "Collision detected between /World/Worker_2 and /World/Worker_1\n",
      "Collision detected between /World/Worker_2 and /World/Worker_1\n",
      "Collision detected between /World/Worker_2 and /World/Worker_1\n",
      "Collide! - agent:  8  - stage:  2\n",
      "Resetting Agent:  8\n",
      "Rank-0: policy_step=21520, average_reward_env_0=119.1233516112832\n",
      "Max step number - agent:  1\n",
      "Resetting Agent:  1\n",
      "Rank-0: policy_step=21552, average_reward_env_0=316.87598463903964\n",
      "Max step number - agent:  4\n",
      "Resetting Agent:  4\n",
      "Rank-0: policy_step=21688, average_reward_env_0=300.1031042496962\n",
      "Collision detected between /World/Worker_1 and /World/Warehouse/Warehouse/SM_Rackshield_1075/SM_Rackshield_02\n",
      "Collision detected between /World/Worker_4 and /World/Worker_1\n",
      "Collision detected between /World/Worker_4 and /World/Worker_1\n",
      "Collision detected between /World/Worker_1 and /World/Warehouse/Warehouse/SM_SignCVer_290/SM_SignCVer_10\n",
      "Collision detected between /World/Worker_1 and /World/Warehouse/Warehouse/SM_RackFrame_4491/SM_RackFrame_03/SM_RackFrame_03/Section1\n",
      "Collide! - agent:  4  - stage:  2\n",
      "Resetting Agent:  4\n",
      "Rank-0: policy_step=21912, average_reward_env_0=231.49115868858615\n",
      "Collide! - agent:  4  - stage:  2\n",
      "Resetting Agent:  4\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collide! - agent:  8  - stage:  2\n",
      "Resetting Agent:  8\n",
      "Rank-0: policy_step=22000, average_reward_env_0=85.52394186187296\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collide! - agent:  8  - stage:  2\n",
      "Resetting Agent:  8\n",
      "Collide! - agent:  6  - stage:  2\n",
      "Resetting Agent:  6\n",
      "Rank-0: policy_step=22024, average_reward_env_0=107.8401065266598\n",
      "Collide! - agent:  6  - stage:  2\n",
      "Resetting Agent:  6\n",
      "Arrive at the goal! - agent:  1  - stage:  2\n",
      "Resetting Agent:  1\n",
      "Collision detected between /World/Worker_2 and /World/Warehouse/Rack_07\n",
      "Arrive at the goal! - agent:  3  - stage:  2\n",
      "Resetting Agent:  3\n",
      "Collide! - agent:  2  - stage:  2\n",
      "Resetting Agent:  2\n",
      "Rank-0: policy_step=22168, average_reward_env_0=151.92016101282093\n",
      "Collide! - agent:  2  - stage:  2\n",
      "Resetting Agent:  2\n",
      "Arrive at the goal! - agent:  7  - stage:  2\n",
      "Resetting Agent:  7\n",
      "Arrive at the goal! - agent:  5  - stage:  2\n",
      "Resetting Agent:  5\n",
      "Arrive at the goal! - agent:  6  - stage:  2\n",
      "Resetting Agent:  6\n",
      "Arrive at the goal! - agent:  1  - stage:  2\n",
      "Resetting Agent:  1\n",
      "Collide! - agent:  3  - stage:  2\n",
      "Resetting Agent:  3\n",
      "Rank-0: policy_step=22456, average_reward_env_0=368.94028976336966\n",
      "Collide! - agent:  3  - stage:  2\n",
      "Resetting Agent:  3\n",
      "Arrive at the goal! - agent:  8  - stage:  2\n",
      "Resetting Agent:  8\n",
      "Collide! - agent:  7  - stage:  2\n",
      "Resetting Agent:  7\n",
      "Rank-0: policy_step=22496, average_reward_env_0=74.10274083097633\n",
      "Collide! - agent:  7  - stage:  2\n",
      "Resetting Agent:  7\n",
      "Arrive at the goal! - agent:  6  - stage:  2\n",
      "Resetting Agent:  6\n",
      "Collide! - agent:  1  - stage:  2\n",
      "Resetting Agent:  1\n",
      "Rank-0: policy_step=22560, average_reward_env_0=288.9825207796775\n",
      "Collide! - agent:  1  - stage:  2\n",
      "Resetting Agent:  1\n",
      "Arrive at the goal! - agent:  5  - stage:  2\n",
      "Resetting Agent:  5\n",
      "Arrive at the goal! - agent:  4  - stage:  2\n",
      "Resetting Agent:  4\n",
      "Arrive at the goal! - agent:  2  - stage:  2\n",
      "Resetting Agent:  2\n",
      "Collide! - agent:  8  - stage:  2\n",
      "Resetting Agent:  8\n",
      "Rank-0: policy_step=22728, average_reward_env_0=81.91290803694311\n",
      "Collide! - agent:  8  - stage:  2\n",
      "Resetting Agent:  8\n",
      "Collision detected between /World/Worker_4 and /World/Warehouse/Warehouse/SM_RackFrame_3840/SM_RackFrame_03/SM_RackFrame_03/Section1\n",
      "Collision detected between /World/Worker_4 and /World/Warehouse/Warehouse/SM_Rackshield_3841/SM_Rackshield_02\n",
      "Collide! - agent:  7  - stage:  2\n",
      "Resetting Agent:  7\n",
      "Rank-0: policy_step=22864, average_reward_env_0=59.33924852163624\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Arrive at the goal! - agent:  3  - stage:  2\n",
      "Resetting Agent:  3\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collide! - agent:  2  - stage:  2\n",
      "Resetting Agent:  2\n",
      "Collide! - agent:  7  - stage:  2\n",
      "Resetting Agent:  7\n",
      "Rank-0: policy_step=23008, average_reward_env_0=134.29590450687533\n",
      "Rank-0: policy_step=23008, average_reward_env_1=45.46560119706263\n",
      "Collide! - agent:  2  - stage:  2\n",
      "Resetting Agent:  2\n",
      "Collide! - agent:  7  - stage:  2\n",
      "Resetting Agent:  7\n",
      "Arrive at the goal! - agent:  4  - stage:  2\n",
      "Resetting Agent:  4\n",
      "Arrive at the goal! - agent:  5  - stage:  2\n",
      "Resetting Agent:  5\n",
      "Collision detected between /World/Worker_2 and /World/Warehouse/Rack_04\n",
      "Collide! - agent:  4  - stage:  2\n",
      "Resetting Agent:  4\n",
      "Rank-0: policy_step=23120, average_reward_env_0=228.62401033493128\n",
      "Arrive at the goal! - agent:  8  - stage:  2\n",
      "Resetting Agent:  8\n",
      "Arrive at the goal! - agent:  1  - stage:  2\n",
      "Resetting Agent:  1\n",
      "Arrive at the goal! - agent:  5  - stage:  2\n",
      "Resetting Agent:  5\n",
      "Collide! - agent:  6  - stage:  2\n",
      "Resetting Agent:  6\n",
      "Rank-0: policy_step=23344, average_reward_env_0=131.30642193034916\n",
      "Collide! - agent:  6  - stage:  2\n",
      "Resetting Agent:  6\n",
      "Collide! - agent:  2  - stage:  2\n",
      "Resetting Agent:  2\n",
      "Collide! - agent:  7  - stage:  2\n",
      "Resetting Agent:  7\n",
      "Rank-0: policy_step=23368, average_reward_env_0=115.40568994510733\n",
      "Rank-0: policy_step=23368, average_reward_env_1=25.232106801621047\n",
      "Max step number - agent:  5\n",
      "Resetting Agent:  5\n",
      "Rank-0: policy_step=23472, average_reward_env_0=149.75598794791338\n",
      "Collide! - agent:  5  - stage:  2\n",
      "Resetting Agent:  5\n",
      "Rank-0: policy_step=23504, average_reward_env_0=120.42417037497118\n",
      "Collide! - agent:  5  - stage:  2\n",
      "Resetting Agent:  5\n",
      "Collide! - agent:  1  - stage:  2\n",
      "Resetting Agent:  1\n",
      "Rank-0: policy_step=23560, average_reward_env_0=253.06321598174796\n",
      "Collide! - agent:  7  - stage:  2\n",
      "Resetting Agent:  7\n",
      "Rank-0: policy_step=23624, average_reward_env_0=18.031005196524905\n",
      "Collide! - agent:  7  - stage:  2\n",
      "Resetting Agent:  7\n",
      "Collide! - agent:  5  - stage:  2\n",
      "Resetting Agent:  5\n",
      "Rank-0: policy_step=23656, average_reward_env_0=99.82875978058193\n",
      "Collide! - agent:  5  - stage:  2\n",
      "Resetting Agent:  5\n",
      "Arrive at the goal! - agent:  4  - stage:  2\n",
      "Resetting Agent:  4\n",
      "Collide! - agent:  5  - stage:  2\n",
      "Resetting Agent:  5\n",
      "Collide! - agent:  5  - stage:  2\n",
      "Resetting Agent:  5\n",
      "Collision detected between /World/Worker_1 and /World/Warehouse/Warehouse/SM_Rackshield_2984/SM_Rackshield_02\n",
      "Collide! - agent:  7  - stage:  2\n",
      "Resetting Agent:  7\n",
      "Rank-0: policy_step=23760, average_reward_env_0=11.366221325541868\n",
      "Collide! - agent:  7  - stage:  2\n",
      "Resetting Agent:  7\n",
      "Collide! - agent:  2  - stage:  2\n",
      "Resetting Agent:  2\n",
      "Rank-0: policy_step=23776, average_reward_env_0=97.01652890760207\n",
      "Arrive at the goal! - agent:  8  - stage:  2\n",
      "Resetting Agent:  8\n",
      "Arrive at the goal! - agent:  6  - stage:  2\n",
      "Resetting Agent:  6\n",
      "Arrive at the goal! - agent:  3  - stage:  2\n",
      "Resetting Agent:  3\n",
      "Arrive at the goal! - agent:  4  - stage:  2\n",
      "Resetting Agent:  4\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collide! - agent:  6  - stage:  2\n",
      "Resetting Agent:  6\n",
      "Rank-0: policy_step=24032, average_reward_env_0=124.7275850152412\n",
      "Arrive at the goal! - agent:  1  - stage:  2\n",
      "Resetting Agent:  1\n",
      "Arrive at the goal! - agent:  5  - stage:  2\n",
      "Resetting Agent:  5\n",
      "Arrive at the goal! - agent:  8  - stage:  2\n",
      "Resetting Agent:  8\n",
      "Collide! - agent:  3  - stage:  2\n",
      "Resetting Agent:  3\n",
      "Rank-0: policy_step=24184, average_reward_env_0=338.0584786548958\n",
      "Collide! - agent:  3  - stage:  2\n",
      "Resetting Agent:  3\n",
      "Collide! - agent:  7  - stage:  2\n",
      "Resetting Agent:  7\n",
      "Rank-0: policy_step=24232, average_reward_env_0=6.171947152712945\n",
      "Collide! - agent:  7  - stage:  2\n",
      "Resetting Agent:  7\n",
      "Arrive at the goal! - agent:  1  - stage:  2\n",
      "Resetting Agent:  1\n",
      "Arrive at the goal! - agent:  2  - stage:  2\n",
      "Resetting Agent:  2\n",
      "Collide! - agent:  1  - stage:  2\n",
      "Resetting Agent:  1\n",
      "Rank-0: policy_step=24280, average_reward_env_0=247.10073607117124\n",
      "Collide! - agent:  1  - stage:  2\n",
      "Resetting Agent:  1\n",
      "Rank-0: policy_step=24392, average_reward_env_0=205.2277547475714\n",
      "Arrive at the goal! - agent:  6  - stage:  2\n",
      "Resetting Agent:  6\n",
      "Arrive at the goal! - agent:  4  - stage:  2\n",
      "Resetting Agent:  4\n",
      "Collide! - agent:  6  - stage:  2\n",
      "Resetting Agent:  6\n",
      "Rank-0: policy_step=24632, average_reward_env_0=122.94149383684376\n",
      "Arrive at the goal! - agent:  5  - stage:  2\n",
      "Resetting Agent:  5\n",
      "Max step number - agent:  8\n",
      "Resetting Agent:  8\n",
      "Rank-0: policy_step=24736, average_reward_env_1=130.0298106512537\n",
      "Collide! - agent:  2  - stage:  2\n",
      "Resetting Agent:  2\n",
      "Rank-0: policy_step=24760, average_reward_env_0=88.20802895461826\n",
      "Collide! - agent:  2  - stage:  2\n",
      "Resetting Agent:  2\n",
      "Collide! - agent:  7  - stage:  2\n",
      "Resetting Agent:  7\n",
      "Rank-0: policy_step=24784, average_reward_env_0=4.649159076934536\n",
      "Collide! - agent:  7  - stage:  2\n",
      "Resetting Agent:  7\n",
      "Collide! - agent:  4  - stage:  2\n",
      "Resetting Agent:  4\n",
      "Rank-0: policy_step=24856, average_reward_env_0=250.1326030096687\n",
      "Collide! - agent:  4  - stage:  2\n",
      "Resetting Agent:  4\n",
      "Arrive at the goal! - agent:  1  - stage:  2\n",
      "Resetting Agent:  1\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Arrive at the goal! - agent:  2  - stage:  2\n",
      "Resetting Agent:  2\n",
      "Arrive at the goal! - agent:  6  - stage:  2\n",
      "Resetting Agent:  6\n",
      "Arrive at the goal! - agent:  3  - stage:  2\n",
      "Resetting Agent:  3\n",
      "Arrive at the goal! - agent:  4  - stage:  2\n",
      "Resetting Agent:  4\n",
      "Collide! - agent:  1  - stage:  2\n",
      "Resetting Agent:  1\n",
      "Rank-0: policy_step=25304, average_reward_env_0=191.61358843375976\n",
      "Arrive at the goal! - agent:  8  - stage:  2\n",
      "Resetting Agent:  8\n",
      "Arrive at the goal! - agent:  6  - stage:  2\n",
      "Resetting Agent:  6\n",
      "Arrive at the goal! - agent:  7  - stage:  2\n",
      "Resetting Agent:  7\n",
      "Arrive at the goal! - agent:  5  - stage:  2\n",
      "Resetting Agent:  5\n",
      "Arrive at the goal! - agent:  3  - stage:  2\n",
      "Resetting Agent:  3\n",
      "Arrive at the goal! - agent:  1  - stage:  2\n",
      "Resetting Agent:  1\n",
      "Arrive at the goal! - agent:  2  - stage:  2\n",
      "Resetting Agent:  2\n",
      "Max step number - agent:  5\n",
      "Resetting Agent:  5\n",
      "Rank-0: policy_step=25696, average_reward_env_0=133.8882875102626\n",
      "Collide! - agent:  4  - stage:  2\n",
      "Resetting Agent:  4\n",
      "Rank-0: policy_step=25784, average_reward_env_0=234.8642031935929\n",
      "Collide! - agent:  4  - stage:  2\n",
      "Resetting Agent:  4\n",
      "Collide! - agent:  5  - stage:  2\n",
      "Resetting Agent:  5\n",
      "Rank-0: policy_step=25880, average_reward_env_0=116.54646867912827\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Arrive at the goal! - agent:  3  - stage:  2\n",
      "Resetting Agent:  3\n",
      "Collide! - agent:  7  - stage:  2\n",
      "Resetting Agent:  7\n",
      "Rank-0: policy_step=26088, average_reward_env_0=6.209082714518773\n",
      "Collide! - agent:  7  - stage:  2\n",
      "Resetting Agent:  7\n",
      "Arrive at the goal! - agent:  1  - stage:  2\n",
      "Resetting Agent:  1\n",
      "Collision detected between /World/Worker_2 and /World/Warehouse/Table_12\n",
      "Max step number - agent:  3\n",
      "Resetting Agent:  3\n",
      "Rank-0: policy_step=26192, average_reward_env_0=348.88496608843406\n",
      "Arrive at the goal! - agent:  4  - stage:  2\n",
      "Resetting Agent:  4\n",
      "Arrive at the goal! - agent:  8  - stage:  2\n",
      "Resetting Agent:  8\n",
      "Collision detected between /World/Worker_2 and /World/Warehouse/Warehouse/SM_Rackshield_3841/SM_Rackshield_02\n",
      "Collision detected between /World/Worker_2 and /World/Warehouse/Warehouse/SM_Rackshield_3841/SM_Rackshield_02\n",
      "Arrive at the goal! - agent:  1  - stage:  2\n",
      "Resetting Agent:  1\n",
      "Collide! - agent:  5  - stage:  2\n",
      "Resetting Agent:  5\n",
      "Rank-0: policy_step=26416, average_reward_env_0=100.87954313959223\n",
      "Collide! - agent:  2  - stage:  2\n",
      "Resetting Agent:  2\n",
      "Rank-0: policy_step=26512, average_reward_env_0=100.60932714392318\n",
      "Collide! - agent:  6  - stage:  2\n",
      "Resetting Agent:  6\n",
      "Rank-0: policy_step=26560, average_reward_env_0=135.60619280906263\n",
      "Arrive at the goal! - agent:  8  - stage:  2\n",
      "Resetting Agent:  8\n",
      "Collide! - agent:  5  - stage:  2\n",
      "Resetting Agent:  5\n",
      "Rank-0: policy_step=26632, average_reward_env_0=88.09435091152612\n",
      "Collide! - agent:  5  - stage:  2\n",
      "Resetting Agent:  5\n",
      "Arrive at the goal! - agent:  3  - stage:  2\n",
      "Resetting Agent:  3\n",
      "Max step number - agent:  8\n",
      "Resetting Agent:  8\n",
      "Rank-0: policy_step=26736, average_reward_env_0=161.13448282222538\n",
      "Arrive at the goal! - agent:  4  - stage:  2\n",
      "Resetting Agent:  4\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Arrive at the goal! - agent:  3  - stage:  2\n",
      "Resetting Agent:  3\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Arrive at the goal! - agent:  8  - stage:  2\n",
      "Resetting Agent:  8\n",
      "Arrive at the goal! - agent:  6  - stage:  2\n",
      "Resetting Agent:  6\n",
      "Arrive at the goal! - agent:  2  - stage:  2\n",
      "Resetting Agent:  2\n",
      "Collide! - agent:  8  - stage:  2\n",
      "Resetting Agent:  8\n",
      "Rank-0: policy_step=27096, average_reward_env_0=152.2664574758129\n",
      "Collide! - agent:  8  - stage:  2\n",
      "Resetting Agent:  8\n",
      "Collide! - agent:  1  - stage:  2\n",
      "Resetting Agent:  1\n",
      "Rank-0: policy_step=27208, average_reward_env_0=206.19765441261916\n",
      "Collide! - agent:  1  - stage:  2\n",
      "Resetting Agent:  1\n",
      "Arrive at the goal! - agent:  3  - stage:  2\n",
      "Resetting Agent:  3\n",
      "Collide! - agent:  5  - stage:  2\n",
      "Resetting Agent:  5\n",
      "Rank-0: policy_step=27328, average_reward_env_0=78.1053025603003\n",
      "Collide! - agent:  5  - stage:  2\n",
      "Resetting Agent:  5\n",
      "Collide! - agent:  7  - stage:  2\n",
      "Resetting Agent:  7\n",
      "Rank-0: policy_step=27400, average_reward_env_0=4.035280297243751\n",
      "Collide! - agent:  7  - stage:  2\n",
      "Resetting Agent:  7\n",
      "Arrive at the goal! - agent:  4  - stage:  2\n",
      "Resetting Agent:  4\n",
      "Collide! - agent:  2  - stage:  2\n",
      "Resetting Agent:  2\n",
      "Rank-0: policy_step=27672, average_reward_env_0=95.46626960736943\n",
      "Collide! - agent:  2  - stage:  2\n",
      "Resetting Agent:  2\n",
      "Arrive at the goal! - agent:  7  - stage:  2\n",
      "Resetting Agent:  7\n",
      "Arrive at the goal! - agent:  8  - stage:  2\n",
      "Resetting Agent:  8\n",
      "Arrive at the goal! - agent:  5  - stage:  2\n",
      "Resetting Agent:  5\n",
      "Arrive at the goal! - agent:  1  - stage:  2\n",
      "Resetting Agent:  1\n",
      "Max step number - agent:  4\n",
      "Resetting Agent:  4\n",
      "Rank-0: policy_step=27792, average_reward_env_0=252.7343878500643\n",
      "Arrive at the goal! - agent:  3  - stage:  2\n",
      "Resetting Agent:  3\n",
      "Collide! - agent:  2  - stage:  2\n",
      "Resetting Agent:  2\n",
      "Rank-0: policy_step=27848, average_reward_env_0=84.86856452496768\n",
      "Collide! - agent:  2  - stage:  2\n",
      "Resetting Agent:  2\n",
      "Collide! - agent:  2  - stage:  2\n",
      "Resetting Agent:  2\n",
      "Rank-0: policy_step=27912, average_reward_env_0=75.52396421105213\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collide! - agent:  6  - stage:  2\n",
      "Resetting Agent:  6\n",
      "Rank-0: policy_step=28064, average_reward_env_0=112.53080067932157\n",
      "Collide! - agent:  6  - stage:  2\n",
      "Resetting Agent:  6\n",
      "Arrive at the goal! - agent:  7  - stage:  2\n",
      "Resetting Agent:  7\n",
      "Collision detected between /World/Worker_1 and /World/Warehouse/Warehouse/SM_Rackshield_2984/SM_Rackshield_02\n",
      "Arrive at the goal! - agent:  1  - stage:  2\n",
      "Resetting Agent:  1\n",
      "Collide! - agent:  8  - stage:  2\n",
      "Resetting Agent:  8\n",
      "Rank-0: policy_step=28184, average_reward_env_1=147.15629266807352\n",
      "Max step number - agent:  3\n",
      "Resetting Agent:  3\n",
      "Arrive at the goal! - agent:  4  - stage:  2\n",
      "Resetting Agent:  4\n",
      "Rank-0: policy_step=28192, average_reward_env_0=387.19862314934045\n",
      "Collide! - agent:  6  - stage:  2\n",
      "Resetting Agent:  6\n",
      "Rank-0: policy_step=28240, average_reward_env_0=97.44940182552337\n",
      "Collide! - agent:  6  - stage:  2\n",
      "Resetting Agent:  6\n",
      "Arrive at the goal! - agent:  2  - stage:  2\n",
      "Resetting Agent:  2\n",
      "Arrive at the goal! - agent:  5  - stage:  2\n",
      "Resetting Agent:  5\n",
      "Arrive at the goal! - agent:  8  - stage:  2\n",
      "Resetting Agent:  8\n",
      "Arrive at the goal! - agent:  4  - stage:  2\n",
      "Resetting Agent:  4\n",
      "Arrive at the goal! - agent:  7  - stage:  2\n",
      "Resetting Agent:  7\n",
      "Arrive at the goal! - agent:  5  - stage:  2\n",
      "Resetting Agent:  5\n",
      "Arrive at the goal! - agent:  3  - stage:  2\n",
      "Resetting Agent:  3\n",
      "Collide! - agent:  4  - stage:  2\n",
      "Resetting Agent:  4\n",
      "Rank-0: policy_step=28728, average_reward_env_0=252.44628269934228\n",
      "Arrive at the goal! - agent:  8  - stage:  2\n",
      "Resetting Agent:  8\n",
      "Arrive at the goal! - agent:  1  - stage:  2\n",
      "Resetting Agent:  1\n",
      "Arrive at the goal! - agent:  6  - stage:  2\n",
      "Resetting Agent:  6\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Arrive at the goal! - agent:  1  - stage:  2\n",
      "Resetting Agent:  1\n",
      "Arrive at the goal! - agent:  3  - stage:  2\n",
      "Resetting Agent:  3\n",
      "Max step number - agent:  1\n",
      "Resetting Agent:  1\n",
      "Rank-0: policy_step=29216, average_reward_env_0=236.02579861791824\n",
      "Arrive at the goal! - agent:  2  - stage:  2\n",
      "Resetting Agent:  2\n",
      "Arrive at the goal! - agent:  7  - stage:  2\n",
      "Resetting Agent:  7\n",
      "Max step number - agent:  5\n",
      "Resetting Agent:  5\n",
      "Rank-0: policy_step=29336, average_reward_env_0=102.12299353307067\n",
      "Arrive at the goal! - agent:  8  - stage:  2\n",
      "Resetting Agent:  8\n",
      "Collide! - agent:  1  - stage:  2\n",
      "Resetting Agent:  1\n",
      "Rank-0: policy_step=29392, average_reward_env_0=208.91819060776913\n",
      "Collide! - agent:  1  - stage:  2\n",
      "Resetting Agent:  1\n",
      "Max step number - agent:  7\n",
      "Resetting Agent:  7\n",
      "Rank-0: policy_step=29408, average_reward_env_0=37.353422510834655\n",
      "Collide! - agent:  3  - stage:  2\n",
      "Resetting Agent:  3\n",
      "Rank-0: policy_step=29464, average_reward_env_0=364.94156724961675\n",
      "Collide! - agent:  3  - stage:  2\n",
      "Resetting Agent:  3\n",
      "Collide! - agent:  3  - stage:  2\n",
      "Resetting Agent:  3\n",
      "Rank-0: policy_step=29544, average_reward_env_0=311.87099216510245\n",
      "Collide! - agent:  3  - stage:  2\n",
      "Resetting Agent:  3\n",
      "Arrive at the goal! - agent:  6  - stage:  2\n",
      "Resetting Agent:  6\n",
      "Arrive at the goal! - agent:  2  - stage:  2\n",
      "Resetting Agent:  2\n",
      "Collide! - agent:  5  - stage:  2\n",
      "Resetting Agent:  5\n",
      "Rank-0: policy_step=29656, average_reward_env_0=91.20810977977435\n",
      "Collide! - agent:  5  - stage:  2\n",
      "Resetting Agent:  5\n",
      "Max step number - agent:  2\n",
      "Resetting Agent:  2\n",
      "Rank-0: policy_step=29912, average_reward_env_0=96.16568243584213\n",
      "Arrive at the goal! - agent:  6  - stage:  2\n",
      "Resetting Agent:  6\n",
      "Arrive at the goal! - agent:  7  - stage:  2\n",
      "Resetting Agent:  7\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Arrive at the goal! - agent:  3  - stage:  2\n",
      "Resetting Agent:  3\n",
      "Collide! - agent:  5  - stage:  2\n",
      "Resetting Agent:  5\n",
      "Rank-0: policy_step=30104, average_reward_env_0=82.64384265362732\n",
      "Collide! - agent:  5  - stage:  2\n",
      "Resetting Agent:  5\n",
      "Collision detected between /World/Worker_4 and /World/Worker_2\n",
      "Collision detected between /World/Worker_4 and /World/Worker_2\n",
      "Collision detected between /World/Worker_4 and /World/Worker_2\n",
      "Collision detected between /World/Worker_4 and /World/Worker_2\n",
      "Max step number - agent:  8\n",
      "Resetting Agent:  8\n",
      "Rank-0: policy_step=30184, average_reward_env_0=170.8828077510612\n",
      "Collide! - agent:  2  - stage:  2\n",
      "Resetting Agent:  2\n",
      "Rank-0: policy_step=30224, average_reward_env_0=89.271409595698\n",
      "Arrive at the goal! - agent:  4  - stage:  2\n",
      "Resetting Agent:  4\n",
      "Max step number - agent:  6\n",
      "Resetting Agent:  6\n",
      "Rank-0: policy_step=30248, average_reward_env_1=123.29095723654264\n",
      "Arrive at the goal! - agent:  1  - stage:  2\n",
      "Resetting Agent:  1\n",
      "Collide! - agent:  7  - stage:  2\n",
      "Resetting Agent:  7\n",
      "Rank-0: policy_step=30344, average_reward_env_1=39.607144789522366\n",
      "Arrive at the goal! - agent:  3  - stage:  2\n",
      "Resetting Agent:  3\n",
      "Collide! - agent:  8  - stage:  2\n",
      "Resetting Agent:  8\n",
      "Rank-0: policy_step=30448, average_reward_env_0=152.44331739505517\n",
      "Collide! - agent:  8  - stage:  2\n",
      "Resetting Agent:  8\n",
      "Collision detected between /World/Worker_3 and /World/Warehouse/Table_03\n",
      "Arrive at the goal! - agent:  2  - stage:  2\n",
      "Resetting Agent:  2\n",
      "Arrive at the goal! - agent:  4  - stage:  2\n",
      "Resetting Agent:  4\n",
      "Arrive at the goal! - agent:  6  - stage:  2\n",
      "Resetting Agent:  6\n",
      "Collision detected between /World/Worker_4 and /World/Warehouse/Rack_07\n",
      "Max step number - agent:  4\n",
      "Resetting Agent:  4\n",
      "Rank-0: policy_step=30728, average_reward_env_0=254.66805113290866\n",
      "Collide! - agent:  1  - stage:  2\n",
      "Resetting Agent:  1\n",
      "Rank-0: policy_step=30744, average_reward_env_0=194.8810827359084\n",
      "Collide! - agent:  1  - stage:  2\n",
      "Resetting Agent:  1\n",
      "Collide! - agent:  3  - stage:  2\n",
      "Resetting Agent:  3\n",
      "Rank-0: policy_step=30872, average_reward_env_0=301.5274347478546\n",
      "Collide! - agent:  3  - stage:  2\n",
      "Resetting Agent:  3\n",
      "Arrive at the goal! - agent:  8  - stage:  2\n",
      "Resetting Agent:  8\n",
      "Arrive at the goal! - agent:  5  - stage:  2\n",
      "Resetting Agent:  5\n",
      "Arrive at the goal! - agent:  7  - stage:  2\n",
      "Resetting Agent:  7\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Arrive at the goal! - agent:  6  - stage:  2\n",
      "Resetting Agent:  6\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Arrive at the goal! - agent:  1  - stage:  2\n",
      "Resetting Agent:  1\n",
      "Collide! - agent:  3  - stage:  2\n",
      "Resetting Agent:  3\n",
      "Rank-0: policy_step=31288, average_reward_env_0=267.4075080291315\n",
      "Collide! - agent:  3  - stage:  2\n",
      "Resetting Agent:  3\n",
      "Collide! - agent:  8  - stage:  2\n",
      "Resetting Agent:  8\n",
      "Rank-0: policy_step=31296, average_reward_env_0=147.3031806671012\n",
      "Collide! - agent:  8  - stage:  2\n",
      "Resetting Agent:  8\n",
      "Arrive at the goal! - agent:  1  - stage:  2\n",
      "Resetting Agent:  1\n",
      "Collide! - agent:  2  - stage:  2\n",
      "Resetting Agent:  2\n",
      "Rank-0: policy_step=31336, average_reward_env_0=91.55521394662267\n",
      "Collide! - agent:  6  - stage:  2\n",
      "Resetting Agent:  6\n",
      "Rank-0: policy_step=31360, average_reward_env_0=126.97546840632685\n",
      "Collide! - agent:  6  - stage:  2\n",
      "Resetting Agent:  6\n",
      "Collision detected between /World/Worker_3 and /World/Worker_1\n",
      "Collision detected between /World/Worker_3 and /World/Worker_1\n",
      "Collision detected between /World/Worker_3 and /World/Worker_1\n",
      "Collision detected between /World/Worker_3 and /World/Worker_1\n",
      "Collide! - agent:  2  - stage:  2\n",
      "Resetting Agent:  2\n",
      "Rank-0: policy_step=31552, average_reward_env_0=83.54615672259096\n",
      "Collide! - agent:  2  - stage:  2\n",
      "Resetting Agent:  2\n",
      "Arrive at the goal! - agent:  7  - stage:  2\n",
      "Resetting Agent:  7\n",
      "Arrive at the goal! - agent:  6  - stage:  2\n",
      "Resetting Agent:  6\n",
      "Arrive at the goal! - agent:  1  - stage:  2\n",
      "Resetting Agent:  1\n",
      "Arrive at the goal! - agent:  2  - stage:  2\n",
      "Resetting Agent:  2\n",
      "Collision detected between /World/Worker_3 and /World/Warehouse/Rack_04\n",
      "Arrive at the goal! - agent:  3  - stage:  2\n",
      "Resetting Agent:  3\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Arrive at the goal! - agent:  7  - stage:  2\n",
      "Resetting Agent:  7\n",
      "Collide! - agent:  1  - stage:  2\n",
      "Resetting Agent:  1\n",
      "Max step number - agent:  5\n",
      "Resetting Agent:  5\n",
      "Rank-0: policy_step=32112, average_reward_env_0=206.09336585993321\n",
      "Rank-0: policy_step=32112, average_reward_env_1=87.75088613872356\n",
      "Collide! - agent:  1  - stage:  2\n",
      "Resetting Agent:  1\n",
      "Arrive at the goal! - agent:  4  - stage:  2\n",
      "Resetting Agent:  4\n",
      "Collide! - agent:  1  - stage:  2\n",
      "Resetting Agent:  1\n",
      "Rank-0: policy_step=32232, average_reward_env_0=188.16000417186697\n",
      "Collide! - agent:  1  - stage:  2\n",
      "Resetting Agent:  1\n",
      "Arrive at the goal! - agent:  8  - stage:  2\n",
      "Resetting Agent:  8\n",
      "Arrive at the goal! - agent:  3  - stage:  2\n",
      "Resetting Agent:  3\n",
      "Max step number - agent:  7\n",
      "Resetting Agent:  7\n",
      "Rank-0: policy_step=32344, average_reward_env_0=58.32174187583483\n",
      "Collide! - agent:  4  - stage:  2\n",
      "Resetting Agent:  4\n",
      "Rank-0: policy_step=32384, average_reward_env_0=243.34302738275144\n",
      "Collide! - agent:  4  - stage:  2\n",
      "Resetting Agent:  4\n",
      "Collide! - agent:  2  - stage:  2\n",
      "Resetting Agent:  2\n",
      "Rank-0: policy_step=32408, average_reward_env_0=83.41930980924327\n",
      "Collide! - agent:  2  - stage:  2\n",
      "Resetting Agent:  2\n",
      "Collision detected between /World/Worker_3 and /World/Worker_1\n",
      "Collision detected between /World/Worker_3 and /World/Worker_1\n",
      "Collision detected between /World/Worker_3 and /World/Worker_1\n",
      "Collision detected between /World/Worker_3 and /World/Worker_1\n",
      "Arrive at the goal! - agent:  5  - stage:  2\n",
      "Resetting Agent:  5\n",
      "Collision detected between /World/Worker_3 and /World/Warehouse/Rack_01\n",
      "Collision detected between /World/Worker_3 and /World/Warehouse/Rack_01\n",
      "Collision detected between /World/Worker_3 and /World/Warehouse/Warehouse/SM_WallA_6M18/SM_WallA_6M/SM_WallA_6M/Section0\n",
      "Collision detected between /World/Worker_3 and /World/Warehouse/Warehouse/SM_WallA_6M18/SM_WallA_6M/SM_WallA_6M/Section0\n",
      "Arrive at the goal! - agent:  1  - stage:  2\n",
      "Resetting Agent:  1\n",
      "Arrive at the goal! - agent:  8  - stage:  2\n",
      "Resetting Agent:  8\n",
      "Collide! - agent:  3  - stage:  2\n",
      "Resetting Agent:  3\n",
      "Rank-0: policy_step=32792, average_reward_env_0=264.5402979854433\n",
      "Collide! - agent:  3  - stage:  2\n",
      "Resetting Agent:  3\n",
      "Arrive at the goal! - agent:  5  - stage:  2\n",
      "Resetting Agent:  5\n",
      "Arrive at the goal! - agent:  4  - stage:  2\n",
      "Resetting Agent:  4\n",
      "Arrive at the goal! - agent:  2  - stage:  2\n",
      "Resetting Agent:  2\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Arrive at the goal! - agent:  8  - stage:  2\n",
      "Resetting Agent:  8\n",
      "Collide! - agent:  6  - stage:  2\n",
      "Resetting Agent:  6\n",
      "Rank-0: policy_step=33168, average_reward_env_0=126.12532875116618\n",
      "Collide! - agent:  6  - stage:  2\n",
      "Resetting Agent:  6\n",
      "Arrive at the goal! - agent:  5  - stage:  2\n",
      "Resetting Agent:  5\n",
      "Collide! - agent:  6  - stage:  2\n",
      "Resetting Agent:  6\n",
      "Rank-0: policy_step=33280, average_reward_env_0=112.1377333717238\n",
      "Collide! - agent:  6  - stage:  2\n",
      "Resetting Agent:  6\n",
      "Max step number - agent:  8\n",
      "Resetting Agent:  8\n",
      "Rank-0: policy_step=33304, average_reward_env_0=161.96001135531014\n",
      "Arrive at the goal! - agent:  7  - stage:  2\n",
      "Resetting Agent:  7\n",
      "Arrive at the goal! - agent:  1  - stage:  2\n",
      "Collide! - agent:  1  - stage:  2\n",
      "Resetting Agent:  1\n",
      "Rank-0: policy_step=33368, average_reward_env_0=186.8302764489861\n",
      "Arrive at the goal! - agent:  4  - stage:  2\n",
      "Resetting Agent:  4\n",
      "Collide! - agent:  4  - stage:  2\n",
      "Resetting Agent:  4\n",
      "Rank-0: policy_step=33392, average_reward_env_0=237.77351498130398\n",
      "Collide! - agent:  4  - stage:  2\n",
      "Resetting Agent:  4\n",
      "Collide! - agent:  5  - stage:  2\n",
      "Resetting Agent:  5\n",
      "Rank-0: policy_step=33504, average_reward_env_0=102.26773047619109\n",
      "Collide! - agent:  1  - stage:  2\n",
      "Resetting Agent:  1\n",
      "Rank-0: policy_step=33536, average_reward_env_0=179.12981913887478\n",
      "Collide! - agent:  1  - stage:  2\n",
      "Resetting Agent:  1\n",
      "Collide! - agent:  2  - stage:  2\n",
      "Resetting Agent:  2\n",
      "Arrive at the goal! - agent:  8  - stage:  2\n",
      "Resetting Agent:  8\n",
      "Rank-0: policy_step=33552, average_reward_env_0=82.4453706114097\n",
      "Collide! - agent:  2  - stage:  2\n",
      "Resetting Agent:  2\n",
      "Arrive at the goal! - agent:  7  - stage:  2\n",
      "Resetting Agent:  7\n",
      "Arrive at the goal! - agent:  6  - stage:  2\n",
      "Resetting Agent:  6\n",
      "Collide! - agent:  3  - stage:  2\n",
      "Resetting Agent:  3\n",
      "Rank-0: policy_step=33792, average_reward_env_0=224.57794262141957\n",
      "Collide! - agent:  3  - stage:  2\n",
      "Resetting Agent:  3\n",
      "Arrive at the goal! - agent:  8  - stage:  2\n",
      "Resetting Agent:  8\n",
      "Arrive at the goal! - agent:  4  - stage:  2\n",
      "Resetting Agent:  4\n",
      "Arrive at the goal! - agent:  1  - stage:  2\n",
      "Resetting Agent:  1\n",
      "Arrive at the goal! - agent:  2  - stage:  2\n",
      "Resetting Agent:  2\n",
      "Arrive at the goal! - agent:  5  - stage:  2\n",
      "Resetting Agent:  5\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Arrive at the goal! - agent:  6  - stage:  2\n",
      "Resetting Agent:  6\n",
      "Collision detected between /World/Worker_2 and /World/Warehouse/Rack_04\n",
      "Arrive at the goal! - agent:  4  - stage:  2\n",
      "Resetting Agent:  4\n",
      "Arrive at the goal! - agent:  3  - stage:  2\n",
      "Resetting Agent:  3\n",
      "Collide! - agent:  2  - stage:  2\n",
      "Resetting Agent:  2\n",
      "Rank-0: policy_step=34328, average_reward_env_0=82.12387111799877\n",
      "Max step number - agent:  7\n",
      "Resetting Agent:  7\n",
      "Rank-0: policy_step=34344, average_reward_env_0=71.67227362483716\n",
      "Arrive at the goal! - agent:  1  - stage:  2\n",
      "Resetting Agent:  1\n",
      "Arrive at the goal! - agent:  8  - stage:  2\n",
      "Resetting Agent:  8\n",
      "Arrive at the goal! - agent:  7  - stage:  2\n",
      "Resetting Agent:  7\n",
      "Collide! - agent:  8  - stage:  2\n",
      "Resetting Agent:  8\n",
      "Rank-0: policy_step=34688, average_reward_env_0=173.61901318590236\n",
      "Arrive at the goal! - agent:  3  - stage:  2\n",
      "Resetting Agent:  3\n",
      "Arrive at the goal! - agent:  1  - stage:  2\n",
      "Resetting Agent:  1\n",
      "Arrive at the goal! - agent:  2  - stage:  2\n",
      "Resetting Agent:  2\n",
      "Arrive at the goal! - agent:  4  - stage:  2\n",
      "Resetting Agent:  4\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collide! - agent:  3  - stage:  2\n",
      "Resetting Agent:  3\n",
      "Rank-0: policy_step=35008, average_reward_env_0=224.71943707297305\n",
      "Collide! - agent:  3  - stage:  2\n",
      "Resetting Agent:  3\n",
      "Collide! - agent:  6  - stage:  2\n",
      "Resetting Agent:  6\n",
      "Rank-0: policy_step=35024, average_reward_env_0=121.4154052246378\n",
      "Collide! - agent:  3  - stage:  2\n",
      "Resetting Agent:  3\n",
      "Rank-0: policy_step=35104, average_reward_env_0=204.8007877693693\n",
      "Collide! - agent:  3  - stage:  2\n",
      "Resetting Agent:  3\n",
      "Arrive at the goal! - agent:  1  - stage:  2\n",
      "Resetting Agent:  1\n",
      "Arrive at the goal! - agent:  5  - stage:  2\n",
      "Resetting Agent:  5\n",
      "Collide! - agent:  8  - stage:  2\n",
      "Resetting Agent:  8\n",
      "Rank-0: policy_step=35160, average_reward_env_0=160.89209705632277\n",
      "Collide! - agent:  8  - stage:  2\n",
      "Resetting Agent:  8\n",
      "Arrive at the goal! - agent:  2  - stage:  2\n",
      "Resetting Agent:  2\n",
      "Arrive at the goal! - agent:  4  - stage:  2\n",
      "Resetting Agent:  4\n",
      "Collide! - agent:  3  - stage:  2\n",
      "Resetting Agent:  3\n",
      "Rank-0: policy_step=35288, average_reward_env_0=188.86642742328107\n",
      "Collide! - agent:  8  - stage:  2\n",
      "Resetting Agent:  8\n",
      "Rank-0: policy_step=35328, average_reward_env_0=148.33168746345345\n",
      "Collide! - agent:  7  - stage:  2\n",
      "Resetting Agent:  7\n",
      "Rank-0: policy_step=35336, average_reward_env_0=72.49855990649739\n",
      "Collide! - agent:  7  - stage:  2\n",
      "Resetting Agent:  7\n",
      "Collide! - agent:  2  - stage:  2\n",
      "Resetting Agent:  2\n",
      "Rank-0: policy_step=35384, average_reward_env_0=88.09282139694128\n",
      "Collide! - agent:  2  - stage:  2\n",
      "Resetting Agent:  2\n",
      "Max step number - agent:  4\n",
      "Resetting Agent:  4\n",
      "Rank-0: policy_step=35400, average_reward_env_0=260.17524337231777\n",
      "Collide! - agent:  8  - stage:  2\n",
      "Resetting Agent:  8\n",
      "Rank-0: policy_step=35464, average_reward_env_0=137.99868019151063\n",
      "Arrive at the goal! - agent:  6  - stage:  2\n",
      "Resetting Agent:  6\n",
      "Max step number - agent:  5\n",
      "Resetting Agent:  5\n",
      "Rank-0: policy_step=35504, average_reward_env_0=113.12458988899134\n",
      "Max step number - agent:  1\n",
      "Resetting Agent:  1\n",
      "Rank-0: policy_step=35544, average_reward_env_0=201.72978068220388\n",
      "Arrive at the goal! - agent:  3  - stage:  2\n",
      "Resetting Agent:  3\n",
      "Collide! - agent:  8  - stage:  2\n",
      "Resetting Agent:  8\n",
      "Rank-0: policy_step=35744, average_reward_env_0=128.28507394763764\n",
      "Collide! - agent:  8  - stage:  2\n",
      "Resetting Agent:  8\n",
      "Arrive at the goal! - agent:  6  - stage:  2\n",
      "Resetting Agent:  6\n",
      "Arrive at the goal! - agent:  4  - stage:  2\n",
      "Resetting Agent:  4\n",
      "Arrive at the goal! - agent:  2  - stage:  2\n",
      "Resetting Agent:  2\n",
      "Arrive at the goal! - agent:  7  - stage:  2\n",
      "Resetting Agent:  7\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collide! - agent:  7  - stage:  2\n",
      "Resetting Agent:  7\n",
      "Rank-0: policy_step=35984, average_reward_env_0=73.40955839653449\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Arrive at the goal! - agent:  5  - stage:  2\n",
      "Resetting Agent:  5\n",
      "Collide! - agent:  3  - stage:  2\n",
      "Resetting Agent:  3\n",
      "Rank-0: policy_step=36024, average_reward_env_0=180.90935285464138\n",
      "Arrive at the goal! - agent:  6  - stage:  2\n",
      "Resetting Agent:  6\n",
      "Collide! - agent:  4  - stage:  2\n",
      "Resetting Agent:  4\n",
      "Rank-0: policy_step=36088, average_reward_env_0=249.5083377240483\n",
      "Collide! - agent:  4  - stage:  2\n",
      "Resetting Agent:  4\n",
      "Arrive at the goal! - agent:  1  - stage:  2\n",
      "Resetting Agent:  1\n",
      "Arrive at the goal! - agent:  8  - stage:  2\n",
      "Resetting Agent:  8\n",
      "Arrive at the goal! - agent:  5  - stage:  2\n",
      "Resetting Agent:  5\n",
      "Collide! - agent:  3  - stage:  2\n",
      "Resetting Agent:  3\n",
      "Rank-0: policy_step=36248, average_reward_env_0=165.3197953625533\n",
      "Collide! - agent:  3  - stage:  2\n",
      "Resetting Agent:  3\n",
      "Arrive at the goal! - agent:  7  - stage:  2\n",
      "Resetting Agent:  7\n",
      "Arrive at the goal! - agent:  6  - stage:  2\n",
      "Resetting Agent:  6\n",
      "Collide! - agent:  2  - stage:  2\n",
      "Resetting Agent:  2\n",
      "Rank-0: policy_step=36400, average_reward_env_0=87.00866731623013\n",
      "Collide! - agent:  2  - stage:  2\n",
      "Resetting Agent:  2\n",
      "Collision detected between /World/Worker_2 and /World/Warehouse/Table_12\n",
      "Arrive at the goal! - agent:  4  - stage:  2\n",
      "Resetting Agent:  4\n",
      "Collide! - agent:  4  - stage:  2\n",
      "Resetting Agent:  4\n",
      "Rank-0: policy_step=36536, average_reward_env_0=239.6945203111086\n",
      "Collide! - agent:  4  - stage:  2\n",
      "Resetting Agent:  4\n",
      "Collide! - agent:  7  - stage:  2\n",
      "Resetting Agent:  7\n",
      "Rank-0: policy_step=36568, average_reward_env_0=74.25072429355777\n",
      "Collide! - agent:  7  - stage:  2\n",
      "Resetting Agent:  7\n",
      "Arrive at the goal! - agent:  8  - stage:  2\n",
      "Resetting Agent:  8\n",
      "Arrive at the goal! - agent:  5  - stage:  2\n",
      "Resetting Agent:  5\n",
      "Arrive at the goal! - agent:  4  - stage:  2\n",
      "Resetting Agent:  4\n",
      "Collision detected between /World/Worker_3 and /World/Warehouse/Table_08\n",
      "Arrive at the goal! - agent:  1  - stage:  2\n",
      "Resetting Agent:  1\n",
      "Collide! - agent:  3  - stage:  2\n",
      "Resetting Agent:  3\n",
      "Rank-0: policy_step=36952, average_reward_env_0=154.84933742233358\n",
      "Arrive at the goal! - agent:  6  - stage:  2\n",
      "Resetting Agent:  6\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collide! - agent:  4  - stage:  2\n",
      "Resetting Agent:  4\n",
      "Rank-0: policy_step=36976, average_reward_env_0=228.56824042843326\n",
      "Collide! - agent:  1  - stage:  2\n",
      "Resetting Agent:  1\n",
      "Rank-0: policy_step=36992, average_reward_env_0=203.46397992437394\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Max step number - agent:  6\n",
      "Resetting Agent:  6\n",
      "Rank-0: policy_step=37024, average_reward_env_0=153.73782923864368\n",
      "Arrive at the goal! - agent:  7  - stage:  2\n",
      "Resetting Agent:  7\n",
      "Arrive at the goal! - agent:  5  - stage:  2\n",
      "Resetting Agent:  5\n",
      "Arrive at the goal! - agent:  8  - stage:  2\n",
      "Resetting Agent:  8\n",
      "Arrive at the goal! - agent:  1  - stage:  2\n",
      "Resetting Agent:  1\n",
      "Arrive at the goal! - agent:  3  - stage:  2\n",
      "Resetting Agent:  3\n",
      "Collide! - agent:  2  - stage:  2\n",
      "Resetting Agent:  2\n",
      "Rank-0: policy_step=37408, average_reward_env_0=80.86107700283377\n",
      "Collide! - agent:  4  - stage:  2\n",
      "Resetting Agent:  4\n",
      "Rank-0: policy_step=37416, average_reward_env_0=214.37766158624154\n",
      "Collide! - agent:  4  - stage:  2\n",
      "Resetting Agent:  4\n",
      "Collision detected between /World/Worker_4 and /World/Warehouse/Warehouse/GroundPlane/CollisionPlane\n",
      "Max step number - agent:  5\n",
      "Resetting Agent:  5\n",
      "Arrive at the goal! - agent:  6  - stage:  2\n",
      "Resetting Agent:  6\n",
      "Rank-0: policy_step=37504, average_reward_env_0=135.2391686914756\n",
      "Arrive at the goal! - agent:  7  - stage:  2\n",
      "Resetting Agent:  7\n",
      "Max step number - agent:  8\n",
      "Resetting Agent:  8\n",
      "Rank-0: policy_step=37752, average_reward_env_0=144.63680036753144\n",
      "Collide! - agent:  4  - stage:  2\n",
      "Resetting Agent:  4\n",
      "Rank-0: policy_step=37856, average_reward_env_0=201.6369902037843\n",
      "Collide! - agent:  4  - stage:  2\n",
      "Resetting Agent:  4\n",
      "Arrive at the goal! - agent:  5  - stage:  2\n",
      "Resetting Agent:  5\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Arrive at the goal! - agent:  7  - stage:  2\n",
      "Resetting Agent:  7\n",
      "Collide! - agent:  4  - stage:  2\n",
      "Resetting Agent:  4\n",
      "Rank-0: policy_step=38128, average_reward_env_0=189.71794951660004\n",
      "Arrive at the goal! - agent:  3  - stage:  2\n",
      "Resetting Agent:  3\n",
      "Arrive at the goal! - agent:  8  - stage:  2\n",
      "Resetting Agent:  8\n",
      "Arrive at the goal! - agent:  1  - stage:  2\n",
      "Resetting Agent:  1\n",
      "Arrive at the goal! - agent:  2  - stage:  2\n",
      "Resetting Agent:  2\n",
      "Arrive at the goal! - agent:  8  - stage:  2\n",
      "Resetting Agent:  8\n",
      "Collide! - agent:  7  - stage:  2\n",
      "Resetting Agent:  7\n",
      "Rank-0: policy_step=38432, average_reward_env_0=83.43090199720544\n",
      "Collide! - agent:  1  - stage:  2\n",
      "Resetting Agent:  1\n",
      "Collide! - agent:  7  - stage:  2\n",
      "Resetting Agent:  7\n",
      "Rank-0: policy_step=38440, average_reward_env_0=204.58644463560668\n",
      "Collide! - agent:  5  - stage:  2\n",
      "Resetting Agent:  5\n",
      "Rank-0: policy_step=38448, average_reward_env_0=133.87735120413473\n",
      "Collide! - agent:  5  - stage:  2\n",
      "Resetting Agent:  5\n",
      "Collide! - agent:  3  - stage:  2\n",
      "Resetting Agent:  3\n",
      "Rank-0: policy_step=38488, average_reward_env_0=156.76640495400372\n",
      "Collide! - agent:  3  - stage:  2\n",
      "Resetting Agent:  3\n",
      "Collide! - agent:  2  - stage:  2\n",
      "Resetting Agent:  2\n",
      "Rank-0: policy_step=38512, average_reward_env_0=63.150621738318875\n",
      "Arrive at the goal! - agent:  1  - stage:  2\n",
      "Resetting Agent:  1\n",
      "Collide! - agent:  3  - stage:  2\n",
      "Resetting Agent:  3\n",
      "Collide! - agent:  4  - stage:  2\n",
      "Resetting Agent:  4\n",
      "Arrive at the goal! - agent:  8  - stage:  2\n",
      "Resetting Agent:  8\n",
      "Rank-0: policy_step=38696, average_reward_env_0=146.46556754664508\n",
      "Rank-0: policy_step=38696, average_reward_env_1=179.18479533573048\n",
      "Collide! - agent:  3  - stage:  2\n",
      "Resetting Agent:  3\n",
      "Arrive at the goal! - agent:  5  - stage:  2\n",
      "Resetting Agent:  5\n",
      "Arrive at the goal! - agent:  7  - stage:  2\n",
      "Resetting Agent:  7\n",
      "Collide! - agent:  2  - stage:  2\n",
      "Resetting Agent:  2\n",
      "Rank-0: policy_step=38880, average_reward_env_0=39.185682937149544\n",
      "Collide! - agent:  2  - stage:  2\n",
      "Resetting Agent:  2\n",
      "Arrive at the goal! - agent:  4  - stage:  2\n",
      "Resetting Agent:  4\n",
      "Collision detected between /World/Worker_2 and /World/Worker_1\n",
      "Collision detected between /World/Worker_2 and /World/Worker_1\n",
      "Collision detected between /World/Worker_2 and /World/Worker_1\n",
      "Collision detected between /World/Worker_2 and /World/Worker_1\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Max step number - agent:  6\n",
      "Resetting Agent:  6\n",
      "Rank-0: policy_step=39024, average_reward_env_0=150.82968366313324\n",
      "Collide! - agent:  8  - stage:  2\n",
      "Resetting Agent:  8\n",
      "Rank-0: policy_step=39032, average_reward_env_0=155.69304685168055\n",
      "Arrive at the goal! - agent:  3  - stage:  2\n",
      "Resetting Agent:  3\n",
      "Arrive at the goal! - agent:  2  - stage:  2\n",
      "Resetting Agent:  2\n",
      "Collide! - agent:  3  - stage:  2\n",
      "Resetting Agent:  3\n",
      "Rank-0: policy_step=39128, average_reward_env_0=141.89100224828613\n",
      "Collide! - agent:  3  - stage:  2\n",
      "Resetting Agent:  3\n",
      "Arrive at the goal! - agent:  5  - stage:  2\n",
      "Resetting Agent:  5\n",
      "Collide! - agent:  4  - stage:  2\n",
      "Resetting Agent:  4\n",
      "Rank-0: policy_step=39248, average_reward_env_0=175.35384957878546\n",
      "Collide! - agent:  8  - stage:  2\n",
      "Resetting Agent:  8\n",
      "Rank-0: policy_step=39360, average_reward_env_0=147.17754680271307\n",
      "Arrive at the goal! - agent:  7  - stage:  2\n",
      "Resetting Agent:  7\n",
      "Collide! - agent:  8  - stage:  2\n",
      "Resetting Agent:  8\n",
      "Collide! - agent:  5  - stage:  2\n",
      "Resetting Agent:  5\n",
      "Rank-0: policy_step=39456, average_reward_env_0=139.13523940410104\n",
      "Arrive at the goal! - agent:  2  - stage:  2\n",
      "Resetting Agent:  2\n",
      "Arrive at the goal! - agent:  6  - stage:  2\n",
      "Resetting Agent:  6\n",
      "Collide! - agent:  8  - stage:  2\n",
      "Resetting Agent:  8\n",
      "Rank-0: policy_step=39568, average_reward_env_0=138.6748247898024\n",
      "Collide! - agent:  8  - stage:  2\n",
      "Resetting Agent:  8\n",
      "Collide! - agent:  7  - stage:  2\n",
      "Resetting Agent:  7\n",
      "Rank-0: policy_step=39704, average_reward_env_0=89.93153323675403\n",
      "Collide! - agent:  7  - stage:  2\n",
      "Resetting Agent:  7\n",
      "Arrive at the goal! - agent:  3  - stage:  2\n",
      "Resetting Agent:  3\n",
      "Collide! - agent:  5  - stage:  2\n",
      "Resetting Agent:  5\n",
      "Rank-0: policy_step=39744, average_reward_env_0=131.12394883065699\n",
      "Collide! - agent:  1  - stage:  2\n",
      "Resetting Agent:  1\n",
      "Rank-0: policy_step=39792, average_reward_env_0=198.10216721248864\n",
      "Collide! - agent:  1  - stage:  2\n",
      "Resetting Agent:  1\n",
      "Arrive at the goal! - agent:  6  - stage:  2\n",
      "Resetting Agent:  6\n",
      "Collide! - agent:  1  - stage:  2\n",
      "Resetting Agent:  1\n",
      "Arrive at the goal! - agent:  2  - stage:  2\n",
      "Resetting Agent:  2\n",
      "Rank-0: policy_step=39912, average_reward_env_0=186.74635582295542\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collide! - agent:  3  - stage:  2\n",
      "Resetting Agent:  3\n",
      "Rank-0: policy_step=40000, average_reward_env_0=139.25069682971576\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collide! - agent:  3  - stage:  2\n",
      "Resetting Agent:  3\n",
      "Collide! - agent:  3  - stage:  2\n",
      "Resetting Agent:  3\n",
      "Arrive at the goal! - agent:  8  - stage:  2\n",
      "Resetting Agent:  8\n",
      "Collide! - agent:  4  - stage:  2\n",
      "Resetting Agent:  4\n",
      "Rank-0: policy_step=40112, average_reward_env_0=166.48556412292476\n",
      "Collide! - agent:  4  - stage:  2\n",
      "Resetting Agent:  4\n",
      "Arrive at the goal! - agent:  6  - stage:  2\n",
      "Resetting Agent:  6\n",
      "Arrive at the goal! - agent:  5  - stage:  2\n",
      "Resetting Agent:  5\n",
      "Arrive at the goal! - agent:  1  - stage:  2\n",
      "Resetting Agent:  1\n",
      "Arrive at the goal! - agent:  7  - stage:  2\n",
      "Resetting Agent:  7\n",
      "Arrive at the goal! - agent:  2  - stage:  2\n",
      "Resetting Agent:  2\n",
      "Arrive at the goal! - agent:  5  - stage:  2\n",
      "Resetting Agent:  5\n",
      "Arrive at the goal! - agent:  7  - stage:  2\n",
      "Resetting Agent:  7\n",
      "Collide! - agent:  8  - stage:  2\n",
      "Resetting Agent:  8\n",
      "Rank-0: policy_step=40456, average_reward_env_0=137.55567294148653\n",
      "Arrive at the goal! - agent:  3  - stage:  2\n",
      "Resetting Agent:  3\n",
      "Collide! - agent:  4  - stage:  2\n",
      "Resetting Agent:  4\n",
      "Rank-0: policy_step=40496, average_reward_env_0=157.88513941505002\n",
      "Collide! - agent:  4  - stage:  2\n",
      "Resetting Agent:  4\n",
      "Collide! - agent:  6  - stage:  2\n",
      "Resetting Agent:  6\n",
      "Rank-0: policy_step=40568, average_reward_env_0=161.7129676270255\n",
      "Collide! - agent:  6  - stage:  2\n",
      "Resetting Agent:  6\n",
      "Collide! - agent:  6  - stage:  2\n",
      "Resetting Agent:  6\n",
      "Collide! - agent:  6  - stage:  2\n",
      "Resetting Agent:  6\n",
      "Arrive at the goal! - agent:  7  - stage:  2\n",
      "Resetting Agent:  7\n",
      "Arrive at the goal! - agent:  1  - stage:  2\n",
      "Resetting Agent:  1\n",
      "Collide! - agent:  1  - stage:  2\n",
      "Resetting Agent:  1\n",
      "Rank-0: policy_step=40712, average_reward_env_0=188.3800208313365\n",
      "Collide! - agent:  1  - stage:  2\n",
      "Resetting Agent:  1\n",
      "Arrive at the goal! - agent:  2  - stage:  2\n",
      "Resetting Agent:  2\n",
      "Arrive at the goal! - agent:  3  - stage:  2\n",
      "Resetting Agent:  3\n",
      "Collide! - agent:  4  - stage:  2\n",
      "Resetting Agent:  4\n",
      "Rank-0: policy_step=40752, average_reward_env_0=150.05804852028933\n",
      "Collide! - agent:  4  - stage:  2\n",
      "Resetting Agent:  4\n",
      "Collide! - agent:  7  - stage:  2\n",
      "Resetting Agent:  7\n",
      "Rank-0: policy_step=40768, average_reward_env_0=99.32850704737984\n",
      "Collide! - agent:  7  - stage:  2\n",
      "Resetting Agent:  7\n",
      "Arrive at the goal! - agent:  8  - stage:  2\n",
      "Resetting Agent:  8\n",
      "Collide! - agent:  2  - stage:  2\n",
      "Resetting Agent:  2\n",
      "Rank-0: policy_step=40864, average_reward_env_0=63.49152032956856\n",
      "Arrive at the goal! - agent:  5  - stage:  2\n",
      "Resetting Agent:  5\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collide! - agent:  6  - stage:  2\n",
      "Resetting Agent:  6\n",
      "Rank-0: policy_step=40992, average_reward_env_0=151.28466648311144\n",
      "Collide! - agent:  6  - stage:  2\n",
      "Resetting Agent:  6\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collide! - agent:  7  - stage:  2\n",
      "Resetting Agent:  7\n",
      "Rank-0: policy_step=41032, average_reward_env_0=84.5570748740836\n",
      "Collide! - agent:  7  - stage:  2\n",
      "Resetting Agent:  7\n",
      "Arrive at the goal! - agent:  5  - stage:  2\n",
      "Resetting Agent:  5\n",
      "Arrive at the goal! - agent:  2  - stage:  2\n",
      "Resetting Agent:  2\n",
      "Arrive at the goal! - agent:  3  - stage:  2\n",
      "Resetting Agent:  3\n",
      "Arrive at the goal! - agent:  6  - stage:  2\n",
      "Resetting Agent:  6\n",
      "Arrive at the goal! - agent:  4  - stage:  2\n",
      "Resetting Agent:  4\n",
      "Arrive at the goal! - agent:  7  - stage:  2\n",
      "Resetting Agent:  7\n",
      "Collide! - agent:  4  - stage:  2\n",
      "Resetting Agent:  4\n",
      "Rank-0: policy_step=41696, average_reward_env_0=128.86135248562542\n",
      "Collision detected between /World/Worker_1 and /World/Warehouse/Table_03\n",
      "Arrive at the goal! - agent:  1  - stage:  2\n",
      "Resetting Agent:  1\n",
      "Collide! - agent:  4  - stage:  2\n",
      "Resetting Agent:  4\n",
      "Collide! - agent:  3  - stage:  2\n",
      "Resetting Agent:  3\n",
      "Rank-0: policy_step=41720, average_reward_env_0=148.99357462536847\n",
      "Max step number - agent:  5\n",
      "Resetting Agent:  5\n",
      "Rank-0: policy_step=41744, average_reward_env_0=149.85616443479844\n",
      "Collision detected between /World/Worker_3 and /World/Worker_1\n",
      "Collision detected between /World/Worker_3 and /World/Worker_1\n",
      "Collision detected between /World/Worker_3 and /World/Worker_1\n",
      "Collision detected between /World/Worker_3 and /World/Worker_1\n",
      "Collide! - agent:  1  - stage:  2\n",
      "Resetting Agent:  1\n",
      "Rank-0: policy_step=41960, average_reward_env_0=183.19144448516838\n",
      "Collide! - agent:  1  - stage:  2\n",
      "Resetting Agent:  1\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Arrive at the goal! - agent:  6  - stage:  2\n",
      "Resetting Agent:  6\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Arrive at the goal! - agent:  7  - stage:  2\n",
      "Resetting Agent:  7\n",
      "Arrive at the goal! - agent:  5  - stage:  2\n",
      "Resetting Agent:  5\n",
      "Arrive at the goal! - agent:  3  - stage:  2\n",
      "Resetting Agent:  3\n",
      "Arrive at the goal! - agent:  4  - stage:  2\n",
      "Resetting Agent:  4\n",
      "Collide! - agent:  1  - stage:  2\n",
      "Resetting Agent:  1\n",
      "Rank-0: policy_step=42176, average_reward_env_0=173.85676196134423\n",
      "Arrive at the goal! - agent:  8  - stage:  2\n",
      "Resetting Agent:  8\n",
      "Collide! - agent:  1  - stage:  2\n",
      "Resetting Agent:  1\n",
      "Rank-0: policy_step=42368, average_reward_env_0=164.88707272853156\n",
      "Collide! - agent:  1  - stage:  2\n",
      "Resetting Agent:  1\n",
      "Rank-0: policy_step=42400, average_reward_env_0=145.27975835084754\n",
      "Collide! - agent:  1  - stage:  2\n",
      "Resetting Agent:  1\n",
      "Arrive at the goal! - agent:  6  - stage:  2\n",
      "Resetting Agent:  6\n",
      "Max step number - agent:  8\n",
      "Resetting Agent:  8\n",
      "Rank-0: policy_step=42456, average_reward_env_0=141.89397662653\n",
      "Arrive at the goal! - agent:  4  - stage:  2\n",
      "Resetting Agent:  4\n",
      "Collide! - agent:  3  - stage:  2\n",
      "Resetting Agent:  3\n",
      "Rank-0: policy_step=42552, average_reward_env_0=147.4123771639002\n",
      "Collide! - agent:  2  - stage:  2\n",
      "Resetting Agent:  2\n",
      "Rank-0: policy_step=42568, average_reward_env_0=69.88231878246114\n",
      "Collide! - agent:  2  - stage:  2\n",
      "Resetting Agent:  2\n",
      "Arrive at the goal! - agent:  7  - stage:  2\n",
      "Resetting Agent:  7\n",
      "Collide! - agent:  4  - stage:  2\n",
      "Resetting Agent:  4\n",
      "Rank-0: policy_step=42624, average_reward_env_0=135.2509257909961\n",
      "Collision detected between /World/Worker_1 and /World/Warehouse/Warehouse/SM_Rackshield_1075/SM_Rackshield_02\n",
      "Arrive at the goal! - agent:  8  - stage:  2\n",
      "Resetting Agent:  8\n",
      "Arrive at the goal! - agent:  1  - stage:  2\n",
      "Resetting Agent:  1\n",
      "Collide! - agent:  3  - stage:  2\n",
      "Resetting Agent:  3\n",
      "Rank-0: policy_step=42864, average_reward_env_0=138.80896833949618\n",
      "Collision detected between /World/Worker_4 and /World/Warehouse/Warehouse/SM_Rackshield_29/SM_Rackshield_02\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Arrive at the goal! - agent:  6  - stage:  2\n",
      "Resetting Agent:  6\n",
      "Max step number - agent:  6\n",
      "Resetting Agent:  6\n",
      "Rank-0: policy_step=43000, average_reward_env_0=170.93252531495\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Arrive at the goal! - agent:  2  - stage:  2\n",
      "Resetting Agent:  2\n",
      "Arrive at the goal! - agent:  5  - stage:  2\n",
      "Resetting Agent:  5\n",
      "Max step number - agent:  7\n",
      "Resetting Agent:  7\n",
      "Rank-0: policy_step=43040, average_reward_env_0=103.32159524278705\n",
      "Arrive at the goal! - agent:  4  - stage:  2\n",
      "Resetting Agent:  4\n",
      "Collide! - agent:  5  - stage:  2\n",
      "Resetting Agent:  5\n",
      "Rank-0: policy_step=43080, average_reward_env_0=150.76401309767914\n",
      "Arrive at the goal! - agent:  3  - stage:  2\n",
      "Resetting Agent:  3\n",
      "Arrive at the goal! - agent:  1  - stage:  2\n",
      "Resetting Agent:  1\n",
      "Arrive at the goal! - agent:  7  - stage:  2\n",
      "Collide! - agent:  7  - stage:  2\n",
      "Resetting Agent:  7\n",
      "Rank-0: policy_step=43360, average_reward_env_0=111.34635382546388\n",
      "Arrive at the goal! - agent:  6  - stage:  2\n",
      "Resetting Agent:  6\n",
      "Arrive at the goal! - agent:  1  - stage:  2\n",
      "Resetting Agent:  1\n",
      "Arrive at the goal! - agent:  8  - stage:  2\n",
      "Resetting Agent:  8\n",
      "Arrive at the goal! - agent:  2  - stage:  2\n",
      "Resetting Agent:  2\n",
      "Arrive at the goal! - agent:  7  - stage:  2\n",
      "Resetting Agent:  7\n",
      "Collide! - agent:  2  - stage:  2\n",
      "Resetting Agent:  2\n",
      "Rank-0: policy_step=43736, average_reward_env_0=74.55974189165582\n",
      "Collide! - agent:  1  - stage:  2\n",
      "Resetting Agent:  1\n",
      "Rank-0: policy_step=43840, average_reward_env_0=150.92938095075172\n",
      "Collide! - agent:  1  - stage:  2\n",
      "Resetting Agent:  1\n",
      "Arrive at the goal! - agent:  6  - stage:  2\n",
      "Resetting Agent:  6\n",
      "Arrive at the goal! - agent:  8  - stage:  2\n",
      "Resetting Agent:  8\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Judging Increasing\n",
      ">>>>>>>>>>>>>>>Increasing Difficulty>>>>>>>>>>>>>>>>>>>\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Arrive at the goal! - agent:  7  - stage:  3\n",
      "Resetting Agent:  7\n",
      "Arrive at the goal! - agent:  5  - stage:  3\n",
      "Resetting Agent:  5\n",
      "Arrive at the goal! - agent:  8  - stage:  3\n",
      "Resetting Agent:  8\n",
      "Collide! - agent:  6  - stage:  3\n",
      "Resetting Agent:  6\n",
      "Rank-0: policy_step=44296, average_reward_env_0=231.70087609667146\n",
      "Collide! - agent:  6  - stage:  3\n",
      "Resetting Agent:  6\n",
      "Arrive at the goal! - agent:  2  - stage:  3\n",
      "Resetting Agent:  2\n",
      "Collide! - agent:  7  - stage:  3\n",
      "Resetting Agent:  7\n",
      "Rank-0: policy_step=44416, average_reward_env_0=314.36681549134164\n",
      "Collide! - agent:  7  - stage:  3\n",
      "Resetting Agent:  7\n",
      "Arrive at the goal! - agent:  1  - stage:  3\n",
      "Resetting Agent:  1\n",
      "Arrive at the goal! - agent:  3  - stage:  3\n",
      "Resetting Agent:  3\n",
      "Collide! - agent:  5  - stage:  3\n",
      "Resetting Agent:  5\n",
      "Rank-0: policy_step=44552, average_reward_env_0=96.49976991622643\n",
      "Arrive at the goal! - agent:  4  - stage:  3\n",
      "Resetting Agent:  4\n",
      "Collide! - agent:  8  - stage:  3\n",
      "Resetting Agent:  8\n",
      "Rank-0: policy_step=44800, average_reward_env_0=481.6801996721594\n",
      "Collide! - agent:  8  - stage:  3\n",
      "Resetting Agent:  8\n",
      "Collide! - agent:  2  - stage:  3\n",
      "Resetting Agent:  2\n",
      "Rank-0: policy_step=44832, average_reward_env_0=90.61221284591758\n",
      "Collide! - agent:  2  - stage:  3\n",
      "Resetting Agent:  2\n",
      "Collide! - agent:  8  - stage:  3\n",
      "Resetting Agent:  8\n",
      "Rank-0: policy_step=44856, average_reward_env_0=211.62445218193207\n",
      "Collide! - agent:  8  - stage:  3\n",
      "Resetting Agent:  8\n",
      "Collide! - agent:  1  - stage:  3\n",
      "Resetting Agent:  1\n",
      "Rank-0: policy_step=44872, average_reward_env_0=132.65702387856388\n",
      "Collide! - agent:  1  - stage:  3\n",
      "Resetting Agent:  1\n",
      "Collide! - agent:  1  - stage:  3\n",
      "Resetting Agent:  1\n",
      "Collide! - agent:  6  - stage:  3\n",
      "Resetting Agent:  6\n",
      "Rank-0: policy_step=44968, average_reward_env_0=103.25422751620658\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collide! - agent:  6  - stage:  3\n",
      "Resetting Agent:  6\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Max step number - agent:  4\n",
      "Resetting Agent:  4\n",
      "Rank-0: policy_step=45024, average_reward_env_0=310.035442346808\n",
      "Arrive at the goal! - agent:  7  - stage:  3\n",
      "Resetting Agent:  7\n",
      "Max step number - agent:  3\n",
      "Resetting Agent:  3\n",
      "Rank-0: policy_step=45264, average_reward_env_0=328.6433522967365\n",
      "Arrive at the goal! - agent:  5  - stage:  3\n",
      "Resetting Agent:  5\n",
      "Collide! - agent:  4  - stage:  3\n",
      "Resetting Agent:  4\n",
      "Rank-0: policy_step=45304, average_reward_env_0=123.57028210671385\n",
      "Collide! - agent:  8  - stage:  3\n",
      "Resetting Agent:  8\n",
      "Rank-0: policy_step=45392, average_reward_env_0=130.8527249913861\n",
      "Collide! - agent:  2  - stage:  3\n",
      "Resetting Agent:  2\n",
      "Rank-0: policy_step=45448, average_reward_env_0=28.421445029175196\n",
      "Collide! - agent:  2  - stage:  3\n",
      "Resetting Agent:  2\n",
      "Collide! - agent:  5  - stage:  3\n",
      "Resetting Agent:  5\n",
      "Rank-0: policy_step=45544, average_reward_env_0=103.15327185488131\n",
      "Arrive at the goal! - agent:  1  - stage:  3\n",
      "Resetting Agent:  1\n",
      "Collide! - agent:  1  - stage:  3\n",
      "Resetting Agent:  1\n",
      "Collide! - agent:  6  - stage:  3\n",
      "Resetting Agent:  6\n",
      "Rank-0: policy_step=45688, average_reward_env_0=121.59005978210607\n",
      "Rank-0: policy_step=45688, average_reward_env_1=71.27214146861874\n",
      "Collide! - agent:  1  - stage:  3\n",
      "Resetting Agent:  1\n",
      "Collide! - agent:  6  - stage:  3\n",
      "Resetting Agent:  6\n",
      "Arrive at the goal! - agent:  7  - stage:  3\n",
      "Resetting Agent:  7\n",
      "Collide! - agent:  2  - stage:  3\n",
      "Resetting Agent:  2\n",
      "Rank-0: policy_step=45784, average_reward_env_0=8.755585943097214\n",
      "Collide! - agent:  8  - stage:  3\n",
      "Resetting Agent:  8\n",
      "Rank-0: policy_step=45808, average_reward_env_0=87.8670742222014\n",
      "Arrive at the goal! - agent:  3  - stage:  3\n",
      "Resetting Agent:  3\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collide! - agent:  7  - stage:  3\n",
      "Resetting Agent:  7\n",
      "Rank-0: policy_step=46192, average_reward_env_0=260.10808574868486\n",
      "Collide! - agent:  7  - stage:  3\n",
      "Resetting Agent:  7\n",
      "Collide! - agent:  8  - stage:  3\n",
      "Resetting Agent:  8\n",
      "Rank-0: policy_step=46224, average_reward_env_0=70.00187055736346\n",
      "Collide! - agent:  8  - stage:  3\n",
      "Resetting Agent:  8\n",
      "Collide! - agent:  2  - stage:  3\n",
      "Resetting Agent:  2\n",
      "Rank-0: policy_step=46240, average_reward_env_0=-10.099970065390965\n",
      "Arrive at the goal! - agent:  1  - stage:  3\n",
      "Resetting Agent:  1\n",
      "Collide! - agent:  2  - stage:  3\n",
      "Resetting Agent:  2\n",
      "Collide! - agent:  4  - stage:  3\n",
      "Resetting Agent:  4\n",
      "Rank-0: policy_step=46472, average_reward_env_0=70.23228873442231\n",
      "Collide! - agent:  4  - stage:  3\n",
      "Resetting Agent:  4\n",
      "Arrive at the goal! - agent:  5  - stage:  3\n",
      "Resetting Agent:  5\n",
      "Arrive at the goal! - agent:  6  - stage:  3\n",
      "Resetting Agent:  6\n",
      "Collide! - agent:  1  - stage:  3\n",
      "Resetting Agent:  1\n",
      "Arrive at the goal! - agent:  2  - stage:  3\n",
      "Resetting Agent:  2\n",
      "Arrive at the goal! - agent:  7  - stage:  3\n",
      "Resetting Agent:  7\n",
      "Rank-0: policy_step=46664, average_reward_env_0=106.6277765691463\n",
      "Collide! - agent:  1  - stage:  3\n",
      "Resetting Agent:  1\n",
      "Collide! - agent:  3  - stage:  3\n",
      "Resetting Agent:  3\n",
      "Rank-0: policy_step=46704, average_reward_env_0=227.18043853940242\n",
      "Collide! - agent:  8  - stage:  3\n",
      "Resetting Agent:  8\n",
      "Rank-0: policy_step=46736, average_reward_env_0=56.916320913935984\n",
      "Collide! - agent:  8  - stage:  3\n",
      "Resetting Agent:  8\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collide! - agent:  6  - stage:  3\n",
      "Resetting Agent:  6\n",
      "Rank-0: policy_step=47024, average_reward_env_0=84.06813569724946\n",
      "Arrive at the goal! - agent:  2  - stage:  3\n",
      "Resetting Agent:  2\n",
      "Arrive at the goal! - agent:  3  - stage:  3\n",
      "Resetting Agent:  3\n",
      "Arrive at the goal! - agent:  1  - stage:  3\n",
      "Resetting Agent:  1\n",
      "Collision detected between /World/Worker_3 and /World/Warehouse/Table_08\n",
      "Collide! - agent:  3  - stage:  3\n",
      "Resetting Agent:  3\n",
      "Rank-0: policy_step=47392, average_reward_env_0=171.76549732923863\n",
      "Arrive at the goal! - agent:  5  - stage:  3\n",
      "Resetting Agent:  5\n",
      "Arrive at the goal! - agent:  4  - stage:  3\n",
      "Resetting Agent:  4\n",
      "Collision detected between /World/Worker_4 and /World/Worker_3\n",
      "Collision detected between /World/Worker_4 and /World/Worker_3\n",
      "Collide! - agent:  5  - stage:  3\n",
      "Resetting Agent:  5\n",
      "Arrive at the goal! - agent:  7  - stage:  3\n",
      "Resetting Agent:  7\n",
      "Rank-0: policy_step=47704, average_reward_env_0=148.54617835797112\n",
      "Collide! - agent:  5  - stage:  3\n",
      "Resetting Agent:  5\n",
      "Collide! - agent:  8  - stage:  3\n",
      "Resetting Agent:  8\n",
      "Rank-0: policy_step=47712, average_reward_env_0=48.293218808624026\n",
      "Collide! - agent:  8  - stage:  3\n",
      "Resetting Agent:  8\n",
      "Arrive at the goal! - agent:  2  - stage:  3\n",
      "Resetting Agent:  2\n",
      "Collide! - agent:  5  - stage:  3\n",
      "Resetting Agent:  5\n",
      "Rank-0: policy_step=47840, average_reward_env_0=91.93591341153268\n",
      "Collide! - agent:  5  - stage:  3\n",
      "Resetting Agent:  5\n",
      "Collide! - agent:  1  - stage:  3\n",
      "Resetting Agent:  1\n",
      "Rank-0: policy_step=47920, average_reward_env_0=115.24582721681321\n",
      "Collide! - agent:  1  - stage:  3\n",
      "Resetting Agent:  1\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Arrive at the goal! - agent:  3  - stage:  3\n",
      "Resetting Agent:  3\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Arrive at the goal! - agent:  6  - stage:  3\n",
      "Resetting Agent:  6\n",
      "Arrive at the goal! - agent:  4  - stage:  3\n",
      "Resetting Agent:  4\n",
      "Collision detected between /World/Worker_2 and /World/Warehouse/Table_12\n",
      "Arrive at the goal! - agent:  8  - stage:  3\n",
      "Resetting Agent:  8\n",
      "Collide! - agent:  3  - stage:  3\n",
      "Resetting Agent:  3\n",
      "Rank-0: policy_step=48320, average_reward_env_0=161.9255479342981\n",
      "Collide! - agent:  3  - stage:  3\n",
      "Resetting Agent:  3\n",
      "Arrive at the goal! - agent:  5  - stage:  3\n",
      "Resetting Agent:  5\n",
      "Arrive at the goal! - agent:  7  - stage:  3\n",
      "Resetting Agent:  7\n",
      "Collide! - agent:  8  - stage:  3\n",
      "Resetting Agent:  8\n",
      "Rank-0: policy_step=48392, average_reward_env_0=53.784314409324516\n",
      "Collide! - agent:  8  - stage:  3\n",
      "Resetting Agent:  8\n",
      "Rank-0: policy_step=48480, average_reward_env_0=40.69933369543113\n",
      "Collide! - agent:  8  - stage:  3\n",
      "Resetting Agent:  8\n",
      "Collide! - agent:  5  - stage:  3\n",
      "Resetting Agent:  5\n",
      "Rank-0: policy_step=48560, average_reward_env_0=89.38916689102084\n",
      "Collide! - agent:  5  - stage:  3\n",
      "Resetting Agent:  5\n",
      "Arrive at the goal! - agent:  4  - stage:  3\n",
      "Resetting Agent:  4\n",
      "Max step number - agent:  7\n",
      "Resetting Agent:  7\n",
      "Rank-0: policy_step=48600, average_reward_env_0=330.2337994529061\n",
      "Max step number - agent:  2\n",
      "Resetting Agent:  2\n",
      "Rank-0: policy_step=48648, average_reward_env_0=95.93211345687335\n",
      "Arrive at the goal! - agent:  6  - stage:  3\n",
      "Resetting Agent:  6\n",
      "Arrive at the goal! - agent:  1  - stage:  3\n",
      "Resetting Agent:  1\n",
      "Max step number - agent:  4\n",
      "Resetting Agent:  4\n",
      "Rank-0: policy_step=48880, average_reward_env_0=167.74012396718177\n",
      "Collision detected between /World/Worker_4 and /World/Worker_3\n",
      "Collision detected between /World/Worker_4 and /World/Worker_3\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collide! - agent:  5  - stage:  3\n",
      "Resetting Agent:  5\n",
      "Rank-0: policy_step=49040, average_reward_env_0=71.78071878831445\n",
      "Collide! - agent:  5  - stage:  3\n",
      "Resetting Agent:  5\n",
      "Collide! - agent:  5  - stage:  3\n",
      "Resetting Agent:  5\n",
      "Collision detected between /World/Worker_4 and /World/Worker_2\n",
      "Collision detected between /World/Worker_4 and /World/Worker_2\n",
      "Collision detected between /World/Worker_4 and /World/Worker_2\n",
      "Collision detected between /World/Worker_4 and /World/Worker_2\n",
      "Collide! - agent:  7  - stage:  3\n",
      "Resetting Agent:  7\n",
      "Rank-0: policy_step=49080, average_reward_env_0=240.20099162772925\n",
      "Collide! - agent:  1  - stage:  3\n",
      "Resetting Agent:  1\n",
      "Collide! - agent:  7  - stage:  3\n",
      "Resetting Agent:  7\n",
      "Rank-0: policy_step=49088, average_reward_env_0=113.24766152691618\n",
      "Collide! - agent:  1  - stage:  3\n",
      "Resetting Agent:  1\n",
      "Arrive at the goal! - agent:  2  - stage:  3\n",
      "Resetting Agent:  2\n",
      "Arrive at the goal! - agent:  6  - stage:  3\n",
      "Resetting Agent:  6\n",
      "Arrive at the goal! - agent:  3  - stage:  3\n",
      "Resetting Agent:  3\n",
      "Max step number - agent:  6\n",
      "Resetting Agent:  6\n",
      "Rank-0: policy_step=49424, average_reward_env_0=158.8056798868879\n",
      "Collide! - agent:  3  - stage:  3\n",
      "Resetting Agent:  3\n",
      "Rank-0: policy_step=49512, average_reward_env_0=152.61690358428976\n",
      "Collide! - agent:  3  - stage:  3\n",
      "Resetting Agent:  3\n",
      "Collide! - agent:  8  - stage:  3\n",
      "Resetting Agent:  8\n",
      "Rank-0: policy_step=49576, average_reward_env_0=34.32001044700686\n",
      "Collide! - agent:  8  - stage:  3\n",
      "Resetting Agent:  8\n",
      "Arrive at the goal! - agent:  2  - stage:  3\n",
      "Resetting Agent:  2\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collide! - agent:  7  - stage:  3\n",
      "Resetting Agent:  7\n",
      "Rank-0: policy_step=49984, average_reward_env_0=187.55574153489417\n",
      "Collide! - agent:  7  - stage:  3\n",
      "Resetting Agent:  7\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Arrive at the goal! - agent:  8  - stage:  3\n",
      "Resetting Agent:  8\n",
      "Arrive at the goal! - agent:  3  - stage:  3\n",
      "Resetting Agent:  3\n",
      "Collide! - agent:  6  - stage:  3\n",
      "Resetting Agent:  6\n",
      "Rank-0: policy_step=50096, average_reward_env_0=125.60287227703283\n",
      "Collide! - agent:  6  - stage:  3\n",
      "Resetting Agent:  6\n",
      "Collide! - agent:  7  - stage:  3\n",
      "Resetting Agent:  7\n",
      "Rank-0: policy_step=50104, average_reward_env_0=148.0409875545935\n",
      "Collide! - agent:  7  - stage:  3\n",
      "Resetting Agent:  7\n",
      "Collide! - agent:  4  - stage:  3\n",
      "Resetting Agent:  4\n",
      "Rank-0: policy_step=50192, average_reward_env_0=133.41225670562633\n",
      "Collide! - agent:  4  - stage:  3\n",
      "Resetting Agent:  4\n",
      "Collide! - agent:  6  - stage:  3\n",
      "Resetting Agent:  6\n",
      "Rank-0: policy_step=50264, average_reward_env_0=99.42979788600316\n",
      "Collide! - agent:  6  - stage:  3\n",
      "Resetting Agent:  6\n",
      "Collide! - agent:  1  - stage:  3\n",
      "Resetting Agent:  1\n",
      "Rank-0: policy_step=50464, average_reward_env_0=95.00512645023963\n",
      "Collide! - agent:  1  - stage:  3\n",
      "Resetting Agent:  1\n",
      "Arrive at the goal! - agent:  8  - stage:  3\n",
      "Resetting Agent:  8\n",
      "Arrive at the goal! - agent:  5  - stage:  3\n",
      "Resetting Agent:  5\n",
      "Collide! - agent:  6  - stage:  3\n",
      "Resetting Agent:  6\n",
      "Rank-0: policy_step=50712, average_reward_env_0=81.945557912056\n",
      "Collide! - agent:  6  - stage:  3\n",
      "Resetting Agent:  6\n",
      "Collide! - agent:  8  - stage:  3\n",
      "Resetting Agent:  8\n",
      "Rank-0: policy_step=50840, average_reward_env_0=40.704453677884636\n",
      "Collide! - agent:  8  - stage:  3\n",
      "Resetting Agent:  8\n",
      "Collide! - agent:  7  - stage:  3\n",
      "Resetting Agent:  7\n",
      "Rank-0: policy_step=50888, average_reward_env_0=121.68607723909531\n",
      "Arrive at the goal! - agent:  2  - stage:  3\n",
      "Resetting Agent:  2\n",
      "Collide! - agent:  7  - stage:  3\n",
      "Resetting Agent:  7\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Arrive at the goal! - agent:  1  - stage:  3\n",
      "Resetting Agent:  1\n",
      "Max step number - agent:  2\n",
      "Resetting Agent:  2\n",
      "Rank-0: policy_step=51048, average_reward_env_0=159.90434085530288\n",
      "Arrive at the goal! - agent:  6  - stage:  3\n",
      "Resetting Agent:  6\n",
      "Collide! - agent:  2  - stage:  3\n",
      "Resetting Agent:  2\n",
      "Rank-0: policy_step=51312, average_reward_env_0=132.64181673082277\n",
      "Collide! - agent:  4  - stage:  3\n",
      "Resetting Agent:  4\n",
      "Rank-0: policy_step=51432, average_reward_env_0=110.00018574302571\n",
      "Arrive at the goal! - agent:  3  - stage:  3\n",
      "Resetting Agent:  3\n",
      "Max step number - agent:  5\n",
      "Resetting Agent:  5\n",
      "Rank-0: policy_step=51456, average_reward_env_1=92.29205139938576\n",
      "Arrive at the goal! - agent:  7  - stage:  3\n",
      "Resetting Agent:  7\n",
      "Collision detected between /World/Worker_1 and /World/Warehouse/Warehouse/GroundPlane/CollisionPlane\n",
      "Collide! - agent:  8  - stage:  3\n",
      "Resetting Agent:  8\n",
      "Rank-0: policy_step=51648, average_reward_env_0=28.116714434348996\n",
      "Collide! - agent:  8  - stage:  3\n",
      "Resetting Agent:  8\n",
      "Collide! - agent:  8  - stage:  3\n",
      "Resetting Agent:  8\n",
      "Max step number - agent:  3\n",
      "Resetting Agent:  3\n",
      "Rank-0: policy_step=51920, average_reward_env_0=183.3412475086243\n",
      "Arrive at the goal! - agent:  1  - stage:  3\n",
      "Resetting Agent:  1\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collide! - agent:  4  - stage:  3\n",
      "Resetting Agent:  4\n",
      "Rank-0: policy_step=52016, average_reward_env_0=87.84779505186816\n",
      "Collide! - agent:  4  - stage:  3\n",
      "Resetting Agent:  4\n",
      "Arrive at the goal! - agent:  5  - stage:  3\n",
      "Resetting Agent:  5\n",
      "Collide! - agent:  4  - stage:  3\n",
      "Resetting Agent:  4\n",
      "Rank-0: policy_step=52192, average_reward_env_0=69.78715392776225\n",
      "Collide! - agent:  4  - stage:  3\n",
      "Resetting Agent:  4\n",
      "Arrive at the goal! - agent:  6  - stage:  3\n",
      "Resetting Agent:  6\n",
      "Collision detected between /World/Worker_4 and /World/Worker_2\n",
      "Collision detected between /World/Worker_4 and /World/Worker_2\n",
      "Collision detected between /World/Worker_4 and /World/Worker_2\n",
      "Collision detected between /World/Worker_4 and /World/Worker_2\n",
      "Arrive at the goal! - agent:  1  - stage:  3\n",
      "Resetting Agent:  1\n",
      "Collide! - agent:  7  - stage:  3\n",
      "Resetting Agent:  7\n",
      "Rank-0: policy_step=52288, average_reward_env_1=122.56084046053789\n",
      "Collide! - agent:  7  - stage:  3\n",
      "Resetting Agent:  7\n",
      "Collide! - agent:  8  - stage:  3\n",
      "Resetting Agent:  8\n",
      "Rank-0: policy_step=52560, average_reward_env_0=24.662411138959687\n",
      "Collide! - agent:  8  - stage:  3\n",
      "Resetting Agent:  8\n",
      "Arrive at the goal! - agent:  6  - stage:  3\n",
      "Resetting Agent:  6\n",
      "Collide! - agent:  5  - stage:  3\n",
      "Resetting Agent:  5\n",
      "Rank-0: policy_step=52592, average_reward_env_0=93.95484247595775\n",
      "Arrive at the goal! - agent:  2  - stage:  3\n",
      "Resetting Agent:  2\n",
      "Arrive at the goal! - agent:  1  - stage:  3\n",
      "Resetting Agent:  1\n",
      "Collide! - agent:  5  - stage:  3\n",
      "Resetting Agent:  5\n",
      "Rank-0: policy_step=52808, average_reward_env_0=71.13830535323201\n",
      "Max step number - agent:  1\n",
      "Resetting Agent:  1\n",
      "Rank-0: policy_step=52872, average_reward_env_0=157.66272702388045\n",
      "Arrive at the goal! - agent:  7  - stage:  3\n",
      "Resetting Agent:  7\n",
      "Collide! - agent:  2  - stage:  3\n",
      "Resetting Agent:  2\n",
      "Rank-0: policy_step=52968, average_reward_env_0=131.61439955004238\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Arrive at the goal! - agent:  6  - stage:  3\n",
      "Resetting Agent:  6\n",
      "Collide! - agent:  8  - stage:  3\n",
      "Resetting Agent:  8\n",
      "Rank-0: policy_step=53080, average_reward_env_1=20.417934674250287\n",
      "Collide! - agent:  8  - stage:  3\n",
      "Resetting Agent:  8\n",
      "Max step number - agent:  6\n",
      "Resetting Agent:  6\n",
      "Rank-0: policy_step=53120, average_reward_env_0=140.25897770534\n",
      "Arrive at the goal! - agent:  1  - stage:  3\n",
      "Resetting Agent:  1\n",
      "Arrive at the goal! - agent:  5  - stage:  3\n",
      "Resetting Agent:  5\n",
      "Collide! - agent:  2  - stage:  3\n",
      "Resetting Agent:  2\n",
      "Rank-0: policy_step=53376, average_reward_env_0=115.31308089483736\n",
      "Arrive at the goal! - agent:  3  - stage:  3\n",
      "Resetting Agent:  3\n",
      "Arrive at the goal! - agent:  7  - stage:  3\n",
      "Resetting Agent:  7\n",
      "Arrive at the goal! - agent:  4  - stage:  3\n",
      "Resetting Agent:  4\n",
      "Collide! - agent:  4  - stage:  3\n",
      "Resetting Agent:  4\n",
      "Rank-0: policy_step=53560, average_reward_env_0=75.08985679460848\n",
      "Collide! - agent:  1  - stage:  3\n",
      "Resetting Agent:  1\n",
      "Rank-0: policy_step=53696, average_reward_env_0=153.98698079600453\n",
      "Arrive at the goal! - agent:  6  - stage:  3\n",
      "Resetting Agent:  6\n",
      "Arrive at the goal! - agent:  8  - stage:  3\n",
      "Resetting Agent:  8\n",
      "Arrive at the goal! - agent:  3  - stage:  3\n",
      "Resetting Agent:  3\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collide! - agent:  8  - stage:  3\n",
      "Resetting Agent:  8\n",
      "Rank-0: policy_step=54088, average_reward_env_0=28.041008678558832\n",
      "Arrive at the goal! - agent:  5  - stage:  3\n",
      "Resetting Agent:  5\n",
      "Collide! - agent:  6  - stage:  3\n",
      "Resetting Agent:  6\n",
      "Rank-0: policy_step=54152, average_reward_env_1=130.5476894198561\n",
      "Collide! - agent:  6  - stage:  3\n",
      "Resetting Agent:  6\n",
      "Arrive at the goal! - agent:  2  - stage:  3\n",
      "Resetting Agent:  2\n",
      "Max step number - agent:  3\n",
      "Resetting Agent:  3\n",
      "Rank-0: policy_step=54320, average_reward_env_0=205.91935029305142\n",
      "Collide! - agent:  4  - stage:  3\n",
      "Resetting Agent:  4\n",
      "Rank-0: policy_step=54328, average_reward_env_0=62.007854044959494\n",
      "Arrive at the goal! - agent:  1  - stage:  3\n",
      "Resetting Agent:  1\n",
      "Collision detected between /World/Worker_2 and /World/Warehouse/Table_12\n",
      "Collide! - agent:  2  - stage:  3\n",
      "Resetting Agent:  2\n",
      "Rank-0: policy_step=54576, average_reward_env_0=115.87109591034366\n",
      "Collide! - agent:  2  - stage:  3\n",
      "Resetting Agent:  2\n",
      "Collision detected between /World/Worker_2 and /World/Warehouse/Forklift_B01_PR_V_NVD_01\n",
      "Collision detected between /World/Worker_2 and /World/Warehouse/Forklift_B01_PR_V_NVD_01\n",
      "Arrive at the goal! - agent:  8  - stage:  3\n",
      "Resetting Agent:  8\n",
      "Max step number - agent:  7\n",
      "Resetting Agent:  7\n",
      "Rank-0: policy_step=54696, average_reward_env_0=146.07209625724093\n",
      "Arrive at the goal! - agent:  3  - stage:  3\n",
      "Resetting Agent:  3\n",
      "Arrive at the goal! - agent:  5  - stage:  3\n",
      "Resetting Agent:  5\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collide! - agent:  6  - stage:  3\n",
      "Resetting Agent:  6\n",
      "Rank-0: policy_step=55040, average_reward_env_0=115.63899367761668\n",
      "Collide! - agent:  6  - stage:  3\n",
      "Resetting Agent:  6\n",
      "Arrive at the goal! - agent:  4  - stage:  3\n",
      "Resetting Agent:  4\n",
      "Max step number - agent:  5\n",
      "Resetting Agent:  5\n",
      "Rank-0: policy_step=55208, average_reward_env_0=114.6098808088457\n",
      "Collision detected between /World/Worker_3 and /World/Warehouse/Table_08\n",
      "Arrive at the goal! - agent:  2  - stage:  3\n",
      "Resetting Agent:  2\n",
      "Collide! - agent:  7  - stage:  3\n",
      "Resetting Agent:  7\n",
      "Rank-0: policy_step=55520, average_reward_env_0=116.19653654238274\n",
      "Collide! - agent:  5  - stage:  3\n",
      "Resetting Agent:  5\n",
      "Collide! - agent:  7  - stage:  3\n",
      "Resetting Agent:  7\n",
      "Rank-0: policy_step=55528, average_reward_env_0=95.13889376123592\n",
      "Arrive at the goal! - agent:  1  - stage:  3\n",
      "Resetting Agent:  1\n",
      "Arrive at the goal! - agent:  6  - stage:  3\n",
      "Resetting Agent:  6\n",
      "Arrive at the goal! - agent:  4  - stage:  3\n",
      "Resetting Agent:  4\n",
      "Collide! - agent:  8  - stage:  3\n",
      "Resetting Agent:  8\n",
      "Rank-0: policy_step=55840, average_reward_env_0=28.019687293791662\n",
      "Collide! - agent:  8  - stage:  3\n",
      "Resetting Agent:  8\n",
      "Arrive at the goal! - agent:  5  - stage:  3\n",
      "Resetting Agent:  5\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collide! - agent:  3  - stage:  3\n",
      "Resetting Agent:  3\n",
      "Rank-0: policy_step=55976, average_reward_env_0=201.60003253070332\n",
      "Collide! - agent:  3  - stage:  3\n",
      "Resetting Agent:  3\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Arrive at the goal! - agent:  2  - stage:  3\n",
      "Resetting Agent:  2\n",
      "Max step number - agent:  1\n",
      "Resetting Agent:  1\n",
      "Rank-0: policy_step=56096, average_reward_env_0=174.63099601381515\n",
      "Arrive at the goal! - agent:  4  - stage:  3\n",
      "Resetting Agent:  4\n",
      "Arrive at the goal! - agent:  6  - stage:  3\n",
      "Resetting Agent:  6\n",
      "Arrive at the goal! - agent:  8  - stage:  3\n",
      "Resetting Agent:  8\n",
      "Collision detected between /World/Worker_3 and /World/Worker_2\n",
      "Collision detected between /World/Worker_3 and /World/Worker_2\n",
      "Collision detected between /World/Worker_3 and /World/Worker_2\n",
      "Collision detected between /World/Worker_3 and /World/Worker_2\n",
      "Arrive at the goal! - agent:  5  - stage:  3\n",
      "Resetting Agent:  5\n",
      "Arrive at the goal! - agent:  1  - stage:  3\n",
      "Resetting Agent:  1\n",
      "Arrive at the goal! - agent:  2  - stage:  3\n",
      "Resetting Agent:  2\n",
      "Max step number - agent:  4\n",
      "Resetting Agent:  4\n",
      "Rank-0: policy_step=56728, average_reward_env_0=102.28915855115996\n",
      "Collision detected between /World/Worker_1 and /World/Warehouse/Table_03\n",
      "Arrive at the goal! - agent:  8  - stage:  3\n",
      "Resetting Agent:  8\n",
      "Arrive at the goal! - agent:  6  - stage:  3\n",
      "Resetting Agent:  6\n",
      "Arrive at the goal! - agent:  3  - stage:  3\n",
      "Resetting Agent:  3\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Max step number - agent:  2\n",
      "Resetting Agent:  2\n",
      "Rank-0: policy_step=56984, average_reward_env_0=149.00340597271307\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collide! - agent:  7  - stage:  3\n",
      "Resetting Agent:  7\n",
      "Rank-0: policy_step=57088, average_reward_env_0=103.97180014775394\n",
      "Collide! - agent:  7  - stage:  3\n",
      "Resetting Agent:  7\n",
      "Collision detected between /World/Worker_4 and /World/Warehouse/Warehouse/SM_FloorDecal_StripeFull_4m102/SM_FloorDecal_StripeFull_4m\n",
      "Collide! - agent:  7  - stage:  3\n",
      "Resetting Agent:  7\n",
      "Rank-0: policy_step=57216, average_reward_env_0=91.4626793441886\n",
      "Collision detected between /World/Worker_1 and /World/Warehouse/Table_08\n",
      "Collide! - agent:  4  - stage:  3\n",
      "Resetting Agent:  4\n",
      "Rank-0: policy_step=57432, average_reward_env_0=85.89196674892553\n",
      "Collide! - agent:  4  - stage:  3\n",
      "Resetting Agent:  4\n",
      "Max step number - agent:  6\n",
      "Resetting Agent:  6\n",
      "Rank-0: policy_step=57448, average_reward_env_0=143.50959265140665\n",
      "Collide! - agent:  1  - stage:  3\n",
      "Resetting Agent:  1\n",
      "Rank-0: policy_step=57472, average_reward_env_0=164.4371430681369\n",
      "Collide! - agent:  1  - stage:  3\n",
      "Resetting Agent:  1\n",
      "Collide! - agent:  6  - stage:  3\n",
      "Resetting Agent:  6\n",
      "Rank-0: policy_step=57528, average_reward_env_0=127.88092643854353\n",
      "Collide! - agent:  6  - stage:  3\n",
      "Resetting Agent:  6\n",
      "Collide! - agent:  7  - stage:  3\n",
      "Resetting Agent:  7\n",
      "Rank-0: policy_step=57560, average_reward_env_0=82.80923511604956\n",
      "Collide! - agent:  4  - stage:  3\n",
      "Resetting Agent:  4\n",
      "Rank-0: policy_step=57656, average_reward_env_0=76.37413612945153\n",
      "Collide! - agent:  3  - stage:  3\n",
      "Resetting Agent:  3\n",
      "Rank-0: policy_step=57680, average_reward_env_0=187.60695743234527\n",
      "Collide! - agent:  3  - stage:  3\n",
      "Resetting Agent:  3\n",
      "Arrive at the goal! - agent:  2  - stage:  3\n",
      "Resetting Agent:  2\n",
      "Collision detected between /World/Worker_4 and /World/Worker_1\n",
      "Collision detected between /World/Worker_4 and /World/Worker_1\n",
      "Collision detected between /World/Worker_4 and /World/Worker_1\n",
      "Collision detected between /World/Worker_4 and /World/Worker_1\n",
      "Arrive at the goal! - agent:  5  - stage:  3\n",
      "Resetting Agent:  5\n",
      "Max step number - agent:  5\n",
      "Resetting Agent:  5\n",
      "Rank-0: policy_step=57928, average_reward_env_0=126.21187954429827\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collide! - agent:  5  - stage:  3\n",
      "Resetting Agent:  5\n",
      "Rank-0: policy_step=58152, average_reward_env_0=113.89186141043774\n",
      "Arrive at the goal! - agent:  6  - stage:  3\n",
      "Resetting Agent:  6\n",
      "Collide! - agent:  6  - stage:  3\n",
      "Resetting Agent:  6\n",
      "Rank-0: policy_step=58216, average_reward_env_0=125.81039869369283\n",
      "Collide! - agent:  6  - stage:  3\n",
      "Resetting Agent:  6\n",
      "Max step number - agent:  8\n",
      "Resetting Agent:  8\n",
      "Rank-0: policy_step=58248, average_reward_env_0=46.508195638073055\n",
      "Collide! - agent:  3  - stage:  3\n",
      "Resetting Agent:  3\n",
      "Rank-0: policy_step=58264, average_reward_env_0=166.22727275862536\n",
      "Collision detected between /World/Worker_2 and /World/Warehouse/Warehouse/SM_FireExtinguisher_1929/SM_FireExtinguisher_02\n",
      "Collide! - agent:  3  - stage:  3\n",
      "Resetting Agent:  3\n",
      "Arrive at the goal! - agent:  1  - stage:  3\n",
      "Resetting Agent:  1\n",
      "Collide! - agent:  5  - stage:  3\n",
      "Resetting Agent:  5\n",
      "Rank-0: policy_step=58320, average_reward_env_1=102.69129680743644\n",
      "Collision detected between /World/Worker_1 and /World/Warehouse/Warehouse/SM_RackFrame_3840/SM_RackFrame_03/SM_RackFrame_03/Section1\n",
      "Collision detected between /World/Worker_1 and /World/Warehouse/Warehouse/SM_Rackshield_3841/SM_Rackshield_02\n",
      "Arrive at the goal! - agent:  7  - stage:  3\n",
      "Resetting Agent:  7\n",
      "Collide! - agent:  6  - stage:  3\n",
      "Resetting Agent:  6\n",
      "Rank-0: policy_step=58456, average_reward_env_0=114.02812432826728\n",
      "Collide! - agent:  6  - stage:  3\n",
      "Resetting Agent:  6\n",
      "Arrive at the goal! - agent:  2  - stage:  3\n",
      "Resetting Agent:  2\n",
      "Collide! - agent:  3  - stage:  3\n",
      "Resetting Agent:  3\n",
      "Rank-0: policy_step=58584, average_reward_env_0=149.7259153704056\n",
      "Collide! - agent:  3  - stage:  3\n",
      "Resetting Agent:  3\n",
      "Collide! - agent:  7  - stage:  3\n",
      "Resetting Agent:  7\n",
      "Rank-0: policy_step=58656, average_reward_env_0=83.32903148324208\n",
      "Collide! - agent:  7  - stage:  3\n",
      "Resetting Agent:  7\n",
      "Arrive at the goal! - agent:  8  - stage:  3\n",
      "Resetting Agent:  8\n",
      "Arrive at the goal! - agent:  5  - stage:  3\n",
      "Resetting Agent:  5\n",
      "Arrive at the goal! - agent:  4  - stage:  3\n",
      "Resetting Agent:  4\n",
      "Collide! - agent:  3  - stage:  3\n",
      "Resetting Agent:  3\n",
      "Rank-0: policy_step=58928, average_reward_env_0=133.75930979332304\n",
      "Collide! - agent:  3  - stage:  3\n",
      "Resetting Agent:  3\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Arrive at the goal! - agent:  6  - stage:  3\n",
      "Resetting Agent:  6\n",
      "Arrive at the goal! - agent:  2  - stage:  3\n",
      "Resetting Agent:  2\n",
      "Collide! - agent:  4  - stage:  3\n",
      "Resetting Agent:  4\n",
      "Rank-0: policy_step=59184, average_reward_env_0=78.44709583138602\n",
      "Arrive at the goal! - agent:  7  - stage:  3\n",
      "Resetting Agent:  7\n",
      "Collide! - agent:  3  - stage:  3\n",
      "Resetting Agent:  3\n",
      "Rank-0: policy_step=59336, average_reward_env_0=120.91694125777789\n",
      "Collide! - agent:  3  - stage:  3\n",
      "Resetting Agent:  3\n",
      "Collide! - agent:  6  - stage:  3\n",
      "Resetting Agent:  6\n",
      "Rank-0: policy_step=59344, average_reward_env_0=113.61912959599022\n",
      "Arrive at the goal! - agent:  1  - stage:  3\n",
      "Resetting Agent:  1\n",
      "Max step number - agent:  2\n",
      "Resetting Agent:  2\n",
      "Rank-0: policy_step=59384, average_reward_env_0=173.61231486536965\n",
      "Arrive at the goal! - agent:  5  - stage:  3\n",
      "Resetting Agent:  5\n",
      "Collide! - agent:  3  - stage:  3\n",
      "Resetting Agent:  3\n",
      "Rank-0: policy_step=59536, average_reward_env_0=108.53839739939122\n",
      "Collide! - agent:  3  - stage:  3\n",
      "Resetting Agent:  3\n",
      "Collide! - agent:  8  - stage:  3\n",
      "Resetting Agent:  8\n",
      "Rank-0: policy_step=59560, average_reward_env_0=47.377995580444065\n",
      "Collide! - agent:  8  - stage:  3\n",
      "Resetting Agent:  8\n",
      "Collide! - agent:  8  - stage:  3\n",
      "Resetting Agent:  8\n",
      "Rank-0: policy_step=59712, average_reward_env_0=41.38842848845413\n",
      "Collide! - agent:  8  - stage:  3\n",
      "Resetting Agent:  8\n",
      "Arrive at the goal! - agent:  6  - stage:  3\n",
      "Resetting Agent:  6\n",
      "Arrive at the goal! - agent:  4  - stage:  3\n",
      "Resetting Agent:  4\n",
      "Collide! - agent:  5  - stage:  3\n",
      "Resetting Agent:  5\n",
      "Rank-0: policy_step=59792, average_reward_env_0=114.01610296071973\n",
      "Collide! - agent:  2  - stage:  3\n",
      "Resetting Agent:  2\n",
      "Collide! - agent:  5  - stage:  3\n",
      "Resetting Agent:  5\n",
      "Rank-0: policy_step=59800, average_reward_env_0=157.5220812278962\n",
      "Collide! - agent:  2  - stage:  3\n",
      "Resetting Agent:  2\n",
      "Collide! - agent:  1  - stage:  3\n",
      "Resetting Agent:  1\n",
      "Collide! - agent:  3  - stage:  3\n",
      "Resetting Agent:  3\n",
      "Rank-0: policy_step=59856, average_reward_env_0=179.09671186967822\n",
      "Rank-0: policy_step=59856, average_reward_env_1=97.8646905836276\n",
      "Collide! - agent:  3  - stage:  3\n",
      "Resetting Agent:  3\n",
      "Collide! - agent:  7  - stage:  3\n",
      "Resetting Agent:  7\n",
      "Rank-0: policy_step=59920, average_reward_env_0=82.61895204740763\n",
      "Collide! - agent:  7  - stage:  3\n",
      "Resetting Agent:  7\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collide! - agent:  6  - stage:  3\n",
      "Resetting Agent:  6\n",
      "Rank-0: policy_step=60208, average_reward_env_0=114.56929144653004\n",
      "Collide! - agent:  6  - stage:  3\n",
      "Resetting Agent:  6\n",
      "Arrive at the goal! - agent:  3  - stage:  3\n",
      "Resetting Agent:  3\n",
      "Collide! - agent:  8  - stage:  3\n",
      "Resetting Agent:  8\n",
      "Rank-0: policy_step=60320, average_reward_env_0=29.797027615304263\n",
      "Collide! - agent:  8  - stage:  3\n",
      "Resetting Agent:  8\n",
      "Arrive at the goal! - agent:  4  - stage:  3\n",
      "Resetting Agent:  4\n",
      "Collide! - agent:  3  - stage:  3\n",
      "Resetting Agent:  3\n",
      "Rank-0: policy_step=60440, average_reward_env_0=98.50563932656178\n",
      "Collide! - agent:  3  - stage:  3\n",
      "Resetting Agent:  3\n",
      "Arrive at the goal! - agent:  1  - stage:  3\n",
      "Resetting Agent:  1\n",
      "Arrive at the goal! - agent:  2  - stage:  3\n",
      "Resetting Agent:  2\n",
      "Collide! - agent:  5  - stage:  3\n",
      "Resetting Agent:  5\n",
      "Rank-0: policy_step=60528, average_reward_env_0=103.34853062462278\n",
      "Collide! - agent:  8  - stage:  3\n",
      "Resetting Agent:  8\n",
      "Rank-0: policy_step=60536, average_reward_env_0=26.23783605390343\n",
      "Collide! - agent:  8  - stage:  3\n",
      "Resetting Agent:  8\n",
      "Collide! - agent:  4  - stage:  3\n",
      "Resetting Agent:  4\n",
      "Rank-0: policy_step=60624, average_reward_env_0=90.76910152324058\n",
      "Collide! - agent:  4  - stage:  3\n",
      "Resetting Agent:  4\n",
      "Arrive at the goal! - agent:  7  - stage:  3\n",
      "Resetting Agent:  7\n",
      "Collide! - agent:  8  - stage:  3\n",
      "Resetting Agent:  8\n",
      "Rank-0: policy_step=60736, average_reward_env_1=23.48418762643314\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collision detected between /World/Worker_3 and /World/Warehouse/Table_09\n",
      "Arrive at the goal! - agent:  1  - stage:  3\n",
      "Resetting Agent:  1\n",
      "Collide! - agent:  5  - stage:  3\n",
      "Resetting Agent:  5\n",
      "Rank-0: policy_step=61112, average_reward_env_0=95.8463349051108\n",
      "Collide! - agent:  5  - stage:  3\n",
      "Resetting Agent:  5\n",
      "Arrive at the goal! - agent:  3  - stage:  3\n",
      "Resetting Agent:  3\n",
      "Arrive at the goal! - agent:  6  - stage:  3\n",
      "Resetting Agent:  6\n",
      "Arrive at the goal! - agent:  4  - stage:  3\n",
      "Resetting Agent:  4\n",
      "Arrive at the goal! - agent:  7  - stage:  3\n",
      "Resetting Agent:  7\n",
      "Arrive at the goal! - agent:  2  - stage:  3\n",
      "Resetting Agent:  2\n",
      "Collide! - agent:  4  - stage:  3\n",
      "Resetting Agent:  4\n",
      "Rank-0: policy_step=61464, average_reward_env_0=92.24352295355294\n",
      "Collision detected between /World/Worker_3 and /World/Worker_2\n",
      "Collision detected between /World/Worker_3 and /World/Worker_2\n",
      "Collision detected between /World/Worker_3 and /World/Worker_2\n",
      "Collision detected between /World/Worker_3 and /World/Worker_2\n",
      "Collide! - agent:  2  - stage:  3\n",
      "Resetting Agent:  2\n",
      "Rank-0: policy_step=61696, average_reward_env_0=165.8544147782289\n",
      "Collide! - agent:  2  - stage:  3\n",
      "Resetting Agent:  2\n",
      "Arrive at the goal! - agent:  1  - stage:  3\n",
      "Resetting Agent:  1\n",
      "Collide! - agent:  3  - stage:  3\n",
      "Resetting Agent:  3\n",
      "Rank-0: policy_step=61824, average_reward_env_0=97.89225053396548\n",
      "Collide! - agent:  3  - stage:  3\n",
      "Resetting Agent:  3\n",
      "Arrive at the goal! - agent:  8  - stage:  3\n",
      "Resetting Agent:  8\n",
      "Arrive at the goal! - agent:  7  - stage:  3\n",
      "Resetting Agent:  7\n",
      "Collide! - agent:  2  - stage:  3\n",
      "Resetting Agent:  2\n",
      "Rank-0: policy_step=61920, average_reward_env_0=152.74279070392257\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collide! - agent:  5  - stage:  3\n",
      "Resetting Agent:  5\n",
      "Rank-0: policy_step=62088, average_reward_env_0=89.53558660943456\n",
      "Collide! - agent:  5  - stage:  3\n",
      "Resetting Agent:  5\n",
      "Arrive at the goal! - agent:  6  - stage:  3\n",
      "Resetting Agent:  6\n",
      "Max step number - agent:  1\n",
      "Resetting Agent:  1\n",
      "Rank-0: policy_step=62256, average_reward_env_0=199.49729504857962\n",
      "Collide! - agent:  8  - stage:  3\n",
      "Resetting Agent:  8\n",
      "Rank-0: policy_step=62264, average_reward_env_0=26.854457379065984\n",
      "Collide! - agent:  8  - stage:  3\n",
      "Resetting Agent:  8\n",
      "Collide! - agent:  1  - stage:  3\n",
      "Resetting Agent:  1\n",
      "Rank-0: policy_step=62296, average_reward_env_0=180.01938272417829\n",
      "Max step number - agent:  7\n",
      "Resetting Agent:  7\n",
      "Collide! - agent:  8  - stage:  3\n",
      "Resetting Agent:  8\n",
      "Rank-0: policy_step=62328, average_reward_env_0=110.06985851798838\n",
      "Rank-0: policy_step=62328, average_reward_env_1=23.62498615066225\n",
      "Collide! - agent:  1  - stage:  3\n",
      "Resetting Agent:  1\n",
      "Rank-0: policy_step=62344, average_reward_env_0=163.01439005320267\n",
      "Collide! - agent:  1  - stage:  3\n",
      "Resetting Agent:  1\n",
      "Arrive at the goal! - agent:  4  - stage:  3\n",
      "Resetting Agent:  4\n",
      "Collide! - agent:  1  - stage:  3\n",
      "Resetting Agent:  1\n",
      "Rank-0: policy_step=62488, average_reward_env_0=149.0994516868992\n",
      "Collide! - agent:  1  - stage:  3\n",
      "Resetting Agent:  1\n",
      "Max step number - agent:  6\n",
      "Resetting Agent:  6\n",
      "Rank-0: policy_step=62616, average_reward_env_0=128.13465719250738\n",
      "Arrive at the goal! - agent:  5  - stage:  3\n",
      "Resetting Agent:  5\n",
      "Collision detected between /World/Worker_4 and /World/Warehouse/Warehouse/SM_FloorDecal_StripeFull_4m102/SM_FloorDecal_StripeFull_4m\n",
      "Arrive at the goal! - agent:  3  - stage:  3\n",
      "Resetting Agent:  3\n",
      "Collide! - agent:  2  - stage:  3\n",
      "Resetting Agent:  2\n",
      "Rank-0: policy_step=62952, average_reward_env_0=137.8676732164492\n",
      "Collide! - agent:  2  - stage:  3\n",
      "Resetting Agent:  2\n",
      "Arrive at the goal! - agent:  8  - stage:  3\n",
      "Resetting Agent:  8\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Arrive at the goal! - agent:  5  - stage:  3\n",
      "Resetting Agent:  5\n",
      "Collide! - agent:  4  - stage:  3\n",
      "Resetting Agent:  4\n",
      "Rank-0: policy_step=63216, average_reward_env_0=90.89604269938182\n",
      "Collide! - agent:  4  - stage:  3\n",
      "Resetting Agent:  4\n",
      "Arrive at the goal! - agent:  7  - stage:  3\n",
      "Collide! - agent:  7  - stage:  3\n",
      "Resetting Agent:  7\n",
      "Rank-0: policy_step=63392, average_reward_env_0=97.88789888034037\n",
      "Arrive at the goal! - agent:  1  - stage:  3\n",
      "Resetting Agent:  1\n",
      "Arrive at the goal! - agent:  6  - stage:  3\n",
      "Resetting Agent:  6\n",
      "Arrive at the goal! - agent:  5  - stage:  3\n",
      "Resetting Agent:  5\n",
      "Arrive at the goal! - agent:  2  - stage:  3\n",
      "Resetting Agent:  2\n",
      "Arrive at the goal! - agent:  4  - stage:  3\n",
      "Resetting Agent:  4\n",
      "Collide! - agent:  3  - stage:  3\n",
      "Resetting Agent:  3\n",
      "Rank-0: policy_step=63784, average_reward_env_0=99.76602862957417\n",
      "Collide! - agent:  3  - stage:  3\n",
      "Resetting Agent:  3\n",
      "Arrive at the goal! - agent:  8  - stage:  3\n",
      "Resetting Agent:  8\n",
      "Collide! - agent:  8  - stage:  3\n",
      "Resetting Agent:  8\n",
      "Rank-0: policy_step=63856, average_reward_env_0=32.872759243733036\n",
      "Collide! - agent:  8  - stage:  3\n",
      "Resetting Agent:  8\n",
      "Collide! - agent:  1  - stage:  3\n",
      "Resetting Agent:  1\n",
      "Rank-0: policy_step=63928, average_reward_env_0=148.80512603280337\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collide! - agent:  2  - stage:  3\n",
      "Resetting Agent:  2\n",
      "Rank-0: policy_step=64136, average_reward_env_0=137.60016896895212\n",
      "Arrive at the goal! - agent:  7  - stage:  3\n",
      "Resetting Agent:  7\n",
      "Arrive at the goal! - agent:  6  - stage:  3\n",
      "Resetting Agent:  6\n",
      "Arrive at the goal! - agent:  5  - stage:  3\n",
      "Resetting Agent:  5\n",
      "Arrive at the goal! - agent:  4  - stage:  3\n",
      "Resetting Agent:  4\n",
      "Collide! - agent:  2  - stage:  3\n",
      "Resetting Agent:  2\n",
      "Rank-0: policy_step=64400, average_reward_env_0=127.62129918846244\n",
      "Collide! - agent:  2  - stage:  3\n",
      "Resetting Agent:  2\n",
      "Collide! - agent:  1  - stage:  3\n",
      "Resetting Agent:  1\n",
      "Rank-0: policy_step=64424, average_reward_env_0=134.59454700005762\n",
      "Collide! - agent:  1  - stage:  3\n",
      "Resetting Agent:  1\n",
      "Collide! - agent:  1  - stage:  3\n",
      "Resetting Agent:  1\n",
      "Rank-0: policy_step=64480, average_reward_env_0=123.03823169107581\n",
      "Collide! - agent:  1  - stage:  3\n",
      "Resetting Agent:  1\n",
      "Max step number - agent:  5\n",
      "Resetting Agent:  5\n",
      "Rank-0: policy_step=64496, average_reward_env_0=117.8756864586402\n",
      "Arrive at the goal! - agent:  8  - stage:  3\n",
      "Resetting Agent:  8\n",
      "Collide! - agent:  3  - stage:  3\n",
      "Resetting Agent:  3\n",
      "Rank-0: policy_step=64720, average_reward_env_0=91.97121102549458\n",
      "Max step number - agent:  7\n",
      "Resetting Agent:  7\n",
      "Rank-0: policy_step=64728, average_reward_env_0=97.22695632390264\n",
      "Collide! - agent:  3  - stage:  3\n",
      "Resetting Agent:  3\n",
      "Rank-0: policy_step=64832, average_reward_env_0=82.93524708424094\n",
      "Collide! - agent:  3  - stage:  3\n",
      "Resetting Agent:  3\n",
      "Arrive at the goal! - agent:  5  - stage:  3\n",
      "Resetting Agent:  5\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Arrive at the goal! - agent:  4  - stage:  3\n",
      "Resetting Agent:  4\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Max step number - agent:  6\n",
      "Resetting Agent:  6\n",
      "Rank-0: policy_step=65016, average_reward_env_0=138.70386879441352\n",
      "Collide! - agent:  7  - stage:  3\n",
      "Resetting Agent:  7\n",
      "Rank-0: policy_step=65056, average_reward_env_0=90.37095791103161\n",
      "Arrive at the goal! - agent:  1  - stage:  3\n",
      "Resetting Agent:  1\n",
      "Collide! - agent:  3  - stage:  3\n",
      "Resetting Agent:  3\n",
      "Rank-0: policy_step=65424, average_reward_env_0=78.15984638152862\n",
      "Arrive at the goal! - agent:  4  - stage:  3\n",
      "Resetting Agent:  4\n",
      "Collide! - agent:  6  - stage:  3\n",
      "Resetting Agent:  6\n",
      "Rank-0: policy_step=65488, average_reward_env_0=129.52522045616\n",
      "Collide! - agent:  6  - stage:  3\n",
      "Resetting Agent:  6\n",
      "Collide! - agent:  2  - stage:  3\n",
      "Resetting Agent:  2\n",
      "Rank-0: policy_step=65536, average_reward_env_0=120.15859014704098\n",
      "Collide! - agent:  2  - stage:  3\n",
      "Resetting Agent:  2\n",
      "Collide! - agent:  4  - stage:  3\n",
      "Resetting Agent:  4\n",
      "Rank-0: policy_step=65608, average_reward_env_0=116.34256507979505\n",
      "Arrive at the goal! - agent:  5  - stage:  3\n",
      "Resetting Agent:  5\n",
      "Arrive at the goal! - agent:  8  - stage:  3\n",
      "Resetting Agent:  8\n",
      "Arrive at the goal! - agent:  7  - stage:  3\n",
      "Resetting Agent:  7\n",
      "Arrive at the goal! - agent:  3  - stage:  3\n",
      "Resetting Agent:  3\n",
      "Collision detected between /World/Worker_3 and /World/Warehouse/Warehouse/GroundPlane/CollisionPlane\n",
      "Collide! - agent:  2  - stage:  3\n",
      "Resetting Agent:  2\n",
      "Rank-0: policy_step=65960, average_reward_env_0=113.2096840520383\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Arrive at the goal! - agent:  6  - stage:  3\n",
      "Resetting Agent:  6\n",
      "Arrive at the goal! - agent:  1  - stage:  3\n",
      "Resetting Agent:  1\n",
      "Collide! - agent:  4  - stage:  3\n",
      "Resetting Agent:  4\n",
      "Rank-0: policy_step=66224, average_reward_env_0=107.94866895690501\n",
      "Collide! - agent:  4  - stage:  3\n",
      "Resetting Agent:  4\n",
      "Collide! - agent:  7  - stage:  3\n",
      "Resetting Agent:  7\n",
      "Rank-0: policy_step=66256, average_reward_env_0=92.15851659528909\n",
      "Max step number - agent:  8\n",
      "Resetting Agent:  8\n",
      "Rank-0: policy_step=66264, average_reward_env_0=28.113125856434127\n",
      "Collide! - agent:  5  - stage:  3\n",
      "Resetting Agent:  5\n",
      "Rank-0: policy_step=66336, average_reward_env_0=126.32152096528321\n",
      "Collide! - agent:  4  - stage:  3\n",
      "Resetting Agent:  4\n",
      "Rank-0: policy_step=66352, average_reward_env_0=100.49577020320866\n",
      "Arrive at the goal! - agent:  7  - stage:  3\n",
      "Resetting Agent:  7\n",
      "Arrive at the goal! - agent:  2  - stage:  3\n",
      "Resetting Agent:  2\n",
      "Arrive at the goal! - agent:  1  - stage:  3\n",
      "Resetting Agent:  1\n",
      "Arrive at the goal! - agent:  3  - stage:  3\n",
      "Resetting Agent:  3\n",
      "Arrive at the goal! - agent:  4  - stage:  3\n",
      "Resetting Agent:  4\n",
      "Arrive at the goal! - agent:  5  - stage:  3\n",
      "Resetting Agent:  5\n",
      "Max step number - agent:  1\n",
      "Resetting Agent:  1\n",
      "Rank-0: policy_step=66888, average_reward_env_0=142.8475659091057\n",
      "Arrive at the goal! - agent:  6  - stage:  3\n",
      "Resetting Agent:  6\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collide! - agent:  3  - stage:  3\n",
      "Resetting Agent:  3\n",
      "Rank-0: policy_step=67096, average_reward_env_0=87.36493233479652\n",
      "Collide! - agent:  8  - stage:  3\n",
      "Resetting Agent:  8\n",
      "Rank-0: policy_step=67144, average_reward_env_0=29.478030678289098\n",
      "Collide! - agent:  8  - stage:  3\n",
      "Resetting Agent:  8\n",
      "Collide! - agent:  1  - stage:  3\n",
      "Resetting Agent:  1\n",
      "Rank-0: policy_step=67232, average_reward_env_0=134.22867209489254\n",
      "Arrive at the goal! - agent:  7  - stage:  3\n",
      "Resetting Agent:  7\n",
      "Arrive at the goal! - agent:  2  - stage:  3\n",
      "Resetting Agent:  2\n",
      "Collide! - agent:  1  - stage:  3\n",
      "Resetting Agent:  1\n",
      "Rank-0: policy_step=67488, average_reward_env_0=125.7754636457932\n",
      "Arrive at the goal! - agent:  8  - stage:  3\n",
      "Resetting Agent:  8\n",
      "Arrive at the goal! - agent:  3  - stage:  3\n",
      "Resetting Agent:  3\n",
      "Arrive at the goal! - agent:  7  - stage:  3\n",
      "Resetting Agent:  7\n",
      "Arrive at the goal! - agent:  5  - stage:  3\n",
      "Resetting Agent:  5\n",
      "Collide! - agent:  1  - stage:  3\n",
      "Resetting Agent:  1\n",
      "Rank-0: policy_step=67680, average_reward_env_0=116.5681460341677\n",
      "Collide! - agent:  8  - stage:  3\n",
      "Resetting Agent:  8\n",
      "Rank-0: policy_step=67736, average_reward_env_0=34.6222028578512\n",
      "Collide! - agent:  8  - stage:  3\n",
      "Resetting Agent:  8\n",
      "Collide! - agent:  8  - stage:  3\n",
      "Resetting Agent:  8\n",
      "Collide! - agent:  8  - stage:  3\n",
      "Resetting Agent:  8\n",
      "Max step number - agent:  6\n",
      "Resetting Agent:  6\n",
      "Rank-0: policy_step=67896, average_reward_env_0=141.87550685067404\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collide! - agent:  6  - stage:  3\n",
      "Resetting Agent:  6\n",
      "Rank-0: policy_step=68016, average_reward_env_0=133.209035724344\n",
      "Collide! - agent:  4  - stage:  3\n",
      "Resetting Agent:  4\n",
      "Collide! - agent:  6  - stage:  3\n",
      "Resetting Agent:  6\n",
      "Rank-0: policy_step=68024, average_reward_env_0=99.61429829779007\n",
      "Collide! - agent:  4  - stage:  3\n",
      "Resetting Agent:  4\n",
      "Arrive at the goal! - agent:  3  - stage:  3\n",
      "Resetting Agent:  3\n",
      "Collision detected between /World/Worker_1 and /World/Warehouse/Rack_02\n",
      "Arrive at the goal! - agent:  1  - stage:  3\n",
      "Resetting Agent:  1\n",
      "Collide! - agent:  2  - stage:  3\n",
      "Resetting Agent:  2\n",
      "Rank-0: policy_step=68248, average_reward_env_0=120.70159231469981\n",
      "Collide! - agent:  2  - stage:  3\n",
      "Resetting Agent:  2\n",
      "Collide! - agent:  4  - stage:  3\n",
      "Resetting Agent:  4\n",
      "Rank-0: policy_step=68256, average_reward_env_0=93.35638666196147\n",
      "Arrive at the goal! - agent:  8  - stage:  3\n",
      "Resetting Agent:  8\n",
      "Collide! - agent:  5  - stage:  3\n",
      "Resetting Agent:  5\n",
      "Rank-0: policy_step=68504, average_reward_env_0=133.75597018910264\n",
      "Collide! - agent:  5  - stage:  3\n",
      "Resetting Agent:  5\n",
      "Collide! - agent:  8  - stage:  3\n",
      "Resetting Agent:  8\n",
      "Rank-0: policy_step=68576, average_reward_env_0=41.567631037688805\n",
      "Collide! - agent:  8  - stage:  3\n",
      "Resetting Agent:  8\n",
      "Max step number - agent:  7\n",
      "Resetting Agent:  7\n",
      "Rank-0: policy_step=68656, average_reward_env_0=111.73190719764621\n",
      "Arrive at the goal! - agent:  6  - stage:  3\n",
      "Resetting Agent:  6\n",
      "Collide! - agent:  1  - stage:  3\n",
      "Resetting Agent:  1\n",
      "Rank-0: policy_step=68704, average_reward_env_0=116.24791937610972\n",
      "Collide! - agent:  1  - stage:  3\n",
      "Resetting Agent:  1\n",
      "Arrive at the goal! - agent:  2  - stage:  3\n",
      "Resetting Agent:  2\n",
      "Collide! - agent:  6  - stage:  3\n",
      "Resetting Agent:  6\n",
      "Rank-0: policy_step=68896, average_reward_env_0=132.59375864684174\n",
      "Collide! - agent:  4  - stage:  3\n",
      "Resetting Agent:  4\n",
      "Rank-0: policy_step=68928, average_reward_env_0=88.84488506429723\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Arrive at the goal! - agent:  8  - stage:  3\n",
      "Resetting Agent:  8\n",
      "Arrive at the goal! - agent:  3  - stage:  3\n",
      "Collide! - agent:  3  - stage:  3\n",
      "Resetting Agent:  3\n",
      "Rank-0: policy_step=69048, average_reward_env_0=101.61110328161365\n",
      "Collide! - agent:  4  - stage:  3\n",
      "Resetting Agent:  4\n",
      "Rank-0: policy_step=69072, average_reward_env_0=83.27334465926923\n",
      "Collide! - agent:  4  - stage:  3\n",
      "Resetting Agent:  4\n",
      "Arrive at the goal! - agent:  5  - stage:  3\n",
      "Resetting Agent:  5\n",
      "Collide! - agent:  2  - stage:  3\n",
      "Resetting Agent:  2\n",
      "Rank-0: policy_step=69208, average_reward_env_0=120.98079952611143\n",
      "Collide! - agent:  3  - stage:  3\n",
      "Resetting Agent:  3\n",
      "Rank-0: policy_step=69416, average_reward_env_0=113.5902596358021\n",
      "Collide! - agent:  7  - stage:  3\n",
      "Resetting Agent:  7\n",
      "Rank-0: policy_step=69440, average_reward_env_0=107.33405712714503\n",
      "Collide! - agent:  2  - stage:  3\n",
      "Resetting Agent:  2\n",
      "Rank-0: policy_step=69528, average_reward_env_0=114.37196141136612\n",
      "Collide! - agent:  7  - stage:  3\n",
      "Resetting Agent:  7\n",
      "Rank-0: policy_step=69552, average_reward_env_0=100.60110841400841\n",
      "Arrive at the goal! - agent:  8  - stage:  3\n",
      "Resetting Agent:  8\n",
      "Arrive at the goal! - agent:  1  - stage:  3\n",
      "Resetting Agent:  1\n",
      "Arrive at the goal! - agent:  6  - stage:  3\n",
      "Resetting Agent:  6\n",
      "Arrive at the goal! - agent:  4  - stage:  3\n",
      "Resetting Agent:  4\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Arrive at the goal! - agent:  2  - stage:  3\n",
      "Resetting Agent:  2\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Arrive at the goal! - agent:  8  - stage:  3\n",
      "Resetting Agent:  8\n",
      "Arrive at the goal! - agent:  7  - stage:  3\n",
      "Resetting Agent:  7\n",
      "Arrive at the goal! - agent:  3  - stage:  3\n",
      "Resetting Agent:  3\n",
      "Collide! - agent:  1  - stage:  3\n",
      "Resetting Agent:  1\n",
      "Rank-0: policy_step=70296, average_reward_env_0=117.43064384042084\n",
      "Collide! - agent:  5  - stage:  3\n",
      "Resetting Agent:  5\n",
      "Rank-0: policy_step=70384, average_reward_env_0=134.80361895562203\n",
      "Collide! - agent:  5  - stage:  3\n",
      "Resetting Agent:  5\n",
      "Arrive at the goal! - agent:  4  - stage:  3\n",
      "Resetting Agent:  4\n",
      "Arrive at the goal! - agent:  8  - stage:  3\n",
      "Resetting Agent:  8\n",
      "Arrive at the goal! - agent:  7  - stage:  3\n",
      "Resetting Agent:  7\n",
      "Collide! - agent:  8  - stage:  3\n",
      "Resetting Agent:  8\n",
      "Rank-0: policy_step=70768, average_reward_env_0=64.76324579507785\n",
      "Arrive at the goal! - agent:  6  - stage:  3\n",
      "Resetting Agent:  6\n",
      "Arrive at the goal! - agent:  1  - stage:  3\n",
      "Resetting Agent:  1\n",
      "Arrive at the goal! - agent:  4  - stage:  3\n",
      "Resetting Agent:  4\n",
      "Arrive at the goal! - agent:  2  - stage:  3\n",
      "Resetting Agent:  2\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Arrive at the goal! - agent:  3  - stage:  3\n",
      "Resetting Agent:  3\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Arrive at the goal! - agent:  7  - stage:  3\n",
      "Resetting Agent:  7\n",
      "Collide! - agent:  4  - stage:  3\n",
      "Resetting Agent:  4\n",
      "Collide! - agent:  7  - stage:  3\n",
      "Resetting Agent:  7\n",
      "Rank-0: policy_step=71192, average_reward_env_0=97.70918447891128\n",
      "Rank-0: policy_step=71192, average_reward_env_1=112.32917862158945\n",
      "Collide! - agent:  7  - stage:  3\n",
      "Resetting Agent:  7\n",
      "Arrive at the goal! - agent:  1  - stage:  3\n",
      "Resetting Agent:  1\n",
      "Max step number - agent:  6\n",
      "Resetting Agent:  6\n",
      "Rank-0: policy_step=71296, average_reward_env_0=140.10094634784687\n",
      "Arrive at the goal! - agent:  8  - stage:  3\n",
      "Resetting Agent:  8\n",
      "Collide! - agent:  1  - stage:  3\n",
      "Resetting Agent:  1\n",
      "Rank-0: policy_step=71360, average_reward_env_0=122.78782034688231\n",
      "Collide! - agent:  1  - stage:  3\n",
      "Resetting Agent:  1\n",
      "Arrive at the goal! - agent:  5  - stage:  3\n",
      "Resetting Agent:  5\n",
      "Arrive at the goal! - agent:  3  - stage:  3\n",
      "Resetting Agent:  3\n",
      "Arrive at the goal! - agent:  4  - stage:  3\n",
      "Resetting Agent:  4\n",
      "Arrive at the goal! - agent:  2  - stage:  3\n",
      "Resetting Agent:  2\n",
      "Collide! - agent:  1  - stage:  3\n",
      "Resetting Agent:  1\n",
      "Rank-0: policy_step=71672, average_reward_env_0=116.3833613715614\n",
      "Collide! - agent:  1  - stage:  3\n",
      "Resetting Agent:  1\n",
      "Arrive at the goal! - agent:  7  - stage:  3\n",
      "Resetting Agent:  7\n",
      "Max step number - agent:  3\n",
      "Resetting Agent:  3\n",
      "Collide! - agent:  5  - stage:  3\n",
      "Resetting Agent:  5\n",
      "Collide! - agent:  6  - stage:  3\n",
      "Resetting Agent:  6\n",
      "Rank-0: policy_step=71816, average_reward_env_0=128.48482788160237\n",
      "Rank-0: policy_step=71816, average_reward_env_1=135.03270674044276\n",
      "Rank-0: policy_step=71816, average_reward_env_2=133.73827663111643\n",
      "Collide! - agent:  6  - stage:  3\n",
      "Resetting Agent:  6\n",
      "Rank-0: policy_step=71888, average_reward_env_0=122.03182886424635\n",
      "Collide! - agent:  6  - stage:  3\n",
      "Resetting Agent:  6\n",
      "Max step number - agent:  2\n",
      "Resetting Agent:  2\n",
      "Rank-0: policy_step=71928, average_reward_env_0=129.29346177522493\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Arrive at the goal! - agent:  8  - stage:  3\n",
      "Resetting Agent:  8\n",
      "Arrive at the goal! - agent:  1  - stage:  3\n",
      "Resetting Agent:  1\n",
      "Collision detected between /World/Worker_1 and /World/Warehouse/Rack_04\n",
      "Collide! - agent:  5  - stage:  3\n",
      "Resetting Agent:  5\n",
      "Rank-0: policy_step=72080, average_reward_env_0=128.40528252202859\n",
      "Collide! - agent:  5  - stage:  3\n",
      "Resetting Agent:  5\n",
      "Arrive at the goal! - agent:  7  - stage:  3\n",
      "Resetting Agent:  7\n",
      "Collide! - agent:  3  - stage:  3\n",
      "Resetting Agent:  3\n",
      "Rank-0: policy_step=72304, average_reward_env_0=114.88920770126377\n",
      "Arrive at the goal! - agent:  2  - stage:  3\n",
      "Resetting Agent:  2\n",
      "Collide! - agent:  1  - stage:  3\n",
      "Resetting Agent:  1\n",
      "Collide! - agent:  3  - stage:  3\n",
      "Resetting Agent:  3\n",
      "Rank-0: policy_step=72488, average_reward_env_0=116.8456929179421\n",
      "Rank-0: policy_step=72488, average_reward_env_1=107.80345243280364\n",
      "Collide! - agent:  1  - stage:  3\n",
      "Resetting Agent:  1\n",
      "Arrive at the goal! - agent:  6  - stage:  3\n",
      "Resetting Agent:  6\n",
      "Collide! - agent:  8  - stage:  3\n",
      "Resetting Agent:  8\n",
      "Rank-0: policy_step=72616, average_reward_env_1=76.15199987405703\n",
      "Arrive at the goal! - agent:  4  - stage:  3\n",
      "Resetting Agent:  4\n",
      "Collide! - agent:  5  - stage:  3\n",
      "Resetting Agent:  5\n",
      "Rank-0: policy_step=72680, average_reward_env_0=121.92726522411536\n",
      "Arrive at the goal! - agent:  7  - stage:  3\n",
      "Resetting Agent:  7\n",
      "Collide! - agent:  2  - stage:  3\n",
      "Resetting Agent:  2\n",
      "Rank-0: policy_step=72864, average_reward_env_0=128.45962350755923\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Arrive at the goal! - agent:  4  - stage:  3\n",
      "Resetting Agent:  4\n",
      "Arrive at the goal! - agent:  3  - stage:  3\n",
      "Resetting Agent:  3\n",
      "Collide! - agent:  3  - stage:  3\n",
      "Resetting Agent:  3\n",
      "Rank-0: policy_step=73240, average_reward_env_0=109.35823253371909\n",
      "Arrive at the goal! - agent:  6  - stage:  3\n",
      "Resetting Agent:  6\n",
      "Arrive at the goal! - agent:  8  - stage:  3\n",
      "Resetting Agent:  8\n",
      "Arrive at the goal! - agent:  4  - stage:  3\n",
      "Resetting Agent:  4\n",
      "Arrive at the goal! - agent:  5  - stage:  3\n",
      "Resetting Agent:  5\n",
      "Max step number - agent:  4\n",
      "Resetting Agent:  4\n",
      "Rank-0: policy_step=73592, average_reward_env_0=110.09331340388931\n",
      "Collide! - agent:  4  - stage:  3\n",
      "Resetting Agent:  4\n",
      "Max step number - agent:  7\n",
      "Resetting Agent:  7\n",
      "Rank-0: policy_step=73600, average_reward_env_0=127.09721508534415\n",
      "Arrive at the goal! - agent:  1  - stage:  3\n",
      "Resetting Agent:  1\n",
      "Arrive at the goal! - agent:  2  - stage:  3\n",
      "Resetting Agent:  2\n",
      "Arrive at the goal! - agent:  3  - stage:  3\n",
      "Resetting Agent:  3\n",
      "Arrive at the goal! - agent:  6  - stage:  3\n",
      "Resetting Agent:  6\n",
      "Collide! - agent:  4  - stage:  3\n",
      "Resetting Agent:  4\n",
      "Rank-0: policy_step=73928, average_reward_env_0=111.00879144171215\n",
      "Collide! - agent:  4  - stage:  3\n",
      "Resetting Agent:  4\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collide! - agent:  7  - stage:  3\n",
      "Resetting Agent:  7\n",
      "Rank-0: policy_step=74080, average_reward_env_0=112.32679193162852\n",
      "Collide! - agent:  7  - stage:  3\n",
      "Resetting Agent:  7\n",
      "Arrive at the goal! - agent:  8  - stage:  3\n",
      "Resetting Agent:  8\n",
      "Collide! - agent:  6  - stage:  3\n",
      "Resetting Agent:  6\n",
      "Rank-0: policy_step=74176, average_reward_env_0=140.49898421710026\n",
      "Collide! - agent:  6  - stage:  3\n",
      "Resetting Agent:  6\n",
      "Arrive at the goal! - agent:  1  - stage:  3\n",
      "Resetting Agent:  1\n",
      "Arrive at the goal! - agent:  2  - stage:  3\n",
      "Resetting Agent:  2\n",
      "Arrive at the goal! - agent:  3  - stage:  3\n",
      "Resetting Agent:  3\n",
      "Arrive at the goal! - agent:  5  - stage:  3\n",
      "Resetting Agent:  5\n",
      "Collide! - agent:  7  - stage:  3\n",
      "Resetting Agent:  7\n",
      "Rank-0: policy_step=74432, average_reward_env_0=100.81958568762089\n",
      "Collide! - agent:  7  - stage:  3\n",
      "Resetting Agent:  7\n",
      "Arrive at the goal! - agent:  4  - stage:  3\n",
      "Resetting Agent:  4\n",
      "Collide! - agent:  7  - stage:  3\n",
      "Resetting Agent:  7\n",
      "Rank-0: policy_step=74480, average_reward_env_1=79.68283370678411\n",
      "Arrive at the goal! - agent:  6  - stage:  3\n",
      "Resetting Agent:  6\n",
      "Arrive at the goal! - agent:  8  - stage:  3\n",
      "Resetting Agent:  8\n",
      "Collide! - agent:  1  - stage:  3\n",
      "Max step number - agent:  1\n",
      "Resetting Agent:  1\n",
      "Rank-0: policy_step=74896, average_reward_env_0=126.31982305711134\n",
      "Collide! - agent:  1  - stage:  3\n",
      "Resetting Agent:  1\n",
      "Arrive at the goal! - agent:  3  - stage:  3\n",
      "Resetting Agent:  3\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Judging Increasing\n",
      ">>>>>>>>>>>>>>>Increasing Difficulty>>>>>>>>>>>>>>>>>>>\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collide! - agent:  5  - stage:  4\n",
      "Resetting Agent:  5\n",
      "Rank-0: policy_step=75104, average_reward_env_0=283.3600271964813\n",
      "Collide! - agent:  3  - stage:  4\n",
      "Resetting Agent:  3\n",
      "Rank-0: policy_step=75120, average_reward_env_0=372.99924256457564\n",
      "Collide! - agent:  1  - stage:  4\n",
      "Resetting Agent:  1\n",
      "Rank-0: policy_step=75136, average_reward_env_0=-49.052710493781255\n",
      "Collision detected between /World/Worker_2 and /World/Warehouse/Warehouse/GroundPlane/CollisionPlane\n",
      "Arrive at the goal! - agent:  6  - stage:  4\n",
      "Resetting Agent:  6\n",
      "Arrive at the goal! - agent:  2  - stage:  4\n",
      "Resetting Agent:  2\n",
      "Collide! - agent:  5  - stage:  4\n",
      "Resetting Agent:  5\n",
      "Rank-0: policy_step=75312, average_reward_env_0=118.18726444955391\n",
      "Collide! - agent:  5  - stage:  4\n",
      "Resetting Agent:  5\n",
      "Collide! - agent:  3  - stage:  4\n",
      "Resetting Agent:  3\n",
      "Rank-0: policy_step=75328, average_reward_env_0=170.9076058825175\n",
      "Collide! - agent:  3  - stage:  4\n",
      "Resetting Agent:  3\n",
      "Max step number - agent:  8\n",
      "Resetting Agent:  8\n",
      "Rank-0: policy_step=75416, average_reward_env_0=490.1849054161854\n",
      "Arrive at the goal! - agent:  7  - stage:  4\n",
      "Resetting Agent:  7\n",
      "Max step number - agent:  2\n",
      "Resetting Agent:  2\n",
      "Rank-0: policy_step=75664, average_reward_env_0=523.6530858750656\n",
      "Collide! - agent:  4  - stage:  4\n",
      "Resetting Agent:  4\n",
      "Rank-0: policy_step=75800, average_reward_env_0=148.10873302224763\n",
      "Collide! - agent:  5  - stage:  4\n",
      "Resetting Agent:  5\n",
      "Rank-0: policy_step=75808, average_reward_env_0=67.99176922631281\n",
      "Collide! - agent:  5  - stage:  4\n",
      "Resetting Agent:  5\n",
      "Collide! - agent:  7  - stage:  4\n",
      "Resetting Agent:  7\n",
      "Rank-0: policy_step=75840, average_reward_env_0=132.21981335637403\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collide! - agent:  4  - stage:  4\n",
      "Resetting Agent:  4\n",
      "Rank-0: policy_step=75984, average_reward_env_0=53.81356725200097\n",
      "Collide! - agent:  4  - stage:  4\n",
      "Resetting Agent:  4\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Arrive at the goal! - agent:  6  - stage:  4\n",
      "Resetting Agent:  6\n",
      "Arrive at the goal! - agent:  1  - stage:  4\n",
      "Resetting Agent:  1\n",
      "Arrive at the goal! - agent:  7  - stage:  4\n",
      "Resetting Agent:  7\n",
      "Collide! - agent:  8  - stage:  4\n",
      "Resetting Agent:  8\n",
      "Rank-0: policy_step=76488, average_reward_env_1=222.0787996056551\n",
      "Collide! - agent:  8  - stage:  4\n",
      "Resetting Agent:  8\n",
      "Arrive at the goal! - agent:  2  - stage:  4\n",
      "Resetting Agent:  2\n",
      "Collide! - agent:  4  - stage:  4\n",
      "Resetting Agent:  4\n",
      "Rank-0: policy_step=76616, average_reward_env_0=34.905268135981004\n",
      "Collide! - agent:  4  - stage:  4\n",
      "Resetting Agent:  4\n",
      "Collision detected between /World/Worker_2 and /World/Worker_1\n",
      "Collision detected between /World/Worker_2 and /World/Worker_1\n",
      "Collision detected between /World/Worker_2 and /World/Worker_1\n",
      "Collision detected between /World/Worker_2 and /World/Worker_1\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Arrive at the goal! - agent:  1  - stage:  4\n",
      "Resetting Agent:  1\n",
      "Arrive at the goal! - agent:  6  - stage:  4\n",
      "Max step number - agent:  6\n",
      "Resetting Agent:  6\n",
      "Rank-0: policy_step=76984, average_reward_env_0=657.8087039532157\n",
      "Max step number - agent:  6\n",
      "Resetting Agent:  6\n",
      "Arrive at the goal! - agent:  8  - stage:  4\n",
      "Resetting Agent:  8\n",
      "Rank-0: policy_step=76992, average_reward_env_0=657.834995708122\n",
      "Collision detected between /World/Worker_2 and /World/Warehouse/Table_02\n",
      "Collision detected between /World/Worker_2 and /World/Warehouse/Table_02\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Arrive at the goal! - agent:  5  - stage:  4\n",
      "Resetting Agent:  5\n",
      "Collide! - agent:  7  - stage:  4\n",
      "Resetting Agent:  7\n",
      "Rank-0: policy_step=77352, average_reward_env_0=132.22769418963532\n",
      "Collide! - agent:  7  - stage:  4\n",
      "Resetting Agent:  7\n",
      "Arrive at the goal! - agent:  4  - stage:  4\n",
      "Resetting Agent:  4\n",
      "Arrive at the goal! - agent:  3  - stage:  4\n",
      "Resetting Agent:  3\n",
      "Collide! - agent:  2  - stage:  4\n",
      "Resetting Agent:  2\n",
      "Rank-0: policy_step=77464, average_reward_env_0=298.028804797049\n",
      "Collide! - agent:  2  - stage:  4\n",
      "Resetting Agent:  2\n",
      "Arrive at the goal! - agent:  6  - stage:  4\n",
      "Resetting Agent:  6\n",
      "Arrive at the goal! - agent:  1  - stage:  4\n",
      "Resetting Agent:  1\n",
      "Max step number - agent:  1\n",
      "Resetting Agent:  1\n",
      "Rank-0: policy_step=77936, average_reward_env_0=234.30286749246775\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Arrive at the goal! - agent:  2  - stage:  4\n",
      "Resetting Agent:  2\n",
      "Collide! - agent:  5  - stage:  4\n",
      "Resetting Agent:  5\n",
      "Rank-0: policy_step=78088, average_reward_env_0=73.41894976661791\n",
      "Collide! - agent:  5  - stage:  4\n",
      "Resetting Agent:  5\n",
      "Max step number - agent:  3\n",
      "Resetting Agent:  3\n",
      "Rank-0: policy_step=78136, average_reward_env_0=177.1459360769145\n",
      "Collide! - agent:  3  - stage:  4\n",
      "Resetting Agent:  3\n",
      "Rank-0: policy_step=78216, average_reward_env_0=119.96970924309603\n",
      "Collide! - agent:  3  - stage:  4\n",
      "Resetting Agent:  3\n",
      "Collide! - agent:  2  - stage:  4\n",
      "Resetting Agent:  2\n",
      "Rank-0: policy_step=78256, average_reward_env_0=239.81981076320494\n",
      "Collide! - agent:  2  - stage:  4\n",
      "Resetting Agent:  2\n",
      "Collide! - agent:  4  - stage:  4\n",
      "Resetting Agent:  4\n",
      "Rank-0: policy_step=78312, average_reward_env_0=76.12640999513114\n",
      "Collide! - agent:  6  - stage:  4\n",
      "Resetting Agent:  6\n",
      "Rank-0: policy_step=78360, average_reward_env_0=498.96281129967383\n",
      "Collide! - agent:  6  - stage:  4\n",
      "Resetting Agent:  6\n",
      "Collide! - agent:  7  - stage:  4\n",
      "Resetting Agent:  7\n",
      "Rank-0: policy_step=78448, average_reward_env_0=-21.13390244562305\n",
      "Collide! - agent:  7  - stage:  4\n",
      "Resetting Agent:  7\n",
      "Collide! - agent:  8  - stage:  4\n",
      "Resetting Agent:  8\n",
      "Rank-0: policy_step=78480, average_reward_env_0=212.16934943249677\n",
      "Collide! - agent:  8  - stage:  4\n",
      "Resetting Agent:  8\n",
      "Collide! - agent:  3  - stage:  4\n",
      "Resetting Agent:  3\n",
      "Rank-0: policy_step=78816, average_reward_env_0=99.03833640876724\n",
      "Collide! - agent:  3  - stage:  4\n",
      "Resetting Agent:  3\n",
      "Arrive at the goal! - agent:  2  - stage:  4\n",
      "Resetting Agent:  2\n",
      "Arrive at the goal! - agent:  4  - stage:  4\n",
      "Resetting Agent:  4\n",
      "Arrive at the goal! - agent:  5  - stage:  4\n",
      "Resetting Agent:  5\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collide! - agent:  7  - stage:  4\n",
      "Resetting Agent:  7\n",
      "Rank-0: policy_step=79432, average_reward_env_0=-13.010294435005703\n",
      "Collide! - agent:  7  - stage:  4\n",
      "Resetting Agent:  7\n",
      "Arrive at the goal! - agent:  6  - stage:  4\n",
      "Resetting Agent:  6\n",
      "Arrive at the goal! - agent:  1  - stage:  4\n",
      "Resetting Agent:  1\n",
      "Arrive at the goal! - agent:  5  - stage:  4\n",
      "Resetting Agent:  5\n",
      "Arrive at the goal! - agent:  8  - stage:  4\n",
      "Resetting Agent:  8\n",
      "Collide! - agent:  1  - stage:  4\n",
      "Resetting Agent:  1\n",
      "Rank-0: policy_step=79888, average_reward_env_0=206.35313338373047\n",
      "Collide! - agent:  1  - stage:  4\n",
      "Resetting Agent:  1\n",
      "Arrive at the goal! - agent:  4  - stage:  4\n",
      "Resetting Agent:  4\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collide! - agent:  4  - stage:  4\n",
      "Resetting Agent:  4\n",
      "Rank-0: policy_step=80000, average_reward_env_0=117.38072582246818\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collide! - agent:  4  - stage:  4\n",
      "Resetting Agent:  4\n",
      "Collide! - agent:  4  - stage:  4\n",
      "Resetting Agent:  4\n",
      "Collide! - agent:  4  - stage:  4\n",
      "Resetting Agent:  4\n",
      "Rank-0: policy_step=80192, average_reward_env_0=89.73213302463745\n",
      "Collide! - agent:  4  - stage:  4\n",
      "Resetting Agent:  4\n",
      "Collide! - agent:  7  - stage:  4\n",
      "Resetting Agent:  7\n",
      "Rank-0: policy_step=80640, average_reward_env_0=-15.642547323623347\n",
      "Collide! - agent:  7  - stage:  4\n",
      "Resetting Agent:  7\n",
      "Arrive at the goal! - agent:  8  - stage:  4\n",
      "Resetting Agent:  8\n",
      "Collide! - agent:  4  - stage:  4\n",
      "Resetting Agent:  4\n",
      "Rank-0: policy_step=80728, average_reward_env_0=75.63063341381023\n",
      "Collide! - agent:  4  - stage:  4\n",
      "Resetting Agent:  4\n",
      "Collision detected between /World/Worker_4 and /World/Warehouse/Rack_04\n",
      "Collision detected between /World/Worker_3 and /World/Warehouse/Table_08\n",
      "Collide! - agent:  5  - stage:  4\n",
      "Resetting Agent:  5\n",
      "Rank-0: policy_step=80848, average_reward_env_0=121.44461784569208\n",
      "Collide! - agent:  5  - stage:  4\n",
      "Resetting Agent:  5\n",
      "Collide! - agent:  4  - stage:  4\n",
      "Resetting Agent:  4\n",
      "Rank-0: policy_step=80864, average_reward_env_0=61.34136319685111\n",
      "Collide! - agent:  4  - stage:  4\n",
      "Resetting Agent:  4\n",
      "Arrive at the goal! - agent:  1  - stage:  4\n",
      "Resetting Agent:  1\n",
      "Arrive at the goal! - agent:  3  - stage:  4\n",
      "Resetting Agent:  3\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Arrive at the goal! - agent:  6  - stage:  4\n",
      "Resetting Agent:  6\n",
      "Collide! - agent:  1  - stage:  4\n",
      "Resetting Agent:  1\n",
      "Rank-0: policy_step=81040, average_reward_env_0=190.53360633033788\n",
      "Max step number - agent:  2\n",
      "Resetting Agent:  2\n",
      "Rank-0: policy_step=81064, average_reward_env_0=252.2987274569047\n",
      "Max step number - agent:  6\n",
      "Resetting Agent:  6\n",
      "Rank-0: policy_step=81168, average_reward_env_0=476.60792431154823\n",
      "Collide! - agent:  1  - stage:  4\n",
      "Resetting Agent:  1\n",
      "Rank-0: policy_step=81224, average_reward_env_0=144.04231287263497\n",
      "Collide! - agent:  1  - stage:  4\n",
      "Resetting Agent:  1\n",
      "Max step number - agent:  8\n",
      "Resetting Agent:  8\n",
      "Rank-0: policy_step=81288, average_reward_env_0=255.57222446819577\n",
      "Collide! - agent:  8  - stage:  4\n",
      "Resetting Agent:  8\n",
      "Rank-0: policy_step=81432, average_reward_env_0=194.72641499301463\n",
      "Collide! - agent:  5  - stage:  4\n",
      "Resetting Agent:  5\n",
      "Rank-0: policy_step=81472, average_reward_env_0=89.21160420178016\n",
      "Collide! - agent:  5  - stage:  4\n",
      "Resetting Agent:  5\n",
      "Collide! - agent:  6  - stage:  4\n",
      "Resetting Agent:  6\n",
      "Rank-0: policy_step=81488, average_reward_env_0=374.2802157322077\n",
      "Collide! - agent:  6  - stage:  4\n",
      "Resetting Agent:  6\n",
      "Collide! - agent:  1  - stage:  4\n",
      "Resetting Agent:  1\n",
      "Rank-0: policy_step=81528, average_reward_env_0=114.811424497438\n",
      "Collide! - agent:  1  - stage:  4\n",
      "Resetting Agent:  1\n",
      "Max step number - agent:  3\n",
      "Resetting Agent:  3\n",
      "Rank-0: policy_step=81624, average_reward_env_0=111.9130686300349\n",
      "Collide! - agent:  1  - stage:  4\n",
      "Resetting Agent:  1\n",
      "Rank-0: policy_step=81792, average_reward_env_0=90.92002340634238\n",
      "Collide! - agent:  1  - stage:  4\n",
      "Resetting Agent:  1\n",
      "Collide! - agent:  7  - stage:  4\n",
      "Resetting Agent:  7\n",
      "Rank-0: policy_step=81920, average_reward_env_0=-11.960760082898853\n",
      "Collide! - agent:  1  - stage:  4\n",
      "Resetting Agent:  1\n",
      "Rank-0: policy_step=81936, average_reward_env_0=74.6970604994491\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Arrive at the goal! - agent:  2  - stage:  4\n",
      "Resetting Agent:  2\n",
      "Collide! - agent:  1  - stage:  4\n",
      "Resetting Agent:  1\n",
      "Arrive at the goal! - agent:  4  - stage:  4\n",
      "Resetting Agent:  4\n",
      "Rank-0: policy_step=82128, average_reward_env_0=60.6369047613604\n",
      "Collide! - agent:  1  - stage:  4\n",
      "Resetting Agent:  1\n",
      "Collide! - agent:  6  - stage:  4\n",
      "Resetting Agent:  6\n",
      "Rank-0: policy_step=82152, average_reward_env_0=312.8131176237492\n",
      "Arrive at the goal! - agent:  3  - stage:  4\n",
      "Resetting Agent:  3\n",
      "Collide! - agent:  7  - stage:  4\n",
      "Resetting Agent:  7\n",
      "Rank-0: policy_step=82424, average_reward_env_0=-13.449156282733465\n",
      "Collide! - agent:  7  - stage:  4\n",
      "Resetting Agent:  7\n",
      "Collide! - agent:  1  - stage:  4\n",
      "Resetting Agent:  1\n",
      "Rank-0: policy_step=82440, average_reward_env_0=51.557652463526644\n",
      "Collide! - agent:  1  - stage:  4\n",
      "Resetting Agent:  1\n",
      "Arrive at the goal! - agent:  8  - stage:  4\n",
      "Resetting Agent:  8\n",
      "Collide! - agent:  3  - stage:  4\n",
      "Resetting Agent:  3\n",
      "Rank-0: policy_step=82504, average_reward_env_0=115.05285496698566\n",
      "Collide! - agent:  1  - stage:  4\n",
      "Resetting Agent:  1\n",
      "Rank-0: policy_step=82688, average_reward_env_0=42.671204366487544\n",
      "Collide! - agent:  6  - stage:  4\n",
      "Resetting Agent:  6\n",
      "Rank-0: policy_step=82720, average_reward_env_0=268.419472580191\n",
      "Collide! - agent:  3  - stage:  4\n",
      "Resetting Agent:  3\n",
      "Rank-0: policy_step=82744, average_reward_env_0=96.4446186871471\n",
      "Collide! - agent:  3  - stage:  4\n",
      "Resetting Agent:  3\n",
      "Collide! - agent:  7  - stage:  4\n",
      "Resetting Agent:  7\n",
      "Rank-0: policy_step=82816, average_reward_env_0=-17.036952076969563\n",
      "Collide! - agent:  2  - stage:  4\n",
      "Resetting Agent:  2\n",
      "Rank-0: policy_step=82840, average_reward_env_0=235.95980132561257\n",
      "Collide! - agent:  2  - stage:  4\n",
      "Resetting Agent:  2\n",
      "Arrive at the goal! - agent:  5  - stage:  4\n",
      "Resetting Agent:  5\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Arrive at the goal! - agent:  4  - stage:  4\n",
      "Resetting Agent:  4\n",
      "Collide! - agent:  6  - stage:  4\n",
      "Resetting Agent:  6\n",
      "Rank-0: policy_step=83072, average_reward_env_0=232.97304457563516\n",
      "Arrive at the goal! - agent:  8  - stage:  4\n",
      "Resetting Agent:  8\n",
      "Collision detected between /World/Worker_1 and /World/Warehouse/Warehouse/GroundPlane/CollisionPlane\n",
      "Collision detected between /World/Worker_2 and /World/Warehouse/Table_08\n",
      "Arrive at the goal! - agent:  3  - stage:  4\n",
      "Resetting Agent:  3\n",
      "Arrive at the goal! - agent:  5  - stage:  4\n",
      "Resetting Agent:  5\n",
      "Arrive at the goal! - agent:  4  - stage:  4\n",
      "Resetting Agent:  4\n",
      "Max step number - agent:  4\n",
      "Resetting Agent:  4\n",
      "Rank-0: policy_step=83672, average_reward_env_0=112.14276361676076\n",
      "Arrive at the goal! - agent:  2  - stage:  4\n",
      "Resetting Agent:  2\n",
      "Arrive at the goal! - agent:  6  - stage:  4\n",
      "Resetting Agent:  6\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collide! - agent:  7  - stage:  4\n",
      "Resetting Agent:  7\n",
      "Rank-0: policy_step=84080, average_reward_env_0=-31.8809731525764\n",
      "Collide! - agent:  7  - stage:  4\n",
      "Resetting Agent:  7\n",
      "Arrive at the goal! - agent:  1  - stage:  4\n",
      "Resetting Agent:  1\n",
      "Collide! - agent:  3  - stage:  4\n",
      "Resetting Agent:  3\n",
      "Rank-0: policy_step=84128, average_reward_env_0=95.04028619493253\n",
      "Collide! - agent:  3  - stage:  4\n",
      "Resetting Agent:  3\n",
      "Collision detected between /World/Worker_4 and /World/Warehouse/Table_12\n",
      "Max step number - agent:  8\n",
      "Resetting Agent:  8\n",
      "Rank-0: policy_step=84232, average_reward_env_0=233.05536322985168\n",
      "Max step number - agent:  5\n",
      "Resetting Agent:  5\n",
      "Rank-0: policy_step=84280, average_reward_env_0=134.4980030920486\n",
      "Collide! - agent:  3  - stage:  4\n",
      "Resetting Agent:  3\n",
      "Rank-0: policy_step=84296, average_reward_env_0=81.3734760070816\n",
      "Collide! - agent:  3  - stage:  4\n",
      "Resetting Agent:  3\n",
      "Collision detected between /World/Worker_4 and /World/Worker_3\n",
      "Collision detected between /World/Worker_4 and /World/Worker_3\n",
      "Arrive at the goal! - agent:  2  - stage:  4\n",
      "Resetting Agent:  2\n",
      "Arrive at the goal! - agent:  4  - stage:  4\n",
      "Resetting Agent:  4\n",
      "Collision detected between /World/Worker_2 and /World/Warehouse/Warehouse/SM_Rackshield_2984/SM_Rackshield_02\n",
      "Collide! - agent:  6  - stage:  4\n",
      "Resetting Agent:  6\n",
      "Rank-0: policy_step=84816, average_reward_env_0=230.67488085292442\n",
      "Collide! - agent:  4  - stage:  4\n",
      "Resetting Agent:  4\n",
      "Rank-0: policy_step=84904, average_reward_env_0=114.27724874098844\n",
      "Arrive at the goal! - agent:  7  - stage:  4\n",
      "Resetting Agent:  7\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Arrive at the goal! - agent:  1  - stage:  4\n",
      "Resetting Agent:  1\n",
      "Arrive at the goal! - agent:  8  - stage:  4\n",
      "Resetting Agent:  8\n",
      "Collide! - agent:  7  - stage:  4\n",
      "Resetting Agent:  7\n",
      "Rank-0: policy_step=85096, average_reward_env_0=-15.240279847759325\n",
      "Arrive at the goal! - agent:  2  - stage:  4\n",
      "Resetting Agent:  2\n",
      "Arrive at the goal! - agent:  3  - stage:  4\n",
      "Resetting Agent:  3\n",
      "Collide! - agent:  2  - stage:  4\n",
      "Resetting Agent:  2\n",
      "Rank-0: policy_step=85256, average_reward_env_0=274.1720875101372\n",
      "Collide! - agent:  4  - stage:  4\n",
      "Resetting Agent:  4\n",
      "Rank-0: policy_step=85392, average_reward_env_0=101.08879036492807\n",
      "Collide! - agent:  4  - stage:  4\n",
      "Resetting Agent:  4\n",
      "Max step number - agent:  1\n",
      "Resetting Agent:  1\n",
      "Rank-0: policy_step=85488, average_reward_env_0=61.06194530816146\n",
      "Collide! - agent:  1  - stage:  4\n",
      "Resetting Agent:  1\n",
      "Arrive at the goal! - agent:  6  - stage:  4\n",
      "Resetting Agent:  6\n",
      "Collide! - agent:  7  - stage:  4\n",
      "Resetting Agent:  7\n",
      "Rank-0: policy_step=85712, average_reward_env_0=-16.085559046972392\n",
      "Collide! - agent:  7  - stage:  4\n",
      "Resetting Agent:  7\n",
      "Arrive at the goal! - agent:  8  - stage:  4\n",
      "Resetting Agent:  8\n",
      "Collide! - agent:  5  - stage:  4\n",
      "Resetting Agent:  5\n",
      "Rank-0: policy_step=85920, average_reward_env_0=118.83144093489145\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collide! - agent:  1  - stage:  4\n",
      "Resetting Agent:  1\n",
      "Rank-0: policy_step=86120, average_reward_env_0=56.287936067257775\n",
      "Collide! - agent:  3  - stage:  4\n",
      "Resetting Agent:  3\n",
      "Rank-0: policy_step=86192, average_reward_env_0=93.16313849343163\n",
      "Collide! - agent:  5  - stage:  4\n",
      "Resetting Agent:  5\n",
      "Rank-0: policy_step=86200, average_reward_env_0=102.05312482681767\n",
      "Collide! - agent:  5  - stage:  4\n",
      "Resetting Agent:  5\n",
      "Arrive at the goal! - agent:  2  - stage:  4\n",
      "Resetting Agent:  2\n",
      "Collide! - agent:  2  - stage:  4\n",
      "Resetting Agent:  2\n",
      "Rank-0: policy_step=86368, average_reward_env_0=251.4186208369347\n",
      "Arrive at the goal! - agent:  4  - stage:  4\n",
      "Resetting Agent:  4\n",
      "Collide! - agent:  7  - stage:  4\n",
      "Resetting Agent:  7\n",
      "Rank-0: policy_step=86424, average_reward_env_0=-15.071911741836248\n",
      "Collide! - agent:  4  - stage:  4\n",
      "Resetting Agent:  4\n",
      "Rank-0: policy_step=86608, average_reward_env_0=98.57026858610851\n",
      "Collide! - agent:  4  - stage:  4\n",
      "Resetting Agent:  4\n",
      "Collide! - agent:  5  - stage:  4\n",
      "Resetting Agent:  5\n",
      "Collide! - agent:  7  - stage:  4\n",
      "Resetting Agent:  7\n",
      "Rank-0: policy_step=86656, average_reward_env_0=88.99314489561206\n",
      "Rank-0: policy_step=86656, average_reward_env_1=-17.595933257494572\n",
      "Collide! - agent:  5  - stage:  4\n",
      "Resetting Agent:  5\n",
      "Collide! - agent:  7  - stage:  4\n",
      "Resetting Agent:  7\n",
      "Collide! - agent:  7  - stage:  4\n",
      "Resetting Agent:  7\n",
      "Rank-0: policy_step=86712, average_reward_env_0=-20.15466483548078\n",
      "Collide! - agent:  7  - stage:  4\n",
      "Resetting Agent:  7\n",
      "Arrive at the goal! - agent:  8  - stage:  4\n",
      "Resetting Agent:  8\n",
      "Arrive at the goal! - agent:  6  - stage:  4\n",
      "Resetting Agent:  6\n",
      "Collide! - agent:  6  - stage:  4\n",
      "Resetting Agent:  6\n",
      "Collide! - agent:  7  - stage:  4\n",
      "Resetting Agent:  7\n",
      "Rank-0: policy_step=86792, average_reward_env_0=237.0519038022203\n",
      "Rank-0: policy_step=86792, average_reward_env_1=-22.36833720556126\n",
      "Collide! - agent:  7  - stage:  4\n",
      "Resetting Agent:  7\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Max step number - agent:  8\n",
      "Resetting Agent:  8\n",
      "Rank-0: policy_step=87032, average_reward_env_0=282.0993095512993\n",
      "Collision detected between /World/Worker_4 and /World/Warehouse/Warehouse/SM_Rackshield_43/SM_Rackshield_02\n",
      "Collision detected between /World/Worker_4 and /World/Warehouse/Warehouse/SM_SignCVer_190/SM_SignCVer_10\n",
      "Arrive at the goal! - agent:  3  - stage:  4\n",
      "Resetting Agent:  3\n",
      "Collide! - agent:  8  - stage:  4\n",
      "Resetting Agent:  8\n",
      "Rank-0: policy_step=87232, average_reward_env_0=241.31320932956262\n",
      "Arrive at the goal! - agent:  1  - stage:  4\n",
      "Resetting Agent:  1\n",
      "Collide! - agent:  8  - stage:  4\n",
      "Resetting Agent:  8\n",
      "Rank-0: policy_step=87496, average_reward_env_0=209.2888973027248\n",
      "Arrive at the goal! - agent:  4  - stage:  4\n",
      "Resetting Agent:  4\n",
      "Arrive at the goal! - agent:  2  - stage:  4\n",
      "Resetting Agent:  2\n",
      "Arrive at the goal! - agent:  6  - stage:  4\n",
      "Resetting Agent:  6\n",
      "Collide! - agent:  3  - stage:  4\n",
      "Resetting Agent:  3\n",
      "Rank-0: policy_step=87800, average_reward_env_0=101.60233267050803\n",
      "Collide! - agent:  3  - stage:  4\n",
      "Resetting Agent:  3\n",
      "Arrive at the goal! - agent:  7  - stage:  4\n",
      "Resetting Agent:  7\n",
      "Collide! - agent:  5  - stage:  4\n",
      "Resetting Agent:  5\n",
      "Rank-0: policy_step=87832, average_reward_env_0=80.91884970201286\n",
      "Collide! - agent:  5  - stage:  4\n",
      "Resetting Agent:  5\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collide! - agent:  6  - stage:  4\n",
      "Resetting Agent:  6\n",
      "Arrive at the goal! - agent:  8  - stage:  4\n",
      "Resetting Agent:  8\n",
      "Rank-0: policy_step=88096, average_reward_env_0=216.88533104983117\n",
      "Arrive at the goal! - agent:  1  - stage:  4\n",
      "Resetting Agent:  1\n",
      "Collide! - agent:  4  - stage:  4\n",
      "Resetting Agent:  4\n",
      "Rank-0: policy_step=88256, average_reward_env_0=103.55465838683358\n",
      "Arrive at the goal! - agent:  7  - stage:  4\n",
      "Resetting Agent:  7\n",
      "Arrive at the goal! - agent:  3  - stage:  4\n",
      "Resetting Agent:  3\n",
      "Arrive at the goal! - agent:  8  - stage:  4\n",
      "Resetting Agent:  8\n",
      "Arrive at the goal! - agent:  2  - stage:  4\n",
      "Resetting Agent:  2\n",
      "Collide! - agent:  4  - stage:  4\n",
      "Resetting Agent:  4\n",
      "Rank-0: policy_step=88744, average_reward_env_0=95.33976960207266\n",
      "Arrive at the goal! - agent:  6  - stage:  4\n",
      "Resetting Agent:  6\n",
      "Collide! - agent:  6  - stage:  4\n",
      "Resetting Agent:  6\n",
      "Rank-0: policy_step=88808, average_reward_env_0=209.58150316260262\n",
      "Collide! - agent:  4  - stage:  4\n",
      "Resetting Agent:  4\n",
      "Rank-0: policy_step=88840, average_reward_env_0=84.49131962529071\n",
      "Max step number - agent:  1\n",
      "Resetting Agent:  1\n",
      "Rank-0: policy_step=88920, average_reward_env_0=77.80947062915966\n",
      "Collide! - agent:  2  - stage:  4\n",
      "Resetting Agent:  2\n",
      "Rank-0: policy_step=88944, average_reward_env_0=250.18079500861464\n",
      "Collide! - agent:  2  - stage:  4\n",
      "Resetting Agent:  2\n",
      "Collide! - agent:  2  - stage:  4\n",
      "Resetting Agent:  2\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collide! - agent:  4  - stage:  4\n",
      "Resetting Agent:  4\n",
      "Rank-0: policy_step=88992, average_reward_env_0=76.7481244103747\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collision detected between /World/Worker_2 and /World/Warehouse/Warehouse/SM_FloorDecal_StripeFull_4m102/SM_FloorDecal_StripeFull_4m\n",
      "Arrive at the goal! - agent:  5  - stage:  4\n",
      "Resetting Agent:  5\n",
      "Collide! - agent:  2  - stage:  4\n",
      "Resetting Agent:  2\n",
      "Rank-0: policy_step=89320, average_reward_env_0=219.92369487080398\n",
      "Collide! - agent:  2  - stage:  4\n",
      "Resetting Agent:  2\n",
      "Collide! - agent:  1  - stage:  4\n",
      "Resetting Agent:  1\n",
      "Rank-0: policy_step=89360, average_reward_env_0=71.44413449089241\n",
      "Collide! - agent:  1  - stage:  4\n",
      "Resetting Agent:  1\n",
      "Collide! - agent:  6  - stage:  4\n",
      "Resetting Agent:  6\n",
      "Rank-0: policy_step=89376, average_reward_env_0=189.41281500146823\n",
      "Collide! - agent:  6  - stage:  4\n",
      "Resetting Agent:  6\n",
      "Arrive at the goal! - agent:  3  - stage:  4\n",
      "Resetting Agent:  3\n",
      "Max step number - agent:  7\n",
      "Resetting Agent:  7\n",
      "Rank-0: policy_step=89600, average_reward_env_0=5.31336043232108\n",
      "Collide! - agent:  2  - stage:  4\n",
      "Resetting Agent:  2\n",
      "Rank-0: policy_step=89616, average_reward_env_0=192.17634662425732\n",
      "Collide! - agent:  4  - stage:  4\n",
      "Resetting Agent:  4\n",
      "Rank-0: policy_step=89632, average_reward_env_0=72.43066250024599\n",
      "Collide! - agent:  4  - stage:  4\n",
      "Resetting Agent:  4\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Arrive at the goal! - agent:  5  - stage:  4\n",
      "Resetting Agent:  5\n",
      "Collision detected between /World/Worker_4 and /World/Worker_2\n",
      "Collision detected between /World/Worker_4 and /World/Worker_2\n",
      "Collision detected between /World/Worker_4 and /World/Worker_2\n",
      "Collision detected between /World/Worker_4 and /World/Worker_2\n",
      "Collide! - agent:  8  - stage:  4\n",
      "Resetting Agent:  8\n",
      "Rank-0: policy_step=90216, average_reward_env_0=220.3621669890712\n",
      "Collide! - agent:  8  - stage:  4\n",
      "Resetting Agent:  8\n",
      "Collide! - agent:  1  - stage:  4\n",
      "Resetting Agent:  1\n",
      "Rank-0: policy_step=90280, average_reward_env_0=66.59835789548552\n",
      "Collide! - agent:  8  - stage:  4\n",
      "Resetting Agent:  8\n",
      "Rank-0: policy_step=90392, average_reward_env_0=196.08809617391879\n",
      "Collide! - agent:  8  - stage:  4\n",
      "Resetting Agent:  8\n",
      "Arrive at the goal! - agent:  7  - stage:  4\n",
      "Resetting Agent:  7\n",
      "Collide! - agent:  1  - stage:  4\n",
      "Resetting Agent:  1\n",
      "Rank-0: policy_step=90504, average_reward_env_0=60.17954701323197\n",
      "Collide! - agent:  1  - stage:  4\n",
      "Resetting Agent:  1\n",
      "Max step number - agent:  3\n",
      "Resetting Agent:  3\n",
      "Rank-0: policy_step=90608, average_reward_env_0=124.15764990908058\n",
      "Max step number - agent:  5\n",
      "Resetting Agent:  5\n",
      "Rank-0: policy_step=90640, average_reward_env_0=109.02302757056056\n",
      "Arrive at the goal! - agent:  2  - stage:  4\n",
      "Resetting Agent:  2\n",
      "Collide! - agent:  5  - stage:  4\n",
      "Resetting Agent:  5\n",
      "Rank-0: policy_step=90904, average_reward_env_0=97.85837252339459\n",
      "Collide! - agent:  5  - stage:  4\n",
      "Resetting Agent:  5\n",
      "Collide! - agent:  6  - stage:  4\n",
      "Resetting Agent:  6\n",
      "Rank-0: policy_step=90912, average_reward_env_0=172.56951491091\n",
      "Collide! - agent:  2  - stage:  4\n",
      "Resetting Agent:  2\n",
      "Rank-0: policy_step=90968, average_reward_env_0=187.04404734022822\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collide! - agent:  2  - stage:  4\n",
      "Resetting Agent:  2\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Arrive at the goal! - agent:  4  - stage:  4\n",
      "Resetting Agent:  4\n",
      "Arrive at the goal! - agent:  7  - stage:  4\n",
      "Resetting Agent:  7\n",
      "Arrive at the goal! - agent:  3  - stage:  4\n",
      "Resetting Agent:  3\n",
      "Collision detected between /World/Worker_3 and /World/Warehouse/Warehouse/GroundPlane/CollisionPlane\n",
      "Arrive at the goal! - agent:  8  - stage:  4\n",
      "Resetting Agent:  8\n",
      "Collide! - agent:  6  - stage:  4\n",
      "Resetting Agent:  6\n",
      "Rank-0: policy_step=91504, average_reward_env_0=161.68085464021559\n",
      "Collide! - agent:  6  - stage:  4\n",
      "Resetting Agent:  6\n",
      "Collide! - agent:  1  - stage:  4\n",
      "Resetting Agent:  1\n",
      "Rank-0: policy_step=91744, average_reward_env_0=56.94306287462636\n",
      "Collide! - agent:  1  - stage:  4\n",
      "Resetting Agent:  1\n",
      "Collision detected between /World/Worker_4 and /World/Worker_1\n",
      "Collision detected between /World/Worker_4 and /World/Worker_1\n",
      "Collision detected between /World/Worker_4 and /World/Worker_1\n",
      "Collision detected between /World/Worker_4 and /World/Worker_1\n",
      "Collision detected between /World/Worker_4 and /World/Warehouse/Warehouse/SM_Rackshield_1075/SM_Rackshield_02\n",
      "Arrive at the goal! - agent:  5  - stage:  4\n",
      "Resetting Agent:  5\n",
      "Collide! - agent:  4  - stage:  4\n",
      "Resetting Agent:  4\n",
      "Rank-0: policy_step=91864, average_reward_env_0=80.34329730224583\n",
      "Collide! - agent:  4  - stage:  4\n",
      "Resetting Agent:  4\n",
      "Collision detected between /World/Worker_4 and /World/Warehouse/Warehouse/SM_RackShelf_1071/SM_RackShelf_01/SM_RackShelf_01/Section2\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Arrive at the goal! - agent:  2  - stage:  4\n",
      "Resetting Agent:  2\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Arrive at the goal! - agent:  7  - stage:  4\n",
      "Resetting Agent:  7\n",
      "Arrive at the goal! - agent:  6  - stage:  4\n",
      "Resetting Agent:  6\n",
      "Arrive at the goal! - agent:  8  - stage:  4\n",
      "Resetting Agent:  8\n",
      "Max step number - agent:  7\n",
      "Resetting Agent:  7\n",
      "Rank-0: policy_step=92400, average_reward_env_0=37.05867976287495\n",
      "Collide! - agent:  6  - stage:  4\n",
      "Resetting Agent:  6\n",
      "Rank-0: policy_step=92536, average_reward_env_0=156.43517391156172\n",
      "Arrive at the goal! - agent:  2  - stage:  4\n",
      "Resetting Agent:  2\n",
      "Collide! - agent:  8  - stage:  4\n",
      "Resetting Agent:  8\n",
      "Rank-0: policy_step=92632, average_reward_env_0=201.2838394049154\n",
      "Collide! - agent:  8  - stage:  4\n",
      "Resetting Agent:  8\n",
      "Collide! - agent:  8  - stage:  4\n",
      "Resetting Agent:  8\n",
      "Rank-0: policy_step=92672, average_reward_env_0=181.25332760729984\n",
      "Collide! - agent:  8  - stage:  4\n",
      "Resetting Agent:  8\n",
      "Arrive at the goal! - agent:  1  - stage:  4\n",
      "Resetting Agent:  1\n",
      "Collide! - agent:  2  - stage:  4\n",
      "Resetting Agent:  2\n",
      "Rank-0: policy_step=92712, average_reward_env_0=196.80037316430773\n",
      "Collide! - agent:  2  - stage:  4\n",
      "Resetting Agent:  2\n",
      "Arrive at the goal! - agent:  5  - stage:  4\n",
      "Resetting Agent:  5\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Arrive at the goal! - agent:  4  - stage:  4\n",
      "Resetting Agent:  4\n",
      "Collide! - agent:  7  - stage:  4\n",
      "Resetting Agent:  7\n",
      "Rank-0: policy_step=93088, average_reward_env_0=35.78712616407953\n",
      "Collide! - agent:  7  - stage:  4\n",
      "Resetting Agent:  7\n",
      "Collide! - agent:  7  - stage:  4\n",
      "Resetting Agent:  7\n",
      "Rank-0: policy_step=93152, average_reward_env_0=30.962964116957078\n",
      "Collide! - agent:  5  - stage:  4\n",
      "Resetting Agent:  5\n",
      "Rank-0: policy_step=93160, average_reward_env_0=102.15849945626671\n",
      "Collide! - agent:  3  - stage:  4\n",
      "Resetting Agent:  3\n",
      "Rank-0: policy_step=93344, average_reward_env_0=130.36862078596366\n",
      "Collide! - agent:  3  - stage:  4\n",
      "Resetting Agent:  3\n",
      "Collide! - agent:  7  - stage:  4\n",
      "Resetting Agent:  7\n",
      "Rank-0: policy_step=93384, average_reward_env_0=27.18565495698536\n",
      "Collide! - agent:  4  - stage:  4\n",
      "Resetting Agent:  4\n",
      "Rank-0: policy_step=93392, average_reward_env_0=84.52802283471675\n",
      "Arrive at the goal! - agent:  8  - stage:  4\n",
      "Resetting Agent:  8\n",
      "Arrive at the goal! - agent:  1  - stage:  4\n",
      "Resetting Agent:  1\n",
      "Collide! - agent:  7  - stage:  4\n",
      "Resetting Agent:  7\n",
      "Rank-0: policy_step=93568, average_reward_env_0=23.943306777845077\n",
      "Arrive at the goal! - agent:  2  - stage:  4\n",
      "Resetting Agent:  2\n",
      "Arrive at the goal! - agent:  6  - stage:  4\n",
      "Resetting Agent:  6\n",
      "Collide! - agent:  2  - stage:  4\n",
      "Resetting Agent:  2\n",
      "Rank-0: policy_step=93712, average_reward_env_0=192.5273463817058\n",
      "Collide! - agent:  2  - stage:  4\n",
      "Resetting Agent:  2\n",
      "Collide! - agent:  6  - stage:  4\n",
      "Resetting Agent:  6\n",
      "Rank-0: policy_step=93776, average_reward_env_0=155.46531157987687\n",
      "Collide! - agent:  6  - stage:  4\n",
      "Resetting Agent:  6\n",
      "Arrive at the goal! - agent:  3  - stage:  4\n",
      "Resetting Agent:  3\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Arrive at the goal! - agent:  4  - stage:  4\n",
      "Resetting Agent:  4\n",
      "Collide! - agent:  7  - stage:  4\n",
      "Resetting Agent:  7\n",
      "Rank-0: policy_step=94112, average_reward_env_0=22.928047367617314\n",
      "Arrive at the goal! - agent:  1  - stage:  4\n",
      "Resetting Agent:  1\n",
      "Arrive at the goal! - agent:  8  - stage:  4\n",
      "Resetting Agent:  8\n",
      "Arrive at the goal! - agent:  5  - stage:  4\n",
      "Resetting Agent:  5\n",
      "Collide! - agent:  1  - stage:  4\n",
      "Resetting Agent:  1\n",
      "Rank-0: policy_step=94392, average_reward_env_0=80.13646689715564\n",
      "Collide! - agent:  1  - stage:  4\n",
      "Resetting Agent:  1\n",
      "Arrive at the goal! - agent:  3  - stage:  4\n",
      "Resetting Agent:  3\n",
      "Arrive at the goal! - agent:  6  - stage:  4\n",
      "Resetting Agent:  6\n",
      "Collide! - agent:  6  - stage:  4\n",
      "Resetting Agent:  6\n",
      "Rank-0: policy_step=94584, average_reward_env_0=153.1426603550085\n",
      "Collide! - agent:  1  - stage:  4\n",
      "Resetting Agent:  1\n",
      "Collide! - agent:  6  - stage:  4\n",
      "Resetting Agent:  6\n",
      "Rank-0: policy_step=94592, average_reward_env_0=74.01556369586241\n",
      "Collide! - agent:  1  - stage:  4\n",
      "Resetting Agent:  1\n",
      "Arrive at the goal! - agent:  2  - stage:  4\n",
      "Resetting Agent:  2\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collide! - agent:  6  - stage:  4\n",
      "Resetting Agent:  6\n",
      "Arrive at the goal! - agent:  7  - stage:  4\n",
      "Collide! - agent:  7  - stage:  4\n",
      "Resetting Agent:  7\n",
      "Rank-0: policy_step=95072, average_reward_env_0=142.83733797163765\n",
      "Rank-0: policy_step=95072, average_reward_env_1=27.935997062657787\n",
      "Collide! - agent:  6  - stage:  4\n",
      "Resetting Agent:  6\n",
      "Collide! - agent:  7  - stage:  4\n",
      "Resetting Agent:  7\n",
      "Rank-0: policy_step=95080, average_reward_env_0=30.66374150417165\n",
      "Arrive at the goal! - agent:  5  - stage:  4\n",
      "Resetting Agent:  5\n",
      "Collide! - agent:  3  - stage:  4\n",
      "Resetting Agent:  3\n",
      "Rank-0: policy_step=95128, average_reward_env_0=141.74248841656646\n",
      "Collision detected between /World/Worker_3 and /World/Warehouse/Table_03\n",
      "Arrive at the goal! - agent:  8  - stage:  4\n",
      "Resetting Agent:  8\n",
      "Collide! - agent:  7  - stage:  4\n",
      "Resetting Agent:  7\n",
      "Rank-0: policy_step=95240, average_reward_env_0=27.95549542242382\n",
      "Collision detected between /World/Worker_3 and /World/Warehouse/Warehouse/SM_RackShelf_3822/SM_RackShelf_01/SM_RackShelf_01/Section0\n",
      "Collision detected between /World/Worker_3 and /World/Warehouse/Warehouse/Box_37278/SM_CardBoxC_01/SM_CardBoxC_01\n",
      "Arrive at the goal! - agent:  4  - stage:  4\n",
      "Resetting Agent:  4\n",
      "Collide! - agent:  4  - stage:  4\n",
      "Resetting Agent:  4\n",
      "Rank-0: policy_step=95456, average_reward_env_0=95.76973777602505\n",
      "Collide! - agent:  4  - stage:  4\n",
      "Resetting Agent:  4\n",
      "Max step number - agent:  8\n",
      "Resetting Agent:  8\n",
      "Rank-0: policy_step=95480, average_reward_env_0=199.29198006337742\n",
      "Collide! - agent:  8  - stage:  4\n",
      "Resetting Agent:  8\n",
      "Collide! - agent:  3  - stage:  4\n",
      "Resetting Agent:  3\n",
      "Rank-0: policy_step=95512, average_reward_env_0=130.29285682324746\n",
      "Arrive at the goal! - agent:  1  - stage:  4\n",
      "Resetting Agent:  1\n",
      "Arrive at the goal! - agent:  6  - stage:  4\n",
      "Resetting Agent:  6\n",
      "Collide! - agent:  5  - stage:  4\n",
      "Resetting Agent:  5\n",
      "Rank-0: policy_step=95920, average_reward_env_0=118.48500143312184\n",
      "Collide! - agent:  5  - stage:  4\n",
      "Resetting Agent:  5\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Arrive at the goal! - agent:  7  - stage:  4\n",
      "Resetting Agent:  7\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Arrive at the goal! - agent:  2  - stage:  4\n",
      "Resetting Agent:  2\n",
      "Collide! - agent:  2  - stage:  4\n",
      "Resetting Agent:  2\n",
      "Rank-0: policy_step=96208, average_reward_env_0=202.15121540025854\n",
      "Collide! - agent:  2  - stage:  4\n",
      "Resetting Agent:  2\n",
      "Arrive at the goal! - agent:  4  - stage:  4\n",
      "Resetting Agent:  4\n",
      "Arrive at the goal! - agent:  8  - stage:  4\n",
      "Resetting Agent:  8\n",
      "Arrive at the goal! - agent:  1  - stage:  4\n",
      "Resetting Agent:  1\n",
      "Arrive at the goal! - agent:  3  - stage:  4\n",
      "Resetting Agent:  3\n",
      "Arrive at the goal! - agent:  7  - stage:  4\n",
      "Resetting Agent:  7\n",
      "Arrive at the goal! - agent:  6  - stage:  4\n",
      "Resetting Agent:  6\n",
      "Collide! - agent:  6  - stage:  4\n",
      "Resetting Agent:  6\n",
      "Rank-0: policy_step=96960, average_reward_env_0=150.4925721692777\n",
      "Collide! - agent:  6  - stage:  4\n",
      "Resetting Agent:  6\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Arrive at the goal! - agent:  2  - stage:  4\n",
      "Resetting Agent:  2\n",
      "Arrive at the goal! - agent:  5  - stage:  4\n",
      "Resetting Agent:  5\n",
      "Collide! - agent:  5  - stage:  4\n",
      "Resetting Agent:  5\n",
      "Rank-0: policy_step=97096, average_reward_env_0=119.58960494532971\n",
      "Collide! - agent:  5  - stage:  4\n",
      "Resetting Agent:  5\n",
      "Arrive at the goal! - agent:  4  - stage:  4\n",
      "Resetting Agent:  4\n",
      "Arrive at the goal! - agent:  1  - stage:  4\n",
      "Resetting Agent:  1\n",
      "Arrive at the goal! - agent:  3  - stage:  4\n",
      "Resetting Agent:  3\n",
      "Arrive at the goal! - agent:  8  - stage:  4\n",
      "Resetting Agent:  8\n",
      "Arrive at the goal! - agent:  7  - stage:  4\n",
      "Resetting Agent:  7\n",
      "Max step number - agent:  1\n",
      "Resetting Agent:  1\n",
      "Rank-0: policy_step=97400, average_reward_env_0=97.93636902070641\n",
      "Collide! - agent:  3  - stage:  4\n",
      "Resetting Agent:  3\n",
      "Rank-0: policy_step=97648, average_reward_env_0=142.64121978863795\n",
      "Collide! - agent:  3  - stage:  4\n",
      "Resetting Agent:  3\n",
      "Arrive at the goal! - agent:  6  - stage:  4\n",
      "Resetting Agent:  6\n",
      "Arrive at the goal! - agent:  2  - stage:  4\n",
      "Resetting Agent:  2\n",
      "Collide! - agent:  6  - stage:  4\n",
      "Resetting Agent:  6\n",
      "Rank-0: policy_step=97968, average_reward_env_0=150.1925534716063\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Arrive at the goal! - agent:  4  - stage:  4\n",
      "Resetting Agent:  4\n",
      "Max step number - agent:  7\n",
      "Resetting Agent:  7\n",
      "Rank-0: policy_step=98040, average_reward_env_1=45.81848140187568\n",
      "Collide! - agent:  7  - stage:  4\n",
      "Resetting Agent:  7\n",
      "Rank-0: policy_step=98160, average_reward_env_0=39.07561171739571\n",
      "Collide! - agent:  7  - stage:  4\n",
      "Resetting Agent:  7\n",
      "Max step number - agent:  4\n",
      "Resetting Agent:  4\n",
      "Rank-0: policy_step=98264, average_reward_env_0=117.05698656065037\n",
      "Collision detected between /World/Worker_2 and /World/Warehouse/Warehouse/GroundPlane/CollisionPlane\n",
      "Max step number - agent:  8\n",
      "Resetting Agent:  8\n",
      "Rank-0: policy_step=98288, average_reward_env_0=212.23244660249534\n",
      "Collide! - agent:  8  - stage:  4\n",
      "Resetting Agent:  8\n",
      "Collide! - agent:  8  - stage:  4\n",
      "Resetting Agent:  8\n",
      "Collide! - agent:  2  - stage:  4\n",
      "Resetting Agent:  2\n",
      "Rank-0: policy_step=98464, average_reward_env_0=212.15215567203722\n",
      "Arrive at the goal! - agent:  1  - stage:  4\n",
      "Resetting Agent:  1\n",
      "Arrive at the goal! - agent:  5  - stage:  4\n",
      "Resetting Agent:  5\n",
      "Arrive at the goal! - agent:  3  - stage:  4\n",
      "Resetting Agent:  3\n",
      "Collide! - agent:  3  - stage:  4\n",
      "Resetting Agent:  3\n",
      "Rank-0: policy_step=98864, average_reward_env_0=142.74361937508922\n",
      "Collide! - agent:  3  - stage:  4\n",
      "Resetting Agent:  3\n",
      "Arrive at the goal! - agent:  6  - stage:  4\n",
      "Resetting Agent:  6\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Arrive at the goal! - agent:  7  - stage:  4\n",
      "Resetting Agent:  7\n",
      "Collide! - agent:  1  - stage:  4\n",
      "Resetting Agent:  1\n",
      "Rank-0: policy_step=99112, average_reward_env_0=99.18039579760739\n",
      "Collide! - agent:  1  - stage:  4\n",
      "Resetting Agent:  1\n",
      "Arrive at the goal! - agent:  4  - stage:  4\n",
      "Resetting Agent:  4\n",
      "Arrive at the goal! - agent:  2  - stage:  4\n",
      "Resetting Agent:  2\n",
      "Collide! - agent:  5  - stage:  4\n",
      "Resetting Agent:  5\n",
      "Rank-0: policy_step=99280, average_reward_env_1=125.80260176268766\n",
      "Collide! - agent:  5  - stage:  4\n",
      "Resetting Agent:  5\n",
      "Collide! - agent:  7  - stage:  4\n",
      "Resetting Agent:  7\n",
      "Rank-0: policy_step=99424, average_reward_env_0=57.76456632271921\n",
      "Collide! - agent:  7  - stage:  4\n",
      "Resetting Agent:  7\n",
      "Collide! - agent:  7  - stage:  4\n",
      "Resetting Agent:  7\n",
      "Collide! - agent:  7  - stage:  4\n",
      "Resetting Agent:  7\n",
      "Collide! - agent:  7  - stage:  4\n",
      "Resetting Agent:  7\n",
      "Collide! - agent:  7  - stage:  4\n",
      "Resetting Agent:  7\n",
      "Rank-0: policy_step=99496, average_reward_env_0=55.48762755830759\n",
      "Arrive at the goal! - agent:  8  - stage:  4\n",
      "Resetting Agent:  8\n",
      "Arrive at the goal! - agent:  6  - stage:  4\n",
      "Resetting Agent:  6\n",
      "Collide! - agent:  5  - stage:  4\n",
      "Resetting Agent:  5\n",
      "Rank-0: policy_step=99720, average_reward_env_0=118.33372509167579\n",
      "Collide! - agent:  1  - stage:  4\n",
      "Resetting Agent:  1\n",
      "Collide! - agent:  5  - stage:  4\n",
      "Resetting Agent:  5\n",
      "Rank-0: policy_step=99728, average_reward_env_0=95.58863281465317\n",
      "Arrive at the goal! - agent:  3  - stage:  4\n",
      "Resetting Agent:  3\n",
      "Collide! - agent:  1  - stage:  4\n",
      "Resetting Agent:  1\n",
      "Collide! - agent:  3  - stage:  4\n",
      "Resetting Agent:  3\n",
      "Rank-0: policy_step=99880, average_reward_env_0=89.684927976904\n",
      "Rank-0: policy_step=99880, average_reward_env_1=141.99573821603434\n",
      "Collide! - agent:  3  - stage:  4\n",
      "Resetting Agent:  3\n",
      "Collide! - agent:  2  - stage:  4\n",
      "Resetting Agent:  2\n",
      "Rank-0: policy_step=99896, average_reward_env_0=209.65057908741156\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collide! - agent:  2  - stage:  4\n",
      "Resetting Agent:  2\n",
      "Rank-0: policy_step=100016, average_reward_env_0=194.49825398963338\n",
      "Arrive at the goal! - agent:  4  - stage:  4\n",
      "Resetting Agent:  4\n",
      "Collide! - agent:  2  - stage:  4\n",
      "Resetting Agent:  2\n",
      "Rank-0: policy_step=100248, average_reward_env_0=181.22590255551117\n",
      "Collide! - agent:  2  - stage:  4\n",
      "Resetting Agent:  2\n",
      "Collide! - agent:  4  - stage:  4\n",
      "Resetting Agent:  4\n",
      "Rank-0: policy_step=100320, average_reward_env_0=125.39164278948843\n",
      "Collide! - agent:  7  - stage:  4\n",
      "Resetting Agent:  7\n",
      "Rank-0: policy_step=100536, average_reward_env_0=56.63773396314753\n",
      "Arrive at the goal! - agent:  5  - stage:  4\n",
      "Resetting Agent:  5\n",
      "Collide! - agent:  7  - stage:  4\n",
      "Resetting Agent:  7\n",
      "Arrive at the goal! - agent:  3  - stage:  4\n",
      "Resetting Agent:  3\n",
      "Collision detected between /World/Worker_3 and /World/Worker_2\n",
      "Collision detected between /World/Worker_3 and /World/Worker_2\n",
      "Arrive at the goal! - agent:  6  - stage:  4\n",
      "Resetting Agent:  6\n",
      "Collision detected between /World/Worker_3 and /World/Worker_2\n",
      "Collision detected between /World/Worker_3 and /World/Worker_2\n",
      "Collide! - agent:  7  - stage:  4\n",
      "Resetting Agent:  7\n",
      "Rank-0: policy_step=100640, average_reward_env_0=54.58399605458834\n",
      "Collide! - agent:  7  - stage:  4\n",
      "Resetting Agent:  7\n",
      "Collide! - agent:  7  - stage:  4\n",
      "Resetting Agent:  7\n",
      "Rank-0: policy_step=100680, average_reward_env_0=53.336356362348596\n",
      "Collide! - agent:  7  - stage:  4\n",
      "Resetting Agent:  7\n",
      "Arrive at the goal! - agent:  8  - stage:  4\n",
      "Resetting Agent:  8\n",
      "Max step number - agent:  6\n",
      "Resetting Agent:  6\n",
      "Rank-0: policy_step=100768, average_reward_env_0=168.84156388137086\n",
      "Arrive at the goal! - agent:  4  - stage:  4\n",
      "Resetting Agent:  4\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collide! - agent:  6  - stage:  4\n",
      "Resetting Agent:  6\n",
      "Rank-0: policy_step=100984, average_reward_env_0=159.12883591075095\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collide! - agent:  6  - stage:  4\n",
      "Resetting Agent:  6\n",
      "Rank-0: policy_step=101056, average_reward_env_0=149.9796758719873\n",
      "Max step number - agent:  8\n",
      "Resetting Agent:  8\n",
      "Rank-0: policy_step=101112, average_reward_env_0=225.25928714197997\n",
      "Collision detected between /World/Worker_3 and /World/Warehouse/Warehouse/SM_Rackshield_3841/SM_Rackshield_02\n",
      "Collide! - agent:  5  - stage:  4\n",
      "Resetting Agent:  5\n",
      "Rank-0: policy_step=101144, average_reward_env_0=121.59503837296594\n",
      "Arrive at the goal! - agent:  1  - stage:  4\n",
      "Resetting Agent:  1\n",
      "Collide! - agent:  3  - stage:  4\n",
      "Resetting Agent:  3\n",
      "Collide! - agent:  8  - stage:  4\n",
      "Resetting Agent:  8\n",
      "Rank-0: policy_step=101304, average_reward_env_0=141.32782261921108\n",
      "Rank-0: policy_step=101304, average_reward_env_1=208.25432155120848\n",
      "Arrive at the goal! - agent:  2  - stage:  4\n",
      "Resetting Agent:  2\n",
      "Collide! - agent:  6  - stage:  4\n",
      "Resetting Agent:  6\n",
      "Rank-0: policy_step=101424, average_reward_env_0=142.84820175726855\n",
      "Collide! - agent:  7  - stage:  4\n",
      "Resetting Agent:  7\n",
      "Rank-0: policy_step=101744, average_reward_env_0=54.487961003232556\n",
      "Collide! - agent:  2  - stage:  4\n",
      "Resetting Agent:  2\n",
      "Collide! - agent:  7  - stage:  4\n",
      "Resetting Agent:  7\n",
      "Rank-0: policy_step=101752, average_reward_env_0=180.19238196438585\n",
      "Collide! - agent:  4  - stage:  4\n",
      "Resetting Agent:  4\n",
      "Rank-0: policy_step=101824, average_reward_env_0=127.47136820278783\n",
      "Arrive at the goal! - agent:  1  - stage:  4\n",
      "Resetting Agent:  1\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Arrive at the goal! - agent:  8  - stage:  4\n",
      "Resetting Agent:  8\n",
      "Arrive at the goal! - agent:  5  - stage:  4\n",
      "Resetting Agent:  5\n",
      "Collide! - agent:  6  - stage:  4\n",
      "Resetting Agent:  6\n",
      "Rank-0: policy_step=102168, average_reward_env_0=115.94920334260242\n",
      "Collide! - agent:  6  - stage:  4\n",
      "Resetting Agent:  6\n",
      "Arrive at the goal! - agent:  3  - stage:  4\n",
      "Resetting Agent:  3\n",
      "Collide! - agent:  7  - stage:  4\n",
      "Resetting Agent:  7\n",
      "Rank-0: policy_step=102280, average_reward_env_0=57.96541839813439\n",
      "Collide! - agent:  7  - stage:  4\n",
      "Resetting Agent:  7\n",
      "Collide! - agent:  8  - stage:  4\n",
      "Resetting Agent:  8\n",
      "Rank-0: policy_step=102288, average_reward_env_0=201.1007396766332\n",
      "Collide! - agent:  8  - stage:  4\n",
      "Resetting Agent:  8\n",
      "Rank-0: policy_step=102456, average_reward_env_0=187.02731803200123\n",
      "Collide! - agent:  2  - stage:  4\n",
      "Resetting Agent:  2\n",
      "Rank-0: policy_step=102552, average_reward_env_0=171.43338364683876\n",
      "Collide! - agent:  2  - stage:  4\n",
      "Resetting Agent:  2\n",
      "Collide! - agent:  4  - stage:  4\n",
      "Resetting Agent:  4\n",
      "Rank-0: policy_step=102616, average_reward_env_0=122.16875599568563\n",
      "Collide! - agent:  4  - stage:  4\n",
      "Resetting Agent:  4\n",
      "Max step number - agent:  1\n",
      "Resetting Agent:  1\n",
      "Rank-0: policy_step=102680, average_reward_env_0=102.50666541344671\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Arrive at the goal! - agent:  5  - stage:  4\n",
      "Resetting Agent:  5\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collision detected between /World/Worker_4 and /World/Warehouse/Warehouse/GroundPlane/CollisionPlane\n",
      "Collide! - agent:  3  - stage:  4\n",
      "Resetting Agent:  3\n",
      "Rank-0: policy_step=103160, average_reward_env_0=143.63682936134987\n",
      "Collide! - agent:  3  - stage:  4\n",
      "Resetting Agent:  3\n",
      "Arrive at the goal! - agent:  4  - stage:  4\n",
      "Resetting Agent:  4\n",
      "Arrive at the goal! - agent:  1  - stage:  4\n",
      "Resetting Agent:  1\n",
      "Collide! - agent:  1  - stage:  4\n",
      "Resetting Agent:  1\n",
      "Rank-0: policy_step=103224, average_reward_env_0=109.13608564106674\n",
      "Arrive at the goal! - agent:  7  - stage:  4\n",
      "Resetting Agent:  7\n",
      "Collide! - agent:  6  - stage:  4\n",
      "Resetting Agent:  6\n",
      "Rank-0: policy_step=103288, average_reward_env_0=90.78671607749597\n",
      "Collide! - agent:  6  - stage:  4\n",
      "Resetting Agent:  6\n",
      "Arrive at the goal! - agent:  2  - stage:  4\n",
      "Resetting Agent:  2\n",
      "Collision detected between /World/Worker_4 and /World/Warehouse/Warehouse/SM_WallA_6M18/SM_WallA_6M/SM_WallA_6M/Section0\n",
      "Collision detected between /World/Worker_4 and /World/Warehouse/Warehouse/SM_WallA_6M18/SM_WallA_6M/SM_WallA_6M/Section1\n",
      "Arrive at the goal! - agent:  8  - stage:  4\n",
      "Resetting Agent:  8\n",
      "Arrive at the goal! - agent:  3  - stage:  4\n",
      "Resetting Agent:  3\n",
      "Arrive at the goal! - agent:  5  - stage:  4\n",
      "Resetting Agent:  5\n",
      "Collide! - agent:  4  - stage:  4\n",
      "Resetting Agent:  4\n",
      "Rank-0: policy_step=103920, average_reward_env_0=124.95998823000869\n",
      "Arrive at the goal! - agent:  1  - stage:  4\n",
      "Resetting Agent:  1\n",
      "Max step number - agent:  5\n",
      "Resetting Agent:  5\n",
      "Arrive at the goal! - agent:  7  - stage:  4\n",
      "Resetting Agent:  7\n",
      "Rank-0: policy_step=103944, average_reward_env_0=142.11421427657973\n",
      "Collide! - agent:  7  - stage:  4\n",
      "Resetting Agent:  7\n",
      "Rank-0: policy_step=103968, average_reward_env_0=64.3827655807926\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collide! - agent:  2  - stage:  4\n",
      "Resetting Agent:  2\n",
      "Rank-0: policy_step=103984, average_reward_env_0=169.16306560055497\n",
      "Collide! - agent:  2  - stage:  4\n",
      "Resetting Agent:  2\n",
      "Arrive at the goal! - agent:  6  - stage:  4\n",
      "Resetting Agent:  6\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Arrive at the goal! - agent:  8  - stage:  4\n",
      "Resetting Agent:  8\n",
      "Arrive at the goal! - agent:  5  - stage:  4\n",
      "Resetting Agent:  5\n",
      "Arrive at the goal! - agent:  2  - stage:  4\n",
      "Resetting Agent:  2\n",
      "Arrive at the goal! - agent:  7  - stage:  4\n",
      "Resetting Agent:  7\n",
      "Arrive at the goal! - agent:  6  - stage:  4\n",
      "Resetting Agent:  6\n",
      "Arrive at the goal! - agent:  1  - stage:  4\n",
      "Resetting Agent:  1\n",
      "Arrive at the goal! - agent:  3  - stage:  4\n",
      "Resetting Agent:  3\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collide! - agent:  5  - stage:  4\n",
      "Resetting Agent:  5\n",
      "Rank-0: policy_step=105032, average_reward_env_0=142.11930689634366\n",
      "Collide! - agent:  7  - stage:  4\n",
      "Resetting Agent:  7\n",
      "Rank-0: policy_step=105048, average_reward_env_0=71.02238165228383\n",
      "Max step number - agent:  8\n",
      "Resetting Agent:  8\n",
      "Rank-0: policy_step=105256, average_reward_env_0=197.41560843476663\n",
      "Collision detected between /World/Worker_2 and /World/Warehouse/Table_12\n",
      "Collision detected between /World/Worker_4 and /World/Warehouse/Warehouse/SM_Rackshield_2984/SM_Rackshield_02\n",
      "Collision detected between /World/Worker_1 and /World/Warehouse/Warehouse/GroundPlane/CollisionPlane\n",
      "Collide! - agent:  3  - stage:  4\n",
      "Resetting Agent:  3\n",
      "Rank-0: policy_step=105504, average_reward_env_0=151.85313751106307\n",
      "Collide! - agent:  3  - stage:  4\n",
      "Resetting Agent:  3\n",
      "Arrive at the goal! - agent:  4  - stage:  4\n",
      "Resetting Agent:  4\n",
      "Arrive at the goal! - agent:  6  - stage:  4\n",
      "Resetting Agent:  6\n",
      "Collide! - agent:  8  - stage:  4\n",
      "Resetting Agent:  8\n",
      "Rank-0: policy_step=105600, average_reward_env_0=184.5141094699105\n",
      "Collide! - agent:  1  - stage:  4\n",
      "Resetting Agent:  1\n",
      "Rank-0: policy_step=105776, average_reward_env_0=103.53253819312997\n",
      "Collide! - agent:  1  - stage:  4\n",
      "Resetting Agent:  1\n",
      "Arrive at the goal! - agent:  2  - stage:  4\n",
      "Resetting Agent:  2\n",
      "Collision detected between /World/Worker_3 and /World/Worker_2\n",
      "Collision detected between /World/Worker_3 and /World/Worker_2\n",
      "Collision detected between /World/Worker_3 and /World/Worker_2\n",
      "Collision detected between /World/Worker_3 and /World/Worker_2\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Arrive at the goal! - agent:  7  - stage:  4\n",
      "Resetting Agent:  7\n",
      "Collide! - agent:  2  - stage:  4\n",
      "Resetting Agent:  2\n",
      "Rank-0: policy_step=106072, average_reward_env_0=173.84600420491208\n",
      "Max step number - agent:  6\n",
      "Resetting Agent:  6\n",
      "Rank-0: policy_step=106096, average_reward_env_0=106.00946197656148\n",
      "Arrive at the goal! - agent:  5  - stage:  4\n",
      "Resetting Agent:  5\n",
      "Arrive at the goal! - agent:  8  - stage:  4\n",
      "Resetting Agent:  8\n",
      "Arrive at the goal! - agent:  3  - stage:  4\n",
      "Resetting Agent:  3\n",
      "Collide! - agent:  7  - stage:  4\n",
      "Resetting Agent:  7\n",
      "Rank-0: policy_step=106288, average_reward_env_0=76.89837354326397\n",
      "Collide! - agent:  7  - stage:  4\n",
      "Resetting Agent:  7\n",
      "Collide! - agent:  8  - stage:  4\n",
      "Resetting Agent:  8\n",
      "Rank-0: policy_step=106304, average_reward_env_0=181.31351364883034\n",
      "Collide! - agent:  1  - stage:  4\n",
      "Resetting Agent:  1\n",
      "Rank-0: policy_step=106352, average_reward_env_0=97.81971449975592\n",
      "Collision detected between /World/Worker_4 and /World/Worker_2\n",
      "Collision detected between /World/Worker_4 and /World/Worker_2\n",
      "Collision detected between /World/Worker_4 and /World/Worker_2\n",
      "Collision detected between /World/Worker_4 and /World/Worker_2\n",
      "Collide! - agent:  5  - stage:  4\n",
      "Resetting Agent:  5\n",
      "Rank-0: policy_step=106456, average_reward_env_0=142.76542153694382\n",
      "Collide! - agent:  5  - stage:  4\n",
      "Resetting Agent:  5\n",
      "Collide! - agent:  5  - stage:  4\n",
      "Resetting Agent:  5\n",
      "Arrive at the goal! - agent:  4  - stage:  4\n",
      "Resetting Agent:  4\n",
      "Collide! - agent:  2  - stage:  4\n",
      "Resetting Agent:  2\n",
      "Rank-0: policy_step=106704, average_reward_env_0=163.82099037769316\n",
      "Collide! - agent:  2  - stage:  4\n",
      "Resetting Agent:  2\n",
      "Max step number - agent:  4\n",
      "Resetting Agent:  4\n",
      "Rank-0: policy_step=106720, average_reward_env_0=133.06377938892516\n",
      "Collide! - agent:  8  - stage:  4\n",
      "Resetting Agent:  8\n",
      "Rank-0: policy_step=106808, average_reward_env_0=172.37843280931023\n",
      "Collision detected between /World/Worker_3 and /World/Warehouse/Warehouse/SM_RackFrame_179/SM_RackFrame_03/SM_RackFrame_03/Section0\n",
      "Collision detected between /World/Worker_3 and /World/Warehouse/Warehouse/SM_RackFrame_179/SM_RackFrame_03/SM_RackFrame_03/Section1\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collide! - agent:  2  - stage:  4\n",
      "Resetting Agent:  2\n",
      "Rank-0: policy_step=107320, average_reward_env_0=156.3415626942348\n",
      "Collide! - agent:  1  - stage:  4\n",
      "Resetting Agent:  1\n",
      "Rank-0: policy_step=107328, average_reward_env_0=91.5455532518717\n",
      "Collide! - agent:  1  - stage:  4\n",
      "Resetting Agent:  1\n",
      "Collide! - agent:  8  - stage:  4\n",
      "Resetting Agent:  8\n",
      "Rank-0: policy_step=107344, average_reward_env_0=165.563481373934\n",
      "Collision detected between /World/Worker_4 and /World/Warehouse/Warehouse/SM_Rackshield_2984/SM_Rackshield_02\n",
      "Arrive at the goal! - agent:  7  - stage:  4\n",
      "Resetting Agent:  7\n",
      "Arrive at the goal! - agent:  3  - stage:  4\n",
      "Resetting Agent:  3\n",
      "Collide! - agent:  5  - stage:  4\n",
      "Resetting Agent:  5\n",
      "Rank-0: policy_step=107544, average_reward_env_0=134.59234743610966\n",
      "Collision detected between /World/Worker_3 and /World/Warehouse/Table_09\n",
      "Collide! - agent:  8  - stage:  4\n",
      "Resetting Agent:  8\n",
      "Rank-0: policy_step=107952, average_reward_env_0=159.36448912857762\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Arrive at the goal! - agent:  6  - stage:  4\n",
      "Resetting Agent:  6\n",
      "Arrive at the goal! - agent:  4  - stage:  4\n",
      "Resetting Agent:  4\n",
      "Collision detected between /World/Worker_4 and /World/Worker_1\n",
      "Collision detected between /World/Worker_4 and /World/Worker_1\n",
      "Collide! - agent:  2  - stage:  4\n",
      "Resetting Agent:  2\n",
      "Rank-0: policy_step=108280, average_reward_env_0=150.37753343021134\n",
      "Collide! - agent:  3  - stage:  4\n",
      "Resetting Agent:  3\n",
      "Rank-0: policy_step=108304, average_reward_env_0=159.27326285291107\n",
      "Collide! - agent:  3  - stage:  4\n",
      "Resetting Agent:  3\n",
      "Collision detected between /World/Worker_2 and /World/Worker_1\n",
      "Collision detected between /World/Worker_2 and /World/Worker_1\n",
      "Collision detected between /World/Worker_2 and /World/Worker_1\n",
      "Collision detected between /World/Worker_2 and /World/Worker_1\n",
      "Arrive at the goal! - agent:  1  - stage:  4\n",
      "Resetting Agent:  1\n",
      "Arrive at the goal! - agent:  5  - stage:  4\n",
      "Resetting Agent:  5\n",
      "Collide! - agent:  8  - stage:  4\n",
      "Resetting Agent:  8\n",
      "Rank-0: policy_step=108496, average_reward_env_0=139.90160367760382\n",
      "Collide! - agent:  8  - stage:  4\n",
      "Resetting Agent:  8\n",
      "Collide! - agent:  6  - stage:  4\n",
      "Resetting Agent:  6\n",
      "Rank-0: policy_step=108560, average_reward_env_0=98.15912915715502\n",
      "Arrive at the goal! - agent:  7  - stage:  4\n",
      "Resetting Agent:  7\n",
      "Collide! - agent:  7  - stage:  4\n",
      "Resetting Agent:  7\n",
      "Rank-0: policy_step=108600, average_reward_env_0=90.2573585804037\n",
      "Collide! - agent:  2  - stage:  4\n",
      "Resetting Agent:  2\n",
      "Rank-0: policy_step=108624, average_reward_env_0=128.74231707745344\n",
      "Collide! - agent:  2  - stage:  4\n",
      "Resetting Agent:  2\n",
      "Arrive at the goal! - agent:  4  - stage:  4\n",
      "Resetting Agent:  4\n",
      "Arrive at the goal! - agent:  3  - stage:  4\n",
      "Resetting Agent:  3\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Arrive at the goal! - agent:  1  - stage:  4\n",
      "Resetting Agent:  1\n",
      "Collide! - agent:  3  - stage:  4\n",
      "Resetting Agent:  3\n",
      "Rank-0: policy_step=109024, average_reward_env_1=156.58522434872495\n",
      "Collide! - agent:  3  - stage:  4\n",
      "Resetting Agent:  3\n",
      "Arrive at the goal! - agent:  5  - stage:  4\n",
      "Resetting Agent:  5\n",
      "Arrive at the goal! - agent:  7  - stage:  4\n",
      "Collide! - agent:  7  - stage:  4\n",
      "Resetting Agent:  7\n",
      "Rank-0: policy_step=109208, average_reward_env_0=96.83306544455178\n",
      "Collide! - agent:  7  - stage:  4\n",
      "Resetting Agent:  7\n",
      "Rank-0: policy_step=109272, average_reward_env_0=101.32341317981083\n",
      "Collide! - agent:  7  - stage:  4\n",
      "Resetting Agent:  7\n",
      "Collide! - agent:  6  - stage:  4\n",
      "Resetting Agent:  6\n",
      "Rank-0: policy_step=109328, average_reward_env_0=99.77006112299843\n",
      "Arrive at the goal! - agent:  2  - stage:  4\n",
      "Resetting Agent:  2\n",
      "Collision detected between /World/Worker_2 and /World/Warehouse/Warehouse/SM_Rackshield_3841/SM_Rackshield_02\n",
      "Collision detected between /World/Worker_4 and /World/Worker_2\n",
      "Collision detected between /World/Worker_4 and /World/Worker_2\n",
      "Collision detected between /World/Worker_4 and /World/Worker_2\n",
      "Collision detected between /World/Worker_4 and /World/Worker_2\n",
      "Max step number - agent:  4\n",
      "Resetting Agent:  4\n",
      "Rank-0: policy_step=109520, average_reward_env_0=150.25770122712063\n",
      "Arrive at the goal! - agent:  3  - stage:  4\n",
      "Resetting Agent:  3\n",
      "Arrive at the goal! - agent:  8  - stage:  4\n",
      "Resetting Agent:  8\n",
      "Arrive at the goal! - agent:  7  - stage:  4\n",
      "Resetting Agent:  7\n",
      "Arrive at the goal! - agent:  6  - stage:  4\n",
      "Resetting Agent:  6\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Arrive at the goal! - agent:  5  - stage:  4\n",
      "Resetting Agent:  5\n",
      "Collide! - agent:  7  - stage:  4\n",
      "Resetting Agent:  7\n",
      "Rank-0: policy_step=110032, average_reward_env_0=88.85704994435919\n",
      "Collide! - agent:  7  - stage:  4\n",
      "Resetting Agent:  7\n",
      "Max step number - agent:  1\n",
      "Resetting Agent:  1\n",
      "Arrive at the goal! - agent:  2  - stage:  4\n",
      "Resetting Agent:  2\n",
      "Rank-0: policy_step=110136, average_reward_env_0=110.18629099252173\n",
      "Max step number - agent:  5\n",
      "Resetting Agent:  5\n",
      "Rank-0: policy_step=110344, average_reward_env_0=151.67002987076415\n",
      "Collide! - agent:  3  - stage:  4\n",
      "Resetting Agent:  3\n",
      "Rank-0: policy_step=110352, average_reward_env_0=156.86144801006597\n",
      "Collide! - agent:  3  - stage:  4\n",
      "Resetting Agent:  3\n",
      "Arrive at the goal! - agent:  4  - stage:  4\n",
      "Resetting Agent:  4\n",
      "Collide! - agent:  8  - stage:  4\n",
      "Resetting Agent:  8\n",
      "Rank-0: policy_step=110424, average_reward_env_0=147.74133536649364\n",
      "Collide! - agent:  2  - stage:  4\n",
      "Resetting Agent:  2\n",
      "Rank-0: policy_step=110464, average_reward_env_0=138.61189788822577\n",
      "Collide! - agent:  2  - stage:  4\n",
      "Resetting Agent:  2\n",
      "Collision detected between /World/Worker_2 and /World/Warehouse/Warehouse/SM_RackFrame_179/SM_RackFrame_03/SM_RackFrame_03/Section1\n",
      "Collision detected between /World/Worker_2 and /World/Warehouse/Warehouse/SM_Rackshield_1075/SM_Rackshield_02\n",
      "Collide! - agent:  8  - stage:  4\n",
      "Resetting Agent:  8\n",
      "Rank-0: policy_step=110632, average_reward_env_0=138.6622252955485\n",
      "Arrive at the goal! - agent:  1  - stage:  4\n",
      "Resetting Agent:  1\n",
      "Collide! - agent:  4  - stage:  4\n",
      "Resetting Agent:  4\n",
      "Rank-0: policy_step=110736, average_reward_env_0=157.29391052931584\n",
      "Arrive at the goal! - agent:  6  - stage:  4\n",
      "Resetting Agent:  6\n",
      "Arrive at the goal! - agent:  7  - stage:  4\n",
      "Resetting Agent:  7\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collide! - agent:  5  - stage:  4\n",
      "Resetting Agent:  5\n",
      "Rank-0: policy_step=110976, average_reward_env_0=145.2622670583452\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collide! - agent:  6  - stage:  4\n",
      "Resetting Agent:  6\n",
      "Rank-0: policy_step=111056, average_reward_env_0=112.14148251523308\n",
      "Collide! - agent:  6  - stage:  4\n",
      "Resetting Agent:  6\n",
      "Collide! - agent:  1  - stage:  4\n",
      "Resetting Agent:  1\n",
      "Rank-0: policy_step=111128, average_reward_env_0=116.0081348114685\n",
      "Collide! - agent:  1  - stage:  4\n",
      "Resetting Agent:  1\n",
      "Collision detected between /World/Worker_3 and /World/Worker_2\n",
      "Collision detected between /World/Worker_3 and /World/Worker_2\n",
      "Arrive at the goal! - agent:  3  - stage:  4\n",
      "Resetting Agent:  3\n",
      "Collide! - agent:  2  - stage:  4\n",
      "Resetting Agent:  2\n",
      "Rank-0: policy_step=111424, average_reward_env_0=132.41058901682482\n",
      "Arrive at the goal! - agent:  8  - stage:  4\n",
      "Resetting Agent:  8\n",
      "Collide! - agent:  3  - stage:  4\n",
      "Resetting Agent:  3\n",
      "Rank-0: policy_step=111752, average_reward_env_0=148.01779530047972\n",
      "Collide! - agent:  4  - stage:  4\n",
      "Resetting Agent:  4\n",
      "Rank-0: policy_step=111792, average_reward_env_0=150.15945241640054\n",
      "Collide! - agent:  3  - stage:  4\n",
      "Resetting Agent:  3\n",
      "Rank-0: policy_step=111888, average_reward_env_0=147.63538720479883\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Judging Increasing\n",
      ">>>>>>>>>>>>>>>Increasing Difficulty>>>>>>>>>>>>>>>>>>>\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Arrive at the goal! - agent:  2  - stage:  5\n",
      "Resetting Agent:  2\n",
      "Collide! - agent:  2  - stage:  5\n",
      "Resetting Agent:  2\n",
      "Arrive at the goal! - agent:  6  - stage:  5\n",
      "Resetting Agent:  6\n",
      "Rank-0: policy_step=112152, average_reward_env_0=106.79448621613679\n",
      "Arrive at the goal! - agent:  7  - stage:  5\n",
      "Resetting Agent:  7\n",
      "Collide! - agent:  3  - stage:  5\n",
      "Resetting Agent:  3\n",
      "Rank-0: policy_step=112280, average_reward_env_0=-26.144930219051144\n",
      "Collide! - agent:  5  - stage:  5\n",
      "Resetting Agent:  5\n",
      "Rank-0: policy_step=112296, average_reward_env_0=-10.900956512617313\n",
      "Collide! - agent:  5  - stage:  5\n",
      "Resetting Agent:  5\n",
      "Arrive at the goal! - agent:  1  - stage:  5\n",
      "Resetting Agent:  1\n",
      "Arrive at the goal! - agent:  4  - stage:  5\n",
      "Resetting Agent:  4\n",
      "Collide! - agent:  6  - stage:  5\n",
      "Resetting Agent:  6  Other Direction\n",
      "Rank-0: policy_step=112736, average_reward_env_0=171.59460321805696\n",
      "Collide! - agent:  6  - stage:  5\n",
      "Resetting Agent:  6\n",
      "Collision detected between /World/Worker_2 and /World/Warehouse/Warehouse/SM_Rackshield_43/SM_Rackshield_02\n",
      "Collision detected between /World/Worker_2 and /World/Warehouse/Warehouse/SM_SignCVer_190/SM_SignCVer_10\n",
      "Collide! - agent:  5  - stage:  5\n",
      "Resetting Agent:  5\n",
      "Rank-0: policy_step=112840, average_reward_env_0=-10.401130637285426\n",
      "Collide! - agent:  5  - stage:  5\n",
      "Resetting Agent:  5\n",
      "Collision detected between /World/Worker_2 and /World/Warehouse/Rack_01\n",
      "Collide! - agent:  6  - stage:  5\n",
      "Resetting Agent:  6\n",
      "Rank-0: policy_step=112952, average_reward_env_0=69.4203191796062\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collide! - agent:  1  - stage:  5\n",
      "Resetting Agent:  1\n",
      "Rank-0: policy_step=113136, average_reward_env_0=222.9992124447866\n",
      "Collide! - agent:  1  - stage:  5\n",
      "Resetting Agent:  1\n",
      "Collide! - agent:  1  - stage:  5\n",
      "Resetting Agent:  1\n",
      "Rank-0: policy_step=113192, average_reward_env_0=83.79778359864865\n",
      "Max step number - agent:  7\n",
      "Resetting Agent:  7\n",
      "Rank-0: policy_step=113240, average_reward_env_0=418.4113410579559\n",
      "Arrive at the goal! - agent:  3  - stage:  5\n",
      "Resetting Agent:  3\n",
      "Collide! - agent:  7  - stage:  5\n",
      "Resetting Agent:  7\n",
      "Rank-0: policy_step=113352, average_reward_env_0=183.11223636156842\n",
      "Collide! - agent:  7  - stage:  5\n",
      "Resetting Agent:  7\n",
      "Collide! - agent:  1  - stage:  5\n",
      "Resetting Agent:  1\n",
      "Rank-0: policy_step=113440, average_reward_env_0=38.84418532788276\n",
      "Arrive at the goal! - agent:  5  - stage:  5\n",
      "Resetting Agent:  5\n",
      "Arrive at the goal! - agent:  4  - stage:  5\n",
      "Resetting Agent:  4\n",
      "Max step number - agent:  8\n",
      "Resetting Agent:  8\n",
      "Rank-0: policy_step=113832, average_reward_env_1=227.70905147899597\n",
      "Collide! - agent:  2  - stage:  5\n",
      "Resetting Agent:  2\n",
      "Rank-0: policy_step=113896, average_reward_env_0=80.0873711038819\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collide! - agent:  5  - stage:  5\n",
      "Resetting Agent:  5\n",
      "Rank-0: policy_step=114008, average_reward_env_0=23.795393276096046\n",
      "Collide! - agent:  5  - stage:  5\n",
      "Resetting Agent:  5\n",
      "Collide! - agent:  6  - stage:  5\n",
      "Resetting Agent:  6\n",
      "Rank-0: policy_step=114032, average_reward_env_0=64.67707480092007\n",
      "Arrive at the goal! - agent:  1  - stage:  5\n",
      "Resetting Agent:  1\n",
      "Collision detected between /World/Worker_2 and /World/Warehouse/Table_03\n",
      "Arrive at the goal! - agent:  7  - stage:  5\n",
      "Resetting Agent:  7\n",
      "Collide! - agent:  1  - stage:  5\n",
      "Resetting Agent:  1\n",
      "Rank-0: policy_step=114592, average_reward_env_0=76.61930951105975\n",
      "Collide! - agent:  3  - stage:  5\n",
      "Resetting Agent:  3\n",
      "Rank-0: policy_step=114672, average_reward_env_0=97.19017131352018\n",
      "Collide! - agent:  3  - stage:  5\n",
      "Resetting Agent:  3\n",
      "Arrive at the goal! - agent:  4  - stage:  5\n",
      "Resetting Agent:  4\n",
      "Arrive at the goal! - agent:  2  - stage:  5\n",
      "Resetting Agent:  2\n",
      "Collision detected between /World/Worker_3 and /World/Worker_1\n",
      "Collision detected between /World/Worker_3 and /World/Worker_1\n",
      "Collide! - agent:  3  - stage:  5\n",
      "Resetting Agent:  3\n",
      "Rank-0: policy_step=114880, average_reward_env_0=47.65784175707893\n",
      "Collide! - agent:  3  - stage:  5\n",
      "Resetting Agent:  3\n",
      "Collide! - agent:  1  - stage:  5\n",
      "Resetting Agent:  1\n",
      "Rank-0: policy_step=114936, average_reward_env_0=49.80444820596586\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Max step number - agent:  4\n",
      "Resetting Agent:  4\n",
      "Rank-0: policy_step=114992, average_reward_env_0=617.8507365382677\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collide! - agent:  7  - stage:  5\n",
      "Resetting Agent:  7\n",
      "Rank-0: policy_step=115008, average_reward_env_0=194.41748946044405\n",
      "Arrive at the goal! - agent:  5  - stage:  5\n",
      "Resetting Agent:  5\n",
      "Collision detected between /World/Worker_1 and /World/Warehouse/Warehouse/SM_Rackshield_3841/SM_Rackshield_02\n",
      "Collide! - agent:  1  - stage:  5\n",
      "Resetting Agent:  1\n",
      "Rank-0: policy_step=115776, average_reward_env_0=44.82783340484269\n",
      "Arrive at the goal! - agent:  8  - stage:  5\n",
      "Resetting Agent:  8\n",
      "Arrive at the goal! - agent:  4  - stage:  5\n",
      "Resetting Agent:  4\n",
      "Collision detected between /World/Worker_3 and /World/Worker_2\n",
      "Collision detected between /World/Worker_3 and /World/Worker_2\n",
      "Arrive at the goal! - agent:  6  - stage:  5\n",
      "Resetting Agent:  6\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collide! - agent:  1  - stage:  5\n",
      "Resetting Agent:  1\n",
      "Rank-0: policy_step=115984, average_reward_env_0=31.63017772845453\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Arrive at the goal! - agent:  2  - stage:  5\n",
      "Resetting Agent:  2\n",
      "Collide! - agent:  5  - stage:  5\n",
      "Resetting Agent:  5\n",
      "Rank-0: policy_step=116160, average_reward_env_0=76.83115972587319\n",
      "Collide! - agent:  5  - stage:  5\n",
      "Resetting Agent:  5\n",
      "Arrive at the goal! - agent:  3  - stage:  5\n",
      "Resetting Agent:  3\n",
      "Arrive at the goal! - agent:  7  - stage:  5\n",
      "Resetting Agent:  7\n",
      "Collision detected between /World/Worker_2 and /World/Warehouse/Table_02\n",
      "Collision detected between /World/Worker_2 and /World/Warehouse/Table_02\n",
      "Collide! - agent:  1  - stage:  5\n",
      "Resetting Agent:  1\n",
      "Rank-0: policy_step=116640, average_reward_env_0=27.02364884531618\n",
      "Collide! - agent:  2  - stage:  5\n",
      "Resetting Agent:  2\n",
      "Rank-0: policy_step=116648, average_reward_env_0=192.77314006542315\n",
      "Arrive at the goal! - agent:  4  - stage:  5\n",
      "Resetting Agent:  4\n",
      "Collide! - agent:  4  - stage:  5\n",
      "Resetting Agent:  4\n",
      "Rank-0: policy_step=116680, average_reward_env_0=475.9173019245076\n",
      "Collide! - agent:  4  - stage:  5\n",
      "Resetting Agent:  4\n",
      "Collision detected between /World/Worker_4 and /World/Warehouse/Table_12\n",
      "Collide! - agent:  4  - stage:  5\n",
      "Resetting Agent:  4\n",
      "Rank-0: policy_step=116792, average_reward_env_0=301.61729132700725\n",
      "Collide! - agent:  4  - stage:  5\n",
      "Resetting Agent:  4\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Arrive at the goal! - agent:  6  - stage:  5\n",
      "Resetting Agent:  6\n",
      "Max step number - agent:  8\n",
      "Resetting Agent:  8\n",
      "Rank-0: policy_step=117032, average_reward_env_1=257.0890065525803\n",
      "Arrive at the goal! - agent:  5  - stage:  5\n",
      "Resetting Agent:  5\n",
      "Max step number - agent:  6\n",
      "Resetting Agent:  6\n",
      "Rank-0: policy_step=117232, average_reward_env_0=148.59476452171836\n",
      "Collide! - agent:  2  - stage:  5\n",
      "Resetting Agent:  2\n",
      "Rank-0: policy_step=117280, average_reward_env_0=145.63335747646715\n",
      "Collide! - agent:  2  - stage:  5\n",
      "Resetting Agent:  2\n",
      "Collide! - agent:  3  - stage:  5\n",
      "Resetting Agent:  3\n",
      "Rank-0: policy_step=117584, average_reward_env_0=101.6278664302852\n",
      "Collide! - agent:  3  - stage:  5\n",
      "Resetting Agent:  3\n",
      "Collide! - agent:  1  - stage:  5\n",
      "Resetting Agent:  1\n",
      "Arrive at the goal! - agent:  7  - stage:  5\n",
      "Resetting Agent:  7\n",
      "Rank-0: policy_step=117680, average_reward_env_0=27.22626476811351\n",
      "Collide! - agent:  1  - stage:  5\n",
      "Resetting Agent:  1\n",
      "Arrive at the goal! - agent:  4  - stage:  5\n",
      "Resetting Agent:  4\n",
      "Collision detected between /World/Worker_4 and /World/Worker_1\n",
      "Collision detected between /World/Worker_4 and /World/Worker_1\n",
      "Collision detected between /World/Worker_4 and /World/Worker_1\n",
      "Collision detected between /World/Worker_4 and /World/Worker_1\n",
      "Collide! - agent:  6  - stage:  5\n",
      "Resetting Agent:  6\n",
      "Rank-0: policy_step=117856, average_reward_env_0=122.81088313203081\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Arrive at the goal! - agent:  5  - stage:  5\n",
      "Resetting Agent:  5\n",
      "Collide! - agent:  8  - stage:  5\n",
      "Resetting Agent:  8\n",
      "Rank-0: policy_step=117976, average_reward_env_1=186.25545745450998\n",
      "Collide! - agent:  8  - stage:  5\n",
      "Resetting Agent:  8\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collision detected between /World/Worker_3 and /World/Warehouse/Table_08\n",
      "Collide! - agent:  5  - stage:  5\n",
      "Resetting Agent:  5\n",
      "Rank-0: policy_step=118128, average_reward_env_0=133.9079849028489\n",
      "Collide! - agent:  4  - stage:  5\n",
      "Resetting Agent:  4\n",
      "Rank-0: policy_step=118184, average_reward_env_0=280.0550816446917\n",
      "Max step number - agent:  7\n",
      "Resetting Agent:  7\n",
      "Rank-0: policy_step=118208, average_reward_env_0=261.5988000415995\n",
      "Arrive at the goal! - agent:  2  - stage:  5\n",
      "Resetting Agent:  2\n",
      "Collide! - agent:  3  - stage:  5\n",
      "Resetting Agent:  3\n",
      "Rank-0: policy_step=118424, average_reward_env_0=83.03961167904474\n",
      "Collide! - agent:  3  - stage:  5\n",
      "Resetting Agent:  3\n",
      "Collide! - agent:  8  - stage:  5\n",
      "Resetting Agent:  8\n",
      "Rank-0: policy_step=118616, average_reward_env_0=138.42908038118526\n",
      "Collide! - agent:  8  - stage:  5\n",
      "Resetting Agent:  8\n",
      "Collide! - agent:  4  - stage:  5\n",
      "Resetting Agent:  4\n",
      "Rank-0: policy_step=118760, average_reward_env_0=222.94772551806972\n",
      "Collide! - agent:  4  - stage:  5\n",
      "Resetting Agent:  4\n",
      "Collide! - agent:  8  - stage:  5\n",
      "Resetting Agent:  8\n",
      "Rank-0: policy_step=118808, average_reward_env_0=99.97168680007417\n",
      "Arrive at the goal! - agent:  6  - stage:  5\n",
      "Resetting Agent:  6\n",
      "Arrive at the goal! - agent:  1  - stage:  5\n",
      "Resetting Agent:  1\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collide! - agent:  5  - stage:  5\n",
      "Resetting Agent:  5\n",
      "Rank-0: policy_step=119032, average_reward_env_0=116.20036859971196\n",
      "Arrive at the goal! - agent:  7  - stage:  5\n",
      "Resetting Agent:  7\n",
      "Arrive at the goal! - agent:  2  - stage:  5\n",
      "Resetting Agent:  2\n",
      "Collide! - agent:  4  - stage:  5\n",
      "Resetting Agent:  4\n",
      "Rank-0: policy_step=119448, average_reward_env_0=186.38143996397676\n",
      "Arrive at the goal! - agent:  6  - stage:  5\n",
      "Resetting Agent:  6\n",
      "Arrive at the goal! - agent:  1  - stage:  5\n",
      "Resetting Agent:  1\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Arrive at the goal! - agent:  7  - stage:  5\n",
      "Resetting Agent:  7\n",
      "Arrive at the goal! - agent:  3  - stage:  5\n",
      "Resetting Agent:  3\n",
      "Arrive at the goal! - agent:  8  - stage:  5\n",
      "Resetting Agent:  8\n",
      "Arrive at the goal! - agent:  2  - stage:  5\n",
      "Resetting Agent:  2\n",
      "Arrive at the goal! - agent:  5  - stage:  5\n",
      "Resetting Agent:  5\n",
      "Arrive at the goal! - agent:  4  - stage:  5\n",
      "Resetting Agent:  4\n",
      "Collide! - agent:  7  - stage:  5\n",
      "Resetting Agent:  7\n",
      "Rank-0: policy_step=120272, average_reward_env_0=278.35711034012326\n",
      "Collide! - agent:  4  - stage:  5\n",
      "Resetting Agent:  4\n",
      "Rank-0: policy_step=120352, average_reward_env_0=178.11925680605276\n",
      "Collide! - agent:  4  - stage:  5\n",
      "Resetting Agent:  4\n",
      "Max step number - agent:  2\n",
      "Resetting Agent:  2\n",
      "Rank-0: policy_step=120488, average_reward_env_0=242.72952854001196\n",
      "Collision detected between /World/Worker_2 and /World/Warehouse/Rack_04\n",
      "Arrive at the goal! - agent:  6  - stage:  5\n",
      "Resetting Agent:  6\n",
      "Arrive at the goal! - agent:  1  - stage:  5\n",
      "Resetting Agent:  1\n",
      "Max step number - agent:  1\n",
      "Resetting Agent:  1\n",
      "Rank-0: policy_step=120888, average_reward_env_0=82.06828648232661\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collide! - agent:  2  - stage:  5\n",
      "Resetting Agent:  2  Other Direction\n",
      "Rank-0: policy_step=120984, average_reward_env_0=194.34088262524324\n",
      "Collide! - agent:  2  - stage:  5\n",
      "Resetting Agent:  2\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Max step number - agent:  6\n",
      "Resetting Agent:  6\n",
      "Rank-0: policy_step=121056, average_reward_env_0=207.8570524215858\n",
      "Arrive at the goal! - agent:  8  - stage:  5\n",
      "Resetting Agent:  8\n",
      "Arrive at the goal! - agent:  3  - stage:  5\n",
      "Resetting Agent:  3\n",
      "Arrive at the goal! - agent:  5  - stage:  5\n",
      "Resetting Agent:  5\n",
      "Arrive at the goal! - agent:  4  - stage:  5\n",
      "Resetting Agent:  4\n",
      "Max step number - agent:  3\n",
      "Resetting Agent:  3\n",
      "Rank-0: policy_step=121632, average_reward_env_0=133.6134481442023\n",
      "Arrive at the goal! - agent:  7  - stage:  5\n",
      "Resetting Agent:  7\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Max step number - agent:  8\n",
      "Resetting Agent:  8\n",
      "Rank-0: policy_step=122008, average_reward_env_0=157.68574679109125\n",
      "Arrive at the goal! - agent:  2  - stage:  5\n",
      "Resetting Agent:  2\n",
      "Collision detected between /World/Worker_4 and /World/Warehouse/Table_08\n",
      "Arrive at the goal! - agent:  6  - stage:  5\n",
      "Resetting Agent:  6\n",
      "Max step number - agent:  5\n",
      "Resetting Agent:  5\n",
      "Rank-0: policy_step=122232, average_reward_env_0=172.96080709094863\n",
      "Collide! - agent:  3  - stage:  5\n",
      "Resetting Agent:  3\n",
      "Rank-0: policy_step=122312, average_reward_env_0=114.58208275593792\n",
      "Collide! - agent:  3  - stage:  5\n",
      "Resetting Agent:  3\n",
      "Arrive at the goal! - agent:  4  - stage:  5\n",
      "Resetting Agent:  4\n",
      "Arrive at the goal! - agent:  1  - stage:  5\n",
      "Resetting Agent:  1\n",
      "Arrive at the goal! - agent:  7  - stage:  5\n",
      "Resetting Agent:  7\n",
      "Collide! - agent:  2  - stage:  5\n",
      "Resetting Agent:  2\n",
      "Rank-0: policy_step=122768, average_reward_env_0=192.76160949079267\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Arrive at the goal! - agent:  8  - stage:  5\n",
      "Resetting Agent:  8\n",
      "Arrive at the goal! - agent:  3  - stage:  5\n",
      "Resetting Agent:  3\n",
      "Collide! - agent:  5  - stage:  5\n",
      "Resetting Agent:  5\n",
      "Rank-0: policy_step=123320, average_reward_env_0=156.36818480862007\n",
      "Arrive at the goal! - agent:  1  - stage:  5\n",
      "Resetting Agent:  1\n",
      "Max step number - agent:  7\n",
      "Resetting Agent:  7\n",
      "Rank-0: policy_step=123472, average_reward_env_1=310.3667545175857\n",
      "Max step number - agent:  4\n",
      "Resetting Agent:  4\n",
      "Rank-0: policy_step=123560, average_reward_env_0=212.24958121066993\n",
      "Arrive at the goal! - agent:  2  - stage:  5\n",
      "Resetting Agent:  2\n",
      "Arrive at the goal! - agent:  6  - stage:  5\n",
      "Resetting Agent:  6\n",
      "Collide! - agent:  6  - stage:  5\n",
      "Resetting Agent:  6\n",
      "Rank-0: policy_step=123920, average_reward_env_0=219.4154971662977\n",
      "Collide! - agent:  6  - stage:  5\n",
      "Resetting Agent:  6\n",
      "Collide! - agent:  6  - stage:  5\n",
      "Resetting Agent:  6\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collide! - agent:  5  - stage:  5\n",
      "Resetting Agent:  5\n",
      "Rank-0: policy_step=124008, average_reward_env_0=134.38621787069317\n",
      "Collide! - agent:  5  - stage:  5\n",
      "Resetting Agent:  5\n",
      "Max step number - agent:  1\n",
      "Resetting Agent:  1\n",
      "Rank-0: policy_step=124088, average_reward_env_0=116.52800911105805\n",
      "Collision detected between /World/Worker_4 and /World/Warehouse/Table_03\n",
      "Collide! - agent:  5  - stage:  5\n",
      "Resetting Agent:  5\n",
      "Rank-0: policy_step=124296, average_reward_env_0=110.21621792047794\n",
      "Collide! - agent:  8  - stage:  5\n",
      "Resetting Agent:  8\n",
      "Rank-0: policy_step=124344, average_reward_env_0=166.40417320484883\n",
      "Arrive at the goal! - agent:  4  - stage:  5\n",
      "Resetting Agent:  4\n",
      "Arrive at the goal! - agent:  3  - stage:  5\n",
      "Resetting Agent:  3\n",
      "Collide! - agent:  5  - stage:  5\n",
      "Resetting Agent:  5\n",
      "Rank-0: policy_step=124648, average_reward_env_0=97.78181847258156\n",
      "Collide! - agent:  5  - stage:  5\n",
      "Resetting Agent:  5\n",
      "Collide! - agent:  7  - stage:  5\n",
      "Resetting Agent:  7\n",
      "Rank-0: policy_step=124672, average_reward_env_0=264.4200475353758\n",
      "Collide! - agent:  7  - stage:  5\n",
      "Resetting Agent:  7\n",
      "Collide! - agent:  3  - stage:  5\n",
      "Resetting Agent:  3\n",
      "Rank-0: policy_step=124696, average_reward_env_0=142.91502536994233\n",
      "Collide! - agent:  3  - stage:  5\n",
      "Resetting Agent:  3\n",
      "Arrive at the goal! - agent:  2  - stage:  5\n",
      "Resetting Agent:  2\n",
      "Collide! - agent:  1  - stage:  5\n",
      "Resetting Agent:  1\n",
      "Rank-0: policy_step=124864, average_reward_env_0=108.7889823382322\n",
      "Collide! - agent:  1  - stage:  5\n",
      "Resetting Agent:  1\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collide! - agent:  3  - stage:  5\n",
      "Resetting Agent:  3\n",
      "Rank-0: policy_step=125056, average_reward_env_0=123.53662508866415\n",
      "Collide! - agent:  3  - stage:  5\n",
      "Resetting Agent:  3\n",
      "Arrive at the goal! - agent:  6  - stage:  5\n",
      "Resetting Agent:  6\n",
      "Arrive at the goal! - agent:  8  - stage:  5\n",
      "Resetting Agent:  8\n",
      "Collide! - agent:  2  - stage:  5\n",
      "Resetting Agent:  2\n",
      "Rank-0: policy_step=125352, average_reward_env_0=200.82629970271876\n",
      "Collide! - agent:  2  - stage:  5\n",
      "Resetting Agent:  2\n",
      "Arrive at the goal! - agent:  4  - stage:  5\n",
      "Resetting Agent:  4\n",
      "Collide! - agent:  7  - stage:  5\n",
      "Resetting Agent:  7\n",
      "Rank-0: policy_step=125496, average_reward_env_0=231.36523738896648\n",
      "Collide! - agent:  7  - stage:  5\n",
      "Resetting Agent:  7\n",
      "Collide! - agent:  5  - stage:  5\n",
      "Resetting Agent:  5\n",
      "Rank-0: policy_step=125520, average_reward_env_0=91.64659719546616\n",
      "Collide! - agent:  5  - stage:  5\n",
      "Resetting Agent:  5\n",
      "Collide! - agent:  6  - stage:  5\n",
      "Resetting Agent:  6\n",
      "Rank-0: policy_step=125624, average_reward_env_0=214.62693883018736\n",
      "Arrive at the goal! - agent:  1  - stage:  5\n",
      "Resetting Agent:  1\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Arrive at the goal! - agent:  3  - stage:  5\n",
      "Resetting Agent:  3\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collide! - agent:  7  - stage:  5\n",
      "Resetting Agent:  7\n",
      "Rank-0: policy_step=126096, average_reward_env_0=205.13230547711765\n",
      "Collide! - agent:  7  - stage:  5\n",
      "Resetting Agent:  7\n",
      "Collide! - agent:  7  - stage:  5\n",
      "Resetting Agent:  7\n",
      "Collide! - agent:  7  - stage:  5\n",
      "Resetting Agent:  7\n",
      "Collide! - agent:  7  - stage:  5\n",
      "Resetting Agent:  7\n",
      "Arrive at the goal! - agent:  2  - stage:  5\n",
      "Resetting Agent:  2\n",
      "Collide! - agent:  3  - stage:  5\n",
      "Resetting Agent:  3\n",
      "Rank-0: policy_step=126664, average_reward_env_0=129.48788200919583\n",
      "Arrive at the goal! - agent:  1  - stage:  5\n",
      "Resetting Agent:  1\n",
      "Arrive at the goal! - agent:  4  - stage:  5\n",
      "Resetting Agent:  4\n",
      "Collide! - agent:  4  - stage:  5\n",
      "Resetting Agent:  4\n",
      "Arrive at the goal! - agent:  8  - stage:  5\n",
      "Resetting Agent:  8\n",
      "Rank-0: policy_step=126752, average_reward_env_0=245.4911191460243\n",
      "Collide! - agent:  8  - stage:  5\n",
      "Resetting Agent:  8\n",
      "Rank-0: policy_step=126760, average_reward_env_0=183.49248184978114\n",
      "Collide! - agent:  8  - stage:  5\n",
      "Resetting Agent:  8\n",
      "Arrive at the goal! - agent:  5  - stage:  5\n",
      "Resetting Agent:  5\n",
      "Collide! - agent:  7  - stage:  5\n",
      "Resetting Agent:  7\n",
      "Rank-0: policy_step=126888, average_reward_env_0=179.22922133443737\n",
      "Collide! - agent:  7  - stage:  5\n",
      "Resetting Agent:  7\n",
      "Collision detected between /World/Worker_1 and /World/Warehouse/Warehouse/SM_FireExtinguisher_1925/SM_FireExtinguisher_02\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collide! - agent:  2  - stage:  5\n",
      "Resetting Agent:  2\n",
      "Rank-0: policy_step=127088, average_reward_env_0=204.07563524675479\n",
      "Collide! - agent:  2  - stage:  5\n",
      "Resetting Agent:  2\n",
      "Rank-0: policy_step=127296, average_reward_env_0=180.19105198424896\n",
      "Collide! - agent:  2  - stage:  5\n",
      "Resetting Agent:  2\n",
      "Collide! - agent:  4  - stage:  5\n",
      "Resetting Agent:  4\n",
      "Rank-0: policy_step=127496, average_reward_env_0=224.03290613639336\n",
      "Collide! - agent:  4  - stage:  5\n",
      "Resetting Agent:  4\n",
      "Arrive at the goal! - agent:  1  - stage:  5\n",
      "Resetting Agent:  1\n",
      "Arrive at the goal! - agent:  3  - stage:  5\n",
      "Resetting Agent:  3\n",
      "Collide! - agent:  4  - stage:  5\n",
      "Resetting Agent:  4\n",
      "Rank-0: policy_step=127832, average_reward_env_0=201.73200256384897\n",
      "Arrive at the goal! - agent:  5  - stage:  5\n",
      "Resetting Agent:  5\n",
      "Arrive at the goal! - agent:  7  - stage:  5\n",
      "Resetting Agent:  7\n",
      "Collide! - agent:  6  - stage:  5\n",
      "Resetting Agent:  6\n",
      "Rank-0: policy_step=127968, average_reward_env_0=197.8429126772728\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collide! - agent:  6  - stage:  5\n",
      "Resetting Agent:  6\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collide! - agent:  1  - stage:  5\n",
      "Resetting Agent:  1\n",
      "Rank-0: policy_step=128032, average_reward_env_0=146.95669581653644\n",
      "Collide! - agent:  1  - stage:  5\n",
      "Resetting Agent:  1\n",
      "Arrive at the goal! - agent:  2  - stage:  5\n",
      "Resetting Agent:  2\n",
      "Collision detected between /World/Worker_2 and /World/Warehouse/Table_12\n",
      "Arrive at the goal! - agent:  8  - stage:  5\n",
      "Resetting Agent:  8\n",
      "Collide! - agent:  3  - stage:  5\n",
      "Resetting Agent:  3\n",
      "Rank-0: policy_step=128496, average_reward_env_0=127.6935746860961\n",
      "Collide! - agent:  2  - stage:  5\n",
      "Resetting Agent:  2\n",
      "Collide! - agent:  3  - stage:  5\n",
      "Resetting Agent:  3\n",
      "Rank-0: policy_step=128504, average_reward_env_0=178.01454569356474\n",
      "Collide! - agent:  2  - stage:  5\n",
      "Resetting Agent:  2\n",
      "Max step number - agent:  5\n",
      "Resetting Agent:  5\n",
      "Rank-0: policy_step=128728, average_reward_env_0=121.03864335825324\n",
      "Collide! - agent:  8  - stage:  5\n",
      "Resetting Agent:  8\n",
      "Rank-0: policy_step=128784, average_reward_env_0=175.02019609033297\n",
      "Collide! - agent:  6  - stage:  5\n",
      "Resetting Agent:  6\n",
      "Rank-0: policy_step=128792, average_reward_env_0=182.43487983867237\n",
      "Collide! - agent:  6  - stage:  5\n",
      "Resetting Agent:  6\n",
      "Arrive at the goal! - agent:  1  - stage:  5\n",
      "Resetting Agent:  1\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collide! - agent:  2  - stage:  5\n",
      "Resetting Agent:  2\n",
      "Rank-0: policy_step=129008, average_reward_env_0=161.76706431755952\n",
      "Collide! - agent:  2  - stage:  5\n",
      "Resetting Agent:  2\n",
      "Rank-0: policy_step=129040, average_reward_env_0=144.5494084221128\n",
      "Collide! - agent:  2  - stage:  5\n",
      "Resetting Agent:  2\n",
      "Arrive at the goal! - agent:  7  - stage:  5\n",
      "Resetting Agent:  7\n",
      "Arrive at the goal! - agent:  4  - stage:  5\n",
      "Resetting Agent:  4\n",
      "Arrive at the goal! - agent:  3  - stage:  5\n",
      "Collide! - agent:  3  - stage:  5\n",
      "Resetting Agent:  3\n",
      "Rank-0: policy_step=129312, average_reward_env_0=129.3192987766255\n",
      "Collide! - agent:  3  - stage:  5\n",
      "Resetting Agent:  3\n",
      "Rank-0: policy_step=129400, average_reward_env_0=126.74210665459097\n",
      "Collide! - agent:  6  - stage:  5\n",
      "Resetting Agent:  6\n",
      "Rank-0: policy_step=129592, average_reward_env_0=167.7669568006324\n",
      "Arrive at the goal! - agent:  1  - stage:  5\n",
      "Resetting Agent:  1\n",
      "Collide! - agent:  6  - stage:  5\n",
      "Resetting Agent:  6\n",
      "Rank-0: policy_step=129768, average_reward_env_0=149.67323199259135\n",
      "Collide! - agent:  6  - stage:  5\n",
      "Resetting Agent:  6\n",
      "Collide! - agent:  1  - stage:  5\n",
      "Resetting Agent:  1\n",
      "Rank-0: policy_step=129792, average_reward_env_0=160.23497029575876\n",
      "Collide! - agent:  7  - stage:  5\n",
      "Resetting Agent:  7\n",
      "Rank-0: policy_step=129832, average_reward_env_0=192.52492028530602\n",
      "Collide! - agent:  7  - stage:  5\n",
      "Resetting Agent:  7\n",
      "Arrive at the goal! - agent:  8  - stage:  5\n",
      "Resetting Agent:  8\n",
      "Arrive at the goal! - agent:  2  - stage:  5\n",
      "Resetting Agent:  2\n",
      "Collide! - agent:  6  - stage:  5\n",
      "Resetting Agent:  6\n",
      "Rank-0: policy_step=129920, average_reward_env_0=135.02579990075282\n",
      "Collide! - agent:  6  - stage:  5\n",
      "Resetting Agent:  6\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collide! - agent:  5  - stage:  5\n",
      "Resetting Agent:  5  Other Direction\n",
      "Rank-0: policy_step=129976, average_reward_env_0=114.6141147001687\n",
      "Collide! - agent:  3  - stage:  5\n",
      "Resetting Agent:  3\n",
      "Collide! - agent:  5  - stage:  5\n",
      "Resetting Agent:  5\n",
      "Rank-0: policy_step=129984, average_reward_env_0=116.3060348817155\n",
      "Collide! - agent:  3  - stage:  5\n",
      "Resetting Agent:  3\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Arrive at the goal! - agent:  4  - stage:  5\n",
      "Resetting Agent:  4\n",
      "Collision detected between /World/Worker_3 and /World/Warehouse/Warehouse/GroundPlane/CollisionPlane\n",
      "Collide! - agent:  2  - stage:  5\n",
      "Resetting Agent:  2\n",
      "Collide! - agent:  5  - stage:  5\n",
      "Resetting Agent:  5\n",
      "Rank-0: policy_step=130272, average_reward_env_0=147.36324681054197\n",
      "Rank-0: policy_step=130272, average_reward_env_1=104.70288353976403\n",
      "Collide! - agent:  8  - stage:  5\n",
      "Resetting Agent:  8\n",
      "Rank-0: policy_step=130608, average_reward_env_0=180.1989092095719\n",
      "Collide! - agent:  6  - stage:  5\n",
      "Resetting Agent:  6\n",
      "Rank-0: policy_step=130712, average_reward_env_0=127.071493251001\n",
      "Arrive at the goal! - agent:  1  - stage:  5\n",
      "Resetting Agent:  1\n",
      "Collide! - agent:  7  - stage:  5\n",
      "Resetting Agent:  7\n",
      "Rank-0: policy_step=130904, average_reward_env_0=177.23190548114823\n",
      "Collide! - agent:  7  - stage:  5\n",
      "Resetting Agent:  7\n",
      "Arrive at the goal! - agent:  3  - stage:  5\n",
      "Resetting Agent:  3\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collide! - agent:  4  - stage:  5\n",
      "Max step number - agent:  4\n",
      "Resetting Agent:  4\n",
      "Rank-0: policy_step=131032, average_reward_env_0=217.93422704656822\n",
      "Collide! - agent:  1  - stage:  5\n",
      "Resetting Agent:  1\n",
      "Rank-0: policy_step=131144, average_reward_env_0=157.1900717224325\n",
      "Arrive at the goal! - agent:  2  - stage:  5\n",
      "Resetting Agent:  2\n",
      "Collide! - agent:  3  - stage:  5\n",
      "Resetting Agent:  3\n",
      "Rank-0: policy_step=131184, average_reward_env_0=120.44814776102932\n",
      "Collide! - agent:  3  - stage:  5\n",
      "Resetting Agent:  3\n",
      "Arrive at the goal! - agent:  5  - stage:  5\n",
      "Resetting Agent:  5\n",
      "Rank-0: policy_step=131344, average_reward_env_0=109.79659280091852\n",
      "Collide! - agent:  3  - stage:  5\n",
      "Resetting Agent:  3\n",
      "Collision detected between /World/Worker_4 and /World/Warehouse/Warehouse/GroundPlane/CollisionPlane\n",
      "Arrive at the goal! - agent:  6  - stage:  5\n",
      "Resetting Agent:  6\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Arrive at the goal! - agent:  2  - stage:  5\n",
      "Resetting Agent:  2\n",
      "Arrive at the goal! - agent:  1  - stage:  5\n",
      "Resetting Agent:  1\n",
      "Arrive at the goal! - agent:  7  - stage:  5\n",
      "Collide! - agent:  7  - stage:  5\n",
      "Resetting Agent:  7\n",
      "Rank-0: policy_step=132152, average_reward_env_0=174.3850817339624\n",
      "Collide! - agent:  6  - stage:  5\n",
      "Resetting Agent:  6\n",
      "Rank-0: policy_step=132192, average_reward_env_0=130.1503391050342\n",
      "Arrive at the goal! - agent:  8  - stage:  5\n",
      "Resetting Agent:  8\n",
      "Arrive at the goal! - agent:  5  - stage:  5\n",
      "Resetting Agent:  5\n",
      "Arrive at the goal! - agent:  3  - stage:  5\n",
      "Resetting Agent:  3\n",
      "Collide! - agent:  5  - stage:  5\n",
      "Resetting Agent:  5\n",
      "Rank-0: policy_step=132376, average_reward_env_0=119.48162062754527\n",
      "Arrive at the goal! - agent:  4  - stage:  5\n",
      "Resetting Agent:  4\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Arrive at the goal! - agent:  2  - stage:  5\n",
      "Resetting Agent:  2\n",
      "Arrive at the goal! - agent:  7  - stage:  5\n",
      "Resetting Agent:  7\n",
      "Arrive at the goal! - agent:  8  - stage:  5\n",
      "Resetting Agent:  8\n",
      "Arrive at the goal! - agent:  1  - stage:  5\n",
      "Resetting Agent:  1\n",
      "Arrive at the goal! - agent:  5  - stage:  5\n",
      "Resetting Agent:  5\n",
      "Max step number - agent:  2\n",
      "Resetting Agent:  2\n",
      "Rank-0: policy_step=133472, average_reward_env_0=179.93220483503458\n",
      "Arrive at the goal! - agent:  6  - stage:  5\n",
      "Resetting Agent:  6\n",
      "Arrive at the goal! - agent:  3  - stage:  5\n",
      "Resetting Agent:  3\n",
      "Arrive at the goal! - agent:  4  - stage:  5\n",
      "Resetting Agent:  4\n",
      "Collide! - agent:  5  - stage:  5\n",
      "Resetting Agent:  5\n",
      "Rank-0: policy_step=133616, average_reward_env_0=122.55994243290733\n",
      "Collide! - agent:  5  - stage:  5\n",
      "Resetting Agent:  5\n",
      "Max step number - agent:  8\n",
      "Resetting Agent:  8\n",
      "Rank-0: policy_step=133808, average_reward_env_0=209.08811489455422\n",
      "Collide! - agent:  7  - stage:  5\n",
      "Resetting Agent:  7\n",
      "Rank-0: policy_step=133888, average_reward_env_0=185.6495146767559\n",
      "Collide! - agent:  2  - stage:  5\n",
      "Resetting Agent:  2\n",
      "Rank-0: policy_step=133944, average_reward_env_0=167.23556595031536\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collide! - agent:  5  - stage:  5\n",
      "Resetting Agent:  5\n",
      "Rank-0: policy_step=133976, average_reward_env_0=114.46156660822801\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collision detected between /World/Worker_2 and /World/Warehouse/Warehouse/SM_Rackshield_2984/SM_Rackshield_02\n",
      "Collide! - agent:  8  - stage:  5\n",
      "Resetting Agent:  8\n",
      "Rank-0: policy_step=134040, average_reward_env_0=188.1017540005021\n",
      "Arrive at the goal! - agent:  1  - stage:  5\n",
      "Resetting Agent:  1\n",
      "Max step number - agent:  4\n",
      "Resetting Agent:  4\n",
      "Rank-0: policy_step=134232, average_reward_env_0=234.00567461994476\n",
      "Collide! - agent:  2  - stage:  5\n",
      "Resetting Agent:  2\n",
      "Rank-0: policy_step=134304, average_reward_env_0=155.00845702203364\n",
      "Max step number - agent:  1\n",
      "Resetting Agent:  1\n",
      "Rank-0: policy_step=134344, average_reward_env_0=185.4828077480486\n",
      "Collide! - agent:  1  - stage:  5\n",
      "Resetting Agent:  1\n",
      "Collide! - agent:  1  - stage:  5\n",
      "Resetting Agent:  1\n",
      "Arrive at the goal! - agent:  3  - stage:  5\n",
      "Resetting Agent:  3\n",
      "Collide! - agent:  3  - stage:  5\n",
      "Resetting Agent:  3\n",
      "Rank-0: policy_step=134408, average_reward_env_0=134.8350288143244\n",
      "Collide! - agent:  2  - stage:  5\n",
      "Resetting Agent:  2\n",
      "Rank-0: policy_step=134592, average_reward_env_0=144.16509577044414\n",
      "Collide! - agent:  2  - stage:  5\n",
      "Resetting Agent:  2\n",
      "Arrive at the goal! - agent:  7  - stage:  5\n",
      "Resetting Agent:  7\n",
      "Collide! - agent:  6  - stage:  5\n",
      "Resetting Agent:  6\n",
      "Rank-0: policy_step=134888, average_reward_env_0=137.98236433965758\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collision detected between /World/Worker_2 and /World/Warehouse/Warehouse/SM_Rackshield_2984/SM_Rackshield_02\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collide! - agent:  1  - stage:  5\n",
      "Resetting Agent:  1\n",
      "Rank-0: policy_step=135040, average_reward_env_0=174.88268420006318\n",
      "Collide! - agent:  1  - stage:  5\n",
      "Resetting Agent:  1\n",
      "Arrive at the goal! - agent:  8  - stage:  5\n",
      "Resetting Agent:  8\n",
      "Arrive at the goal! - agent:  4  - stage:  5\n",
      "Resetting Agent:  4\n",
      "Arrive at the goal! - agent:  3  - stage:  5\n",
      "Resetting Agent:  3\n",
      "Arrive at the goal! - agent:  7  - stage:  5\n",
      "Resetting Agent:  7\n",
      "Collision detected between /World/Worker_3 and /World/Warehouse/Rack_02\n",
      "Collide! - agent:  2  - stage:  5\n",
      "Resetting Agent:  2\n",
      "Rank-0: policy_step=135768, average_reward_env_0=139.1943548661705\n",
      "Arrive at the goal! - agent:  6  - stage:  5\n",
      "Resetting Agent:  6\n",
      "Collide! - agent:  3  - stage:  5\n",
      "Resetting Agent:  3\n",
      "Rank-0: policy_step=135936, average_reward_env_0=134.98462147037046\n",
      "Collide! - agent:  7  - stage:  5\n",
      "Resetting Agent:  7\n",
      "Rank-0: policy_step=135960, average_reward_env_0=192.8051211421348\n",
      "Collide! - agent:  6  - stage:  5\n",
      "Resetting Agent:  6\n",
      "Rank-0: policy_step=135968, average_reward_env_0=139.27912456627183\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collide! - agent:  1  - stage:  5\n",
      "Resetting Agent:  1\n",
      "Rank-0: policy_step=136032, average_reward_env_0=167.17414399536258\n",
      "Collide! - agent:  1  - stage:  5\n",
      "Resetting Agent:  1\n",
      "Collide! - agent:  1  - stage:  5\n",
      "Resetting Agent:  1\n",
      "Rank-0: policy_step=136072, average_reward_env_0=155.51761003575297\n",
      "Collide! - agent:  1  - stage:  5\n",
      "Resetting Agent:  1\n",
      "Rank-0: policy_step=136216, average_reward_env_0=145.54610460739625\n",
      "Arrive at the goal! - agent:  5  - stage:  5\n",
      "Resetting Agent:  5\n",
      "Arrive at the goal! - agent:  4  - stage:  5\n",
      "Resetting Agent:  4\n",
      "Arrive at the goal! - agent:  2  - stage:  5\n",
      "Resetting Agent:  2\n",
      "Collide! - agent:  5  - stage:  5\n",
      "Resetting Agent:  5\n",
      "Rank-0: policy_step=136528, average_reward_env_0=115.71124129887525\n",
      "Collide! - agent:  5  - stage:  5\n",
      "Resetting Agent:  5\n",
      "Arrive at the goal! - agent:  8  - stage:  5\n",
      "Resetting Agent:  8\n",
      "Arrive at the goal! - agent:  3  - stage:  5\n",
      "Resetting Agent:  3\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Arrive at the goal! - agent:  6  - stage:  5\n",
      "Resetting Agent:  6\n",
      "Collide! - agent:  1  - stage:  5\n",
      "Resetting Agent:  1\n",
      "Rank-0: policy_step=137192, average_reward_env_0=139.64113480174203\n",
      "Max step number - agent:  8\n",
      "Resetting Agent:  8\n",
      "Rank-0: policy_step=137240, average_reward_env_0=212.01482520202728\n",
      "Collide! - agent:  6  - stage:  5\n",
      "Resetting Agent:  6\n",
      "Rank-0: policy_step=137272, average_reward_env_0=140.0043064406044\n",
      "Collide! - agent:  6  - stage:  5\n",
      "Resetting Agent:  6  Other Direction\n",
      "Collide! - agent:  1  - stage:  5\n",
      "Resetting Agent:  1\n",
      "Max step number - agent:  4\n",
      "Resetting Agent:  4\n",
      "Rank-0: policy_step=137432, average_reward_env_0=131.3536994571581\n",
      "Rank-0: policy_step=137432, average_reward_env_1=252.0271040530395\n",
      "Collide! - agent:  1  - stage:  5\n",
      "Resetting Agent:  1\n",
      "Collide! - agent:  1  - stage:  5\n",
      "Resetting Agent:  1\n",
      "Arrive at the goal! - agent:  2  - stage:  5\n",
      "Resetting Agent:  2\n",
      "Collide! - agent:  7  - stage:  5\n",
      "Resetting Agent:  7\n",
      "Rank-0: policy_step=137672, average_reward_env_0=182.56538414519957\n",
      "Arrive at the goal! - agent:  3  - stage:  5\n",
      "Resetting Agent:  3\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collide! - agent:  1  - stage:  5\n",
      "Resetting Agent:  1\n",
      "Rank-0: policy_step=138016, average_reward_env_0=125.17936776304516\n",
      "Collide! - agent:  1  - stage:  5\n",
      "Resetting Agent:  1\n",
      "Collide! - agent:  2  - stage:  5\n",
      "Resetting Agent:  2\n",
      "Rank-0: policy_step=138048, average_reward_env_0=150.03213173952173\n",
      "Collide! - agent:  2  - stage:  5\n",
      "Resetting Agent:  2\n",
      "Collide! - agent:  3  - stage:  5\n",
      "Resetting Agent:  3\n",
      "Rank-0: policy_step=138088, average_reward_env_0=146.88100510835852\n",
      "Collide! - agent:  3  - stage:  5\n",
      "Resetting Agent:  3\n",
      "Collide! - agent:  8  - stage:  5\n",
      "Resetting Agent:  8\n",
      "Rank-0: policy_step=138096, average_reward_env_0=194.01351731147233\n",
      "Collide! - agent:  8  - stage:  5\n",
      "Resetting Agent:  8\n",
      "Collide! - agent:  4  - stage:  5\n",
      "Resetting Agent:  4\n",
      "Rank-0: policy_step=138184, average_reward_env_0=236.0870859327116\n",
      "Collide! - agent:  4  - stage:  5\n",
      "Resetting Agent:  4\n",
      "Arrive at the goal! - agent:  5  - stage:  5\n",
      "Resetting Agent:  5\n",
      "Arrive at the goal! - agent:  6  - stage:  5\n",
      "Resetting Agent:  6\n",
      "Collide! - agent:  6  - stage:  5\n",
      "Resetting Agent:  6\n",
      "Rank-0: policy_step=138520, average_reward_env_0=140.02028061491268\n",
      "Arrive at the goal! - agent:  7  - stage:  5\n",
      "Resetting Agent:  7\n",
      "Collide! - agent:  2  - stage:  5\n",
      "Resetting Agent:  2\n",
      "Rank-0: policy_step=138616, average_reward_env_0=143.45362028121224\n",
      "Collision detected between /World/Worker_4 and /World/Warehouse/Warehouse/SM_SignCVer_190/SM_SignCVer_10\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Arrive at the goal! - agent:  1  - stage:  5\n",
      "Resetting Agent:  1\n",
      "Arrive at the goal! - agent:  8  - stage:  5\n",
      "Resetting Agent:  8\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collide! - agent:  1  - stage:  5\n",
      "Resetting Agent:  1\n",
      "Collide! - agent:  8  - stage:  5\n",
      "Resetting Agent:  8\n",
      "Rank-0: policy_step=139048, average_reward_env_0=125.84761794180982\n",
      "Rank-0: policy_step=139048, average_reward_env_1=190.71377878352993\n",
      "Collide! - agent:  1  - stage:  5\n",
      "Resetting Agent:  1\n",
      "Collide! - agent:  1  - stage:  5\n",
      "Resetting Agent:  1\n",
      "Arrive at the goal! - agent:  3  - stage:  5\n",
      "Resetting Agent:  3\n",
      "Arrive at the goal! - agent:  7  - stage:  5\n",
      "Resetting Agent:  7\n",
      "Collide! - agent:  1  - stage:  5\n",
      "Resetting Agent:  1\n",
      "Rank-0: policy_step=139368, average_reward_env_0=119.52371112886634\n",
      "Collision detected between /World/Worker_1 and /World/Warehouse/Warehouse/SM_Rackshield_3841/SM_Rackshield_02\n",
      "Arrive at the goal! - agent:  6  - stage:  5\n",
      "Resetting Agent:  6\n",
      "Collide! - agent:  4  - stage:  5\n",
      "Resetting Agent:  4\n",
      "Rank-0: policy_step=139680, average_reward_env_0=222.60880357028967\n",
      "Collide! - agent:  4  - stage:  5\n",
      "Resetting Agent:  4\n",
      "Collide! - agent:  6  - stage:  5\n",
      "Resetting Agent:  6\n",
      "Rank-0: policy_step=139720, average_reward_env_0=141.39562350658102\n",
      "Collide! - agent:  6  - stage:  5\n",
      "Resetting Agent:  6\n",
      "Max step number - agent:  5\n",
      "Resetting Agent:  5\n",
      "Rank-0: policy_step=139736, average_reward_env_0=127.07293861857606\n",
      "Arrive at the goal! - agent:  2  - stage:  5\n",
      "Resetting Agent:  2\n",
      "Collide! - agent:  2  - stage:  5\n",
      "Resetting Agent:  2\n",
      "Rank-0: policy_step=139960, average_reward_env_0=144.16919723509204\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Arrive at the goal! - agent:  8  - stage:  5\n",
      "Resetting Agent:  8\n",
      "Arrive at the goal! - agent:  1  - stage:  5\n",
      "Resetting Agent:  1\n",
      "Collide! - agent:  2  - stage:  5\n",
      "Resetting Agent:  2\n",
      "Rank-0: policy_step=140200, average_reward_env_1=136.13104573764755\n",
      "Collide! - agent:  2  - stage:  5\n",
      "Resetting Agent:  2\n",
      "Arrive at the goal! - agent:  3  - stage:  5\n",
      "Resetting Agent:  3\n",
      "Collide! - agent:  1  - stage:  5\n",
      "Resetting Agent:  1\n",
      "Rank-0: policy_step=140320, average_reward_env_0=115.98795771192414\n",
      "Collide! - agent:  1  - stage:  5\n",
      "Resetting Agent:  1\n",
      "Arrive at the goal! - agent:  7  - stage:  5\n",
      "Resetting Agent:  7\n",
      "Arrive at the goal! - agent:  6  - stage:  5\n",
      "Resetting Agent:  6\n",
      "Collide! - agent:  2  - stage:  5\n",
      "Resetting Agent:  2\n",
      "Rank-0: policy_step=140576, average_reward_env_0=129.6402063022095\n",
      "Collide! - agent:  2  - stage:  5\n",
      "Resetting Agent:  2\n",
      "Collide! - agent:  6  - stage:  5\n",
      "Resetting Agent:  6\n",
      "Collide! - agent:  8  - stage:  5\n",
      "Resetting Agent:  8\n",
      "Rank-0: policy_step=140648, average_reward_env_0=139.89809939044926\n",
      "Rank-0: policy_step=140648, average_reward_env_1=188.6517540780031\n",
      "Collide! - agent:  8  - stage:  5\n",
      "Resetting Agent:  8\n",
      "Collide! - agent:  6  - stage:  5\n",
      "Resetting Agent:  6\n",
      "Rank-0: policy_step=140704, average_reward_env_0=131.24582785257914\n",
      "Arrive at the goal! - agent:  4  - stage:  5\n",
      "Resetting Agent:  4\n",
      "Max step number - agent:  7\n",
      "Resetting Agent:  7\n",
      "Rank-0: policy_step=140872, average_reward_env_0=204.8781200876084\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Arrive at the goal! - agent:  5  - stage:  5\n",
      "Resetting Agent:  5\n",
      "Arrive at the goal! - agent:  1  - stage:  5\n",
      "Resetting Agent:  1\n",
      "Max step number - agent:  3\n",
      "Resetting Agent:  3\n",
      "Rank-0: policy_step=141296, average_reward_env_0=164.77850787862013\n",
      "Collision detected between /World/Worker_2 and /World/Warehouse/Table_12\n",
      "Collide! - agent:  3  - stage:  5\n",
      "Resetting Agent:  3\n",
      "Arrive at the goal! - agent:  8  - stage:  5\n",
      "Resetting Agent:  8\n",
      "Rank-0: policy_step=141360, average_reward_env_0=154.29026463512224\n",
      "Collide! - agent:  3  - stage:  5\n",
      "Resetting Agent:  3\n",
      "Arrive at the goal! - agent:  2  - stage:  5\n",
      "Resetting Agent:  2\n",
      "Collide! - agent:  3  - stage:  5\n",
      "Resetting Agent:  3\n",
      "Rank-0: policy_step=141536, average_reward_env_0=145.31707450593441\n",
      "Collide! - agent:  3  - stage:  5\n",
      "Resetting Agent:  3\n",
      "Arrive at the goal! - agent:  6  - stage:  5\n",
      "Resetting Agent:  6\n",
      "Arrive at the goal! - agent:  4  - stage:  5\n",
      "Resetting Agent:  4\n",
      "Arrive at the goal! - agent:  5  - stage:  5\n",
      "Resetting Agent:  5\n",
      "Collide! - agent:  1  - stage:  5\n",
      "Resetting Agent:  1\n",
      "Rank-0: policy_step=141912, average_reward_env_0=125.69535151954751\n",
      "Arrive at the goal! - agent:  7  - stage:  5\n",
      "Resetting Agent:  7\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collide! - agent:  3  - stage:  5\n",
      "Resetting Agent:  3\n",
      "Rank-0: policy_step=142016, average_reward_env_0=137.131727430261\n",
      "Collide! - agent:  3  - stage:  5\n",
      "Resetting Agent:  3\n",
      "Collide! - agent:  7  - stage:  5\n",
      "Resetting Agent:  7\n",
      "Rank-0: policy_step=142232, average_reward_env_0=203.19777553101957\n",
      "Collide! - agent:  7  - stage:  5\n",
      "Resetting Agent:  7\n",
      "Arrive at the goal! - agent:  8  - stage:  5\n",
      "Resetting Agent:  8\n",
      "Collide! - agent:  2  - stage:  5\n",
      "Resetting Agent:  2\n",
      "Rank-0: policy_step=142344, average_reward_env_0=128.2166423367844\n",
      "Collide! - agent:  2  - stage:  5\n",
      "Resetting Agent:  2\n",
      "Collide! - agent:  2  - stage:  5\n",
      "Resetting Agent:  2\n",
      "Arrive at the goal! - agent:  4  - stage:  5\n",
      "Resetting Agent:  4\n",
      "Collision detected between /World/Worker_3 and /World/Warehouse/Table_08\n",
      "Collide! - agent:  3  - stage:  5\n",
      "Resetting Agent:  3\n",
      "Collide! - agent:  5  - stage:  5\n",
      "Resetting Agent:  5\n",
      "Rank-0: policy_step=142792, average_reward_env_0=132.9660040529113\n",
      "Rank-0: policy_step=142792, average_reward_env_1=141.61303180127206\n",
      "Collide! - agent:  3  - stage:  5\n",
      "Resetting Agent:  3\n",
      "Collide! - agent:  1  - stage:  5\n",
      "Resetting Agent:  1\n",
      "Rank-0: policy_step=142824, average_reward_env_0=129.22551678819315\n",
      "Collide! - agent:  1  - stage:  5\n",
      "Resetting Agent:  1\n",
      "Max step number - agent:  4\n",
      "Resetting Agent:  4\n",
      "Rank-0: policy_step=142888, average_reward_env_0=246.1942730568353\n",
      "Arrive at the goal! - agent:  6  - stage:  5\n",
      "Resetting Agent:  6\n",
      "Collide! - agent:  1  - stage:  5\n",
      "Resetting Agent:  1\n",
      "Rank-0: policy_step=142944, average_reward_env_0=117.98974133453538\n",
      "Collide! - agent:  1  - stage:  5\n",
      "Resetting Agent:  1\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collide! - agent:  2  - stage:  5\n",
      "Resetting Agent:  2\n",
      "Rank-0: policy_step=143048, average_reward_env_0=121.87811303635402\n",
      "Collide! - agent:  2  - stage:  5\n",
      "Resetting Agent:  2\n",
      "Collide! - agent:  3  - stage:  5\n",
      "Resetting Agent:  3\n",
      "Rank-0: policy_step=143144, average_reward_env_0=126.12189974767026\n",
      "Arrive at the goal! - agent:  7  - stage:  5\n",
      "Resetting Agent:  7\n",
      "Collide! - agent:  2  - stage:  5\n",
      "Resetting Agent:  2\n",
      "Rank-0: policy_step=143272, average_reward_env_0=117.71111019577624\n",
      "Arrive at the goal! - agent:  8  - stage:  5\n",
      "Resetting Agent:  8\n",
      "Collide! - agent:  8  - stage:  5\n",
      "Resetting Agent:  8\n",
      "Rank-0: policy_step=143648, average_reward_env_0=207.82919851484598\n",
      "Collide! - agent:  8  - stage:  5\n",
      "Resetting Agent:  8\n",
      "Collide! - agent:  6  - stage:  5\n",
      "Resetting Agent:  6\n",
      "Rank-0: policy_step=143704, average_reward_env_0=144.2551635479051\n",
      "Collide! - agent:  6  - stage:  5\n",
      "Resetting Agent:  6\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Arrive at the goal! - agent:  4  - stage:  5\n",
      "Resetting Agent:  4\n",
      "Collide! - agent:  3  - stage:  5\n",
      "Resetting Agent:  3\n",
      "Rank-0: policy_step=144080, average_reward_env_0=127.14567574350896\n",
      "Arrive at the goal! - agent:  2  - stage:  5\n",
      "Resetting Agent:  2\n",
      "Collide! - agent:  5  - stage:  5\n",
      "Resetting Agent:  5\n",
      "Rank-0: policy_step=144160, average_reward_env_0=135.9975825549887\n",
      "Collide! - agent:  5  - stage:  5\n",
      "Resetting Agent:  5\n",
      "Collide! - agent:  1  - stage:  5\n",
      "Resetting Agent:  1\n",
      "Rank-0: policy_step=144232, average_reward_env_0=119.56585187343482\n",
      "Collide! - agent:  1  - stage:  5\n",
      "Resetting Agent:  1\n",
      "Arrive at the goal! - agent:  7  - stage:  5\n",
      "Resetting Agent:  7\n",
      "Collide! - agent:  8  - stage:  5\n",
      "Resetting Agent:  8\n",
      "Rank-0: policy_step=144288, average_reward_env_0=196.81260459261364\n",
      "Collide! - agent:  8  - stage:  5\n",
      "Resetting Agent:  8\n",
      "Collide! - agent:  3  - stage:  5\n",
      "Resetting Agent:  3\n",
      "Rank-0: policy_step=144568, average_reward_env_0=117.24776649876421\n",
      "Collide! - agent:  3  - stage:  5\n",
      "Resetting Agent:  3\n",
      "Collide! - agent:  7  - stage:  5\n",
      "Resetting Agent:  7\n",
      "Rank-0: policy_step=144600, average_reward_env_0=210.92621480044005\n",
      "Collide! - agent:  4  - stage:  5\n",
      "Resetting Agent:  4\n",
      "Rank-0: policy_step=144664, average_reward_env_0=242.44474943809388\n",
      "Collide! - agent:  4  - stage:  5\n",
      "Resetting Agent:  4\n",
      "Collide! - agent:  5  - stage:  5\n",
      "Resetting Agent:  5\n",
      "Rank-0: policy_step=144760, average_reward_env_0=130.35608273936546\n",
      "Collide! - agent:  1  - stage:  5\n",
      "Resetting Agent:  1\n",
      "Rank-0: policy_step=144776, average_reward_env_0=118.11649128204552\n",
      "Collide! - agent:  1  - stage:  5\n",
      "Resetting Agent:  1\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collide! - agent:  1  - stage:  5\n",
      "Resetting Agent:  1\n",
      "Rank-0: policy_step=145064, average_reward_env_0=118.92547600657755\n",
      "Arrive at the goal! - agent:  8  - stage:  5\n",
      "Resetting Agent:  8\n",
      "Arrive at the goal! - agent:  2  - stage:  5\n",
      "Resetting Agent:  2\n",
      "Collision detected between /World/Worker_4 and /World/Warehouse/Warehouse/GroundPlane/CollisionPlane\n",
      "Collide! - agent:  1  - stage:  5\n",
      "Resetting Agent:  1\n",
      "Arrive at the goal! - agent:  6  - stage:  5\n",
      "Resetting Agent:  6\n",
      "Rank-0: policy_step=145544, average_reward_env_0=117.98517807169604\n",
      "Collide! - agent:  1  - stage:  5\n",
      "Resetting Agent:  1\n",
      "Arrive at the goal! - agent:  7  - stage:  5\n",
      "Resetting Agent:  7\n",
      "Arrive at the goal! - agent:  4  - stage:  5\n",
      "Resetting Agent:  4\n",
      "Arrive at the goal! - agent:  5  - stage:  5\n",
      "Resetting Agent:  5\n",
      "Collision detected between /World/Worker_1 and /World/Warehouse/Table_08\n",
      "Collide! - agent:  3  - stage:  5\n",
      "Resetting Agent:  3\n",
      "Rank-0: policy_step=145864, average_reward_env_0=115.3565556133217\n",
      "Collide! - agent:  3  - stage:  5\n",
      "Resetting Agent:  3\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collision detected between /World/Worker_4 and /World/Warehouse/Warehouse/SM_SignCVer_155/SM_SignCVer_10\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collide! - agent:  2  - stage:  5\n",
      "Resetting Agent:  2\n",
      "Rank-0: policy_step=146016, average_reward_env_0=111.60495461310647\n",
      "Collide! - agent:  2  - stage:  5\n",
      "Resetting Agent:  2\n",
      "Collide! - agent:  8  - stage:  5\n",
      "Resetting Agent:  8\n",
      "Rank-0: policy_step=146096, average_reward_env_0=197.35012562046978\n",
      "Collision detected between /World/Worker_4 and /World/Warehouse/Warehouse/SM_FloorDecal_StripeFull_4m67_1162/SM_FloorDecal_StripeFull_4m\n",
      "Collide! - agent:  8  - stage:  5\n",
      "Resetting Agent:  8\n",
      "Arrive at the goal! - agent:  1  - stage:  5\n",
      "Resetting Agent:  1\n",
      "Collide! - agent:  4  - stage:  5\n",
      "Resetting Agent:  4\n",
      "Rank-0: policy_step=146624, average_reward_env_0=241.46994530235966\n",
      "Collide! - agent:  4  - stage:  5\n",
      "Resetting Agent:  4\n",
      "Collide! - agent:  5  - stage:  5\n",
      "Resetting Agent:  5\n",
      "Rank-0: policy_step=146896, average_reward_env_0=135.04760400462538\n",
      "Collide! - agent:  5  - stage:  5\n",
      "Resetting Agent:  5\n",
      "Max step number - agent:  6\n",
      "Resetting Agent:  6\n",
      "Rank-0: policy_step=146912, average_reward_env_0=146.76676295289224\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Arrive at the goal! - agent:  3  - stage:  5\n",
      "Resetting Agent:  3\n",
      "Arrive at the goal! - agent:  7  - stage:  5\n",
      "Resetting Agent:  7\n",
      "Arrive at the goal! - agent:  8  - stage:  5\n",
      "Resetting Agent:  8\n",
      "Collide! - agent:  8  - stage:  5\n",
      "Resetting Agent:  8\n",
      "Rank-0: policy_step=147304, average_reward_env_0=193.45955751090852\n",
      "Collide! - agent:  8  - stage:  5\n",
      "Resetting Agent:  8\n",
      "Arrive at the goal! - agent:  1  - stage:  5\n",
      "Resetting Agent:  1\n",
      "Arrive at the goal! - agent:  2  - stage:  5\n",
      "Resetting Agent:  2\n",
      "Arrive at the goal! - agent:  4  - stage:  5\n",
      "Resetting Agent:  4\n",
      "Max step number - agent:  7\n",
      "Resetting Agent:  7\n",
      "Rank-0: policy_step=147800, average_reward_env_0=224.7143624211773\n",
      "Arrive at the goal! - agent:  6  - stage:  5\n",
      "Resetting Agent:  6\n",
      "Arrive at the goal! - agent:  3  - stage:  5\n",
      "Resetting Agent:  3\n",
      "Collide! - agent:  6  - stage:  5\n",
      "Resetting Agent:  6\n",
      "Rank-0: policy_step=147888, average_reward_env_0=147.02789336771005\n",
      "Collide! - agent:  6  - stage:  5\n",
      "Resetting Agent:  6\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collide! - agent:  5  - stage:  5\n",
      "Resetting Agent:  5\n",
      "Rank-0: policy_step=147976, average_reward_env_0=130.8501933496322\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collide! - agent:  5  - stage:  5\n",
      "Resetting Agent:  5\n",
      "Rank-0: policy_step=148112, average_reward_env_0=129.53535951790158\n",
      "Collide! - agent:  5  - stage:  5\n",
      "Resetting Agent:  5\n",
      "Arrive at the goal! - agent:  8  - stage:  5\n",
      "Resetting Agent:  8\n",
      "Collide! - agent:  8  - stage:  5\n",
      "Resetting Agent:  8\n",
      "Rank-0: policy_step=148264, average_reward_env_0=191.0514691531706\n",
      "Collide! - agent:  8  - stage:  5\n",
      "Resetting Agent:  8\n",
      "Collide! - agent:  8  - stage:  5\n",
      "Resetting Agent:  8\n",
      "Rank-0: policy_step=148320, average_reward_env_0=180.03825796655283\n",
      "Arrive at the goal! - agent:  1  - stage:  5\n",
      "Resetting Agent:  1\n",
      "Collide! - agent:  6  - stage:  5\n",
      "Resetting Agent:  6\n",
      "Rank-0: policy_step=148608, average_reward_env_0=140.39478667616578\n",
      "Collide! - agent:  6  - stage:  5\n",
      "Resetting Agent:  6\n",
      "Arrive at the goal! - agent:  3  - stage:  5\n",
      "Resetting Agent:  3\n",
      "Max step number - agent:  1\n",
      "Resetting Agent:  1\n",
      "Rank-0: policy_step=148752, average_reward_env_0=141.4206145283782\n",
      "Arrive at the goal! - agent:  2  - stage:  5\n",
      "Resetting Agent:  2\n",
      "Arrive at the goal! - agent:  7  - stage:  5\n",
      "Resetting Agent:  7\n",
      "Collision detected between /World/Worker_4 and /World/Warehouse/Rack_02\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Arrive at the goal! - agent:  4  - stage:  5\n",
      "Resetting Agent:  4\n",
      "Collide! - agent:  7  - stage:  5\n",
      "Resetting Agent:  7\n",
      "Rank-0: policy_step=149024, average_reward_env_0=220.61961586840684\n",
      "Collide! - agent:  3  - stage:  5\n",
      "Resetting Agent:  3\n",
      "Rank-0: policy_step=149056, average_reward_env_0=128.2222327810995\n",
      "Collide! - agent:  3  - stage:  5\n",
      "Resetting Agent:  3\n",
      "Collide! - agent:  2  - stage:  5\n",
      "Resetting Agent:  2\n",
      "Collide! - agent:  6  - stage:  5\n",
      "Resetting Agent:  6\n",
      "Rank-0: policy_step=149160, average_reward_env_0=124.7436318823684\n",
      "Rank-0: policy_step=149160, average_reward_env_1=141.3721178083786\n",
      "Collide! - agent:  2  - stage:  5\n",
      "Resetting Agent:  2\n",
      "Collide! - agent:  6  - stage:  5\n",
      "Resetting Agent:  6\n",
      "Collide! - agent:  1  - stage:  5\n",
      "Resetting Agent:  1\n",
      "Rank-0: policy_step=149184, average_reward_env_0=117.54988273602055\n",
      "Arrive at the goal! - agent:  5  - stage:  5\n",
      "Resetting Agent:  5\n",
      "Arrive at the goal! - agent:  8  - stage:  5\n",
      "Resetting Agent:  8\n",
      "Collide! - agent:  5  - stage:  5\n",
      "Resetting Agent:  5\n",
      "Rank-0: policy_step=149552, average_reward_env_0=136.0866326452296\n",
      "Collide! - agent:  7  - stage:  5\n",
      "Resetting Agent:  7\n",
      "Rank-0: policy_step=149680, average_reward_env_0=211.41779930664114\n",
      "Max step number - agent:  4\n",
      "Resetting Agent:  4\n",
      "Rank-0: policy_step=149832, average_reward_env_0=255.61049071113356\n",
      "Collision detected between /World/Worker_2 and /World/Warehouse/Table_12\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Arrive at the goal! - agent:  3  - stage:  5\n",
      "Resetting Agent:  3\n",
      "Collision detected between /World/Worker_2 and /World/Warehouse/Warehouse/SM_WallA_6M18/SM_WallA_6M/SM_WallA_6M/Section0\n",
      "Collision detected between /World/Worker_2 and /World/Warehouse/Warehouse/SM_WallA_6M18/SM_WallA_6M/SM_WallA_6M/Section1\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collide! - agent:  5  - stage:  5\n",
      "Resetting Agent:  5\n",
      "Rank-0: policy_step=150136, average_reward_env_0=131.48029947284746\n",
      "Collide! - agent:  5  - stage:  5\n",
      "Resetting Agent:  5\n",
      "Arrive at the goal! - agent:  8  - stage:  5\n",
      "Resetting Agent:  8\n",
      "Collide! - agent:  5  - stage:  5\n",
      "Resetting Agent:  5\n",
      "Arrive at the goal! - agent:  6  - stage:  5\n",
      "Collide! - agent:  6  - stage:  5\n",
      "Resetting Agent:  6\n",
      "Rank-0: policy_step=150208, average_reward_env_0=145.41664867008802\n",
      "Collide! - agent:  7  - stage:  5\n",
      "Resetting Agent:  7\n",
      "Rank-0: policy_step=150488, average_reward_env_0=203.5309617764828\n",
      "Collide! - agent:  3  - stage:  5\n",
      "Resetting Agent:  3\n",
      "Rank-0: policy_step=150576, average_reward_env_0=135.2176515222947\n",
      "Collide! - agent:  1  - stage:  5\n",
      "Resetting Agent:  1\n",
      "Rank-0: policy_step=150584, average_reward_env_0=100.61502224971446\n",
      "Collide! - agent:  1  - stage:  5\n",
      "Resetting Agent:  1\n",
      "Arrive at the goal! - agent:  4  - stage:  5\n",
      "Resetting Agent:  4\n",
      "Collide! - agent:  1  - stage:  5\n",
      "Resetting Agent:  1\n",
      "Rank-0: policy_step=150888, average_reward_env_0=98.49163661706005\n",
      "Arrive at the goal! - agent:  8  - stage:  5\n",
      "Resetting Agent:  8\n",
      "Collide! - agent:  4  - stage:  5\n",
      "Resetting Agent:  4\n",
      "Rank-0: policy_step=150936, average_reward_env_0=251.0893474412304\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collide! - agent:  2  - stage:  5\n",
      "Resetting Agent:  2\n",
      "Rank-0: policy_step=151032, average_reward_env_0=100.0638337726994\n",
      "Collide! - agent:  2  - stage:  5\n",
      "Resetting Agent:  2\n",
      "Arrive at the goal! - agent:  5  - stage:  5\n",
      "Resetting Agent:  5\n",
      "Collide! - agent:  8  - stage:  5\n",
      "Resetting Agent:  8\n",
      "Rank-0: policy_step=151232, average_reward_env_0=196.2850245332981\n",
      "Collide! - agent:  3  - stage:  5\n",
      "Resetting Agent:  3\n",
      "Rank-0: policy_step=151352, average_reward_env_0=106.17427613463501\n",
      "Arrive at the goal! - agent:  7  - stage:  5\n",
      "Resetting Agent:  7\n",
      "Collide! - agent:  3  - stage:  5\n",
      "Resetting Agent:  3\n",
      "Rank-0: policy_step=151528, average_reward_env_0=104.67077651797501\n",
      "Collide! - agent:  3  - stage:  5\n",
      "Resetting Agent:  3\n",
      "Arrive at the goal! - agent:  6  - stage:  5\n",
      "Collide! - agent:  6  - stage:  5\n",
      "Resetting Agent:  6\n",
      "Rank-0: policy_step=151536, average_reward_env_0=141.61427997240747\n",
      "Collide! - agent:  2  - stage:  5\n",
      "Resetting Agent:  2\n",
      "Rank-0: policy_step=151616, average_reward_env_0=102.3478328726684\n",
      "Collide! - agent:  2  - stage:  5\n",
      "Resetting Agent:  2\n",
      "Collide! - agent:  8  - stage:  5\n",
      "Resetting Agent:  8\n",
      "Rank-0: policy_step=151736, average_reward_env_0=187.217820043228\n",
      "Collide! - agent:  8  - stage:  5\n",
      "Resetting Agent:  8\n",
      "Arrive at the goal! - agent:  4  - stage:  5\n",
      "Resetting Agent:  4\n",
      "Collide! - agent:  3  - stage:  5\n",
      "Resetting Agent:  3\n",
      "Rank-0: policy_step=151848, average_reward_env_0=90.17782557259412\n",
      "Collide! - agent:  3  - stage:  5\n",
      "Resetting Agent:  3\n",
      "Collide! - agent:  5  - stage:  5\n",
      "Resetting Agent:  5\n",
      "Rank-0: policy_step=151928, average_reward_env_0=131.41902343006257\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collide! - agent:  5  - stage:  5\n",
      "Resetting Agent:  5\n",
      "Rank-0: policy_step=151976, average_reward_env_0=114.80253781060455\n",
      "Collide! - agent:  5  - stage:  5\n",
      "Resetting Agent:  5\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collide! - agent:  6  - stage:  5\n",
      "Resetting Agent:  6\n",
      "Rank-0: policy_step=152136, average_reward_env_0=151.24438970714615\n",
      "Arrive at the goal! - agent:  1  - stage:  5\n",
      "Resetting Agent:  1\n",
      "Collision detected between /World/Worker_4 and /World/Warehouse/Rack_02\n",
      "Arrive at the goal! - agent:  7  - stage:  5\n",
      "Resetting Agent:  7\n",
      "Collide! - agent:  3  - stage:  5\n",
      "Resetting Agent:  3\n",
      "Arrive at the goal! - agent:  8  - stage:  5\n",
      "Resetting Agent:  8\n",
      "Rank-0: policy_step=152736, average_reward_env_0=92.5834686035425\n",
      "Collide! - agent:  1  - stage:  5\n",
      "Resetting Agent:  1\n",
      "Rank-0: policy_step=152760, average_reward_env_0=83.16528217274097\n",
      "Arrive at the goal! - agent:  2  - stage:  5\n",
      "Resetting Agent:  2\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Arrive at the goal! - agent:  4  - stage:  5\n",
      "Resetting Agent:  4\n",
      "Collide! - agent:  5  - stage:  5\n",
      "Resetting Agent:  5\n",
      "Rank-0: policy_step=153168, average_reward_env_0=114.75892618004406\n",
      "Collide! - agent:  5  - stage:  5\n",
      "Resetting Agent:  5\n",
      "Collide! - agent:  7  - stage:  5\n",
      "Resetting Agent:  7\n",
      "Rank-0: policy_step=153256, average_reward_env_0=213.10245853682397\n",
      "Collide! - agent:  1  - stage:  5\n",
      "Resetting Agent:  1\n",
      "Collide! - agent:  7  - stage:  5\n",
      "Resetting Agent:  7\n",
      "Rank-0: policy_step=153264, average_reward_env_0=68.74376400043371\n",
      "Collide! - agent:  5  - stage:  5\n",
      "Resetting Agent:  5\n",
      "Rank-0: policy_step=153456, average_reward_env_0=92.16615798315064\n",
      "Collide! - agent:  5  - stage:  5\n",
      "Resetting Agent:  5\n",
      "Collide! - agent:  5  - stage:  5\n",
      "Resetting Agent:  5\n",
      "Rank-0: policy_step=153552, average_reward_env_0=88.41092181449613\n",
      "Collide! - agent:  5  - stage:  5\n",
      "Resetting Agent:  5\n",
      "Arrive at the goal! - agent:  6  - stage:  5\n",
      "Resetting Agent:  6\n",
      "Collide! - agent:  1  - stage:  5\n",
      "Resetting Agent:  1\n",
      "Rank-0: policy_step=153616, average_reward_env_0=62.96322476275818\n",
      "Collide! - agent:  1  - stage:  5\n",
      "Resetting Agent:  1\n",
      "Arrive at the goal! - agent:  2  - stage:  5\n",
      "Resetting Agent:  2\n",
      "Collide! - agent:  4  - stage:  5\n",
      "Resetting Agent:  4\n",
      "Rank-0: policy_step=153744, average_reward_env_1=258.66124100422076\n",
      "Collide! - agent:  4  - stage:  5\n",
      "Resetting Agent:  4\n",
      "Collide! - agent:  2  - stage:  5\n",
      "Resetting Agent:  2\n",
      "Rank-0: policy_step=153768, average_reward_env_0=109.13285394157829\n",
      "Collide! - agent:  2  - stage:  5\n",
      "Resetting Agent:  2\n",
      "Arrive at the goal! - agent:  8  - stage:  5\n",
      "Resetting Agent:  8\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collide! - agent:  3  - stage:  5\n",
      "Resetting Agent:  3\n",
      "Rank-0: policy_step=154032, average_reward_env_0=86.84614940426913\n",
      "Collide! - agent:  1  - stage:  5\n",
      "Resetting Agent:  1\n",
      "Rank-0: policy_step=154088, average_reward_env_0=36.68638513379719\n",
      "Collide! - agent:  1  - stage:  5\n",
      "Resetting Agent:  1\n",
      "Collide! - agent:  6  - stage:  5\n",
      "Resetting Agent:  6\n",
      "Rank-0: policy_step=154208, average_reward_env_0=134.27283593362566\n",
      "Collide! - agent:  6  - stage:  5\n",
      "Resetting Agent:  6\n",
      "Collide! - agent:  1  - stage:  5\n",
      "Resetting Agent:  1\n",
      "Rank-0: policy_step=154256, average_reward_env_0=34.61806802675841\n",
      "Collide! - agent:  1  - stage:  5\n",
      "Resetting Agent:  1\n",
      "Collide! - agent:  1  - stage:  5\n",
      "Resetting Agent:  1\n",
      "Rank-0: policy_step=154304, average_reward_env_0=31.11046207649159\n",
      "Arrive at the goal! - agent:  7  - stage:  5\n",
      "Resetting Agent:  7\n",
      "Collide! - agent:  3  - stage:  5\n",
      "Resetting Agent:  3\n",
      "Rank-0: policy_step=154408, average_reward_env_0=80.93686497205215\n",
      "Collide! - agent:  4  - stage:  5\n",
      "Resetting Agent:  4\n",
      "Rank-0: policy_step=154440, average_reward_env_0=248.69134055012415\n",
      "Collide! - agent:  1  - stage:  5\n",
      "Resetting Agent:  1\n",
      "Rank-0: policy_step=154528, average_reward_env_0=31.444881906937297\n",
      "Collide! - agent:  1  - stage:  5\n",
      "Resetting Agent:  1\n",
      "Arrive at the goal! - agent:  5  - stage:  5\n",
      "Resetting Agent:  5\n",
      "Collision detected between /World/Worker_3 and /World/Warehouse/Warehouse/GroundPlane/CollisionPlane\n",
      "Collision detected between /World/Worker_3 and /World/Warehouse/Warehouse/Box_19295/SM_CardBoxA_01/SM_CardBoxA_01\n",
      "Collide! - agent:  5  - stage:  5\n",
      "Resetting Agent:  5\n",
      "Rank-0: policy_step=154792, average_reward_env_0=97.32250439817173\n",
      "Max step number - agent:  8\n",
      "Resetting Agent:  8\n",
      "Rank-0: policy_step=154944, average_reward_env_0=198.94940134012052\n",
      "Arrive at the goal! - agent:  6  - stage:  5\n",
      "Resetting Agent:  6\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Judging Increasing\n",
      ">>>>>>>>>>>>>>>Increasing Difficulty>>>>>>>>>>>>>>>>>>>\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Rank-0: policy_step=155144, average_reward_env_0=11.137146094878894\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Rank-0: policy_step=155168, average_reward_env_0=-61.16755715708961\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Rank-0: policy_step=155272, average_reward_env_0=64.06268487781053\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Rank-0: policy_step=155288, average_reward_env_0=171.09446669169304\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Arrive at the goal! - agent:  4  - stage:  6\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Rank-0: policy_step=155568, average_reward_env_0=-11.567613397613076\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Rank-0: policy_step=155600, average_reward_env_0=67.83592720620194\n",
      "Rank-0: policy_step=155600, average_reward_env_1=-41.05119449038247\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Rank-0: policy_step=155696, average_reward_env_0=25.38631551856761\n",
      "Arrive at the goal! - agent:  5  - stage:  6\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Rank-0: policy_step=155864, average_reward_env_0=-31.97656914188271\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Arrive at the goal! - agent:  1  - stage:  6\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Rank-0: policy_step=156016, average_reward_env_0=-34.92953907576062\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Rank-0: policy_step=156072, average_reward_env_0=180.10792914771696\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Rank-0: policy_step=156080, average_reward_env_0=42.846056901984845\n",
      "Arrive at the goal! - agent:  7  - stage:  6\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Rank-0: policy_step=156328, average_reward_env_0=69.88879177011826\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Rank-0: policy_step=156352, average_reward_env_0=23.320085072260742\n",
      "Max step number - agent:  7\n",
      "Rank-0: policy_step=156464, average_reward_env_0=428.28851799773065\n",
      "Arrive at the goal! - agent:  1  - stage:  6\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Rank-0: policy_step=156664, average_reward_env_0=33.5628152383847\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Arrive at the goal! - agent:  5  - stage:  6\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Rank-0: policy_step=156864, average_reward_env_0=-6.534160796054218\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Collision detected between /World/Worker_3 and /World/Warehouse/Table_08\n",
      "Collision detected between /World/Worker_3 and /World/Warehouse/Warehouse/SM_Rackshield_60/SM_Rackshield_02\n",
      "Collision detected between /World/Worker_3 and /World/Warehouse/Warehouse/SM_RackFrame_217/SM_RackFrame_03/SM_RackFrame_03/Section0\n",
      "Arrive at the goal! - agent:  1  - stage:  6\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Arrive at the goal! - agent:  1  - stage:  6\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Arrive at the goal! - agent:  8  - stage:  6\n",
      "Collision detected between /World/Worker_2 and /World/Warehouse/Warehouse/GroundPlane/CollisionPlane\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Rank-0: policy_step=157240, average_reward_env_0=34.95203954477025\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Rank-0: policy_step=157264, average_reward_env_0=-14.099934527721524\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Rank-0: policy_step=157544, average_reward_env_0=-18.501926463355954\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Max step number - agent:  1\n",
      "Rank-0: policy_step=157736, average_reward_env_0=664.4237142327013\n",
      "Arrive at the goal! - agent:  4  - stage:  6\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Arrive at the goal! - agent:  8  - stage:  6\n",
      "Max step number - agent:  5\n",
      "Rank-0: policy_step=157992, average_reward_env_0=511.8559140011922\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Rank-0: policy_step=158024, average_reward_env_0=1.1283209637677512\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Rank-0: policy_step=158152, average_reward_env_0=-20.31962605493615\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Rank-0: policy_step=158192, average_reward_env_0=103.64214461731005\n",
      "Arrive at the goal! - agent:  8  - stage:  6\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Rank-0: policy_step=158280, average_reward_env_0=241.7240872887843\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Arrive at the goal! - agent:  7  - stage:  6\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Rank-0: policy_step=158456, average_reward_env_0=-1.3320393346982087\n",
      "Rank-0: policy_step=158456, average_reward_env_1=329.30837961447116\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Arrive at the goal! - agent:  2  - stage:  6\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Rank-0: policy_step=158488, average_reward_env_0=-7.367836499690485\n",
      "Collision detected between /World/Worker_4 and /World/Worker_2\n",
      "Collision detected between /World/Worker_4 and /World/Worker_2\n",
      "Collision detected between /World/Worker_4 and /World/Worker_2\n",
      "Collision detected between /World/Worker_4 and /World/Worker_2\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Rank-0: policy_step=158808, average_reward_env_0=-15.839416751004292\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Arrive at the goal! - agent:  1  - stage:  6\n",
      "Collision detected between /World/Worker_1 and /World/Warehouse/Warehouse/GroundPlane/CollisionPlane\n",
      "Arrive at the goal! - agent:  5  - stage:  6\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Rank-0: policy_step=159120, average_reward_env_0=204.2656069041394\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Rank-0: policy_step=159168, average_reward_env_0=214.07031371209575\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Rank-0: policy_step=159184, average_reward_env_0=-17.970938556832575\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Arrive at the goal! - agent:  1  - stage:  6\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Rank-0: policy_step=159520, average_reward_env_0=148.78860829213065\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Rank-0: policy_step=159536, average_reward_env_0=120.28953410714428\n",
      "Arrive at the goal! - agent:  1  - stage:  6\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Rank-0: policy_step=159704, average_reward_env_0=111.52156307658285\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Arrive at the goal! - agent:  4  - stage:  6\n",
      "Collision detected between /World/Worker_1 and /World/Warehouse/Forklift_B01_PR_V_NVD_05\n",
      "Arrive at the goal! - agent:  6  - stage:  6\n",
      "Max step number - agent:  2\n",
      "Rank-0: policy_step=159872, average_reward_env_0=129.15054194587609\n",
      "Collision detected between /World/Worker_1 and /World/Warehouse/Warehouse/SM_RackShelf_3822/SM_RackShelf_01/SM_RackShelf_01/Section0\n",
      "Collision detected between /World/Worker_1 and /World/Warehouse/Warehouse/Box_37270/SM_CardBoxC_01/SM_CardBoxC_01\n",
      "Arrive at the goal! - agent:  5  - stage:  6\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Rank-0: policy_step=159920, average_reward_env_0=-21.269027752575447\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Rank-0: policy_step=160224, average_reward_env_0=95.43190290742915\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Arrive at the goal! - agent:  6  - stage:  6\n",
      "Rank-0: policy_step=160600, average_reward_env_0=109.67902521619618\n",
      "Arrive at the goal! - agent:  7  - stage:  6\n",
      "Arrive at the goal! - agent:  7  - stage:  6\n",
      "Arrive at the goal! - agent:  8  - stage:  6\n",
      "Arrive at the goal! - agent:  4  - stage:  6\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Rank-0: policy_step=160736, average_reward_env_0=-18.515898530621058\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Arrive at the goal! - agent:  1  - stage:  6\n",
      "Arrive at the goal! - agent:  3  - stage:  6\n",
      "Collision detected between /World/Worker_2 and /World/Warehouse/Table_03\n",
      "Max step number - agent:  1\n",
      "Rank-0: policy_step=160936, average_reward_env_0=696.0553261625853\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Rank-0: policy_step=160984, average_reward_env_0=-11.815998610678099\n",
      "Rank-0: policy_step=160984, average_reward_env_1=27.436688628933194\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Rank-0: policy_step=161040, average_reward_env_0=444.19990134160435\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Rank-0: policy_step=161272, average_reward_env_0=93.49548111482976\n",
      "Arrive at the goal! - agent:  2  - stage:  6\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Rank-0: policy_step=161384, average_reward_env_0=129.24633788236866\n",
      "Max step number - agent:  4\n",
      "Rank-0: policy_step=161392, average_reward_env_0=208.69703985633794\n",
      "Arrive at the goal! - agent:  7  - stage:  6\n",
      "Arrive at the goal! - agent:  6  - stage:  6\n",
      "Collision detected between /World/Worker_2 and /World/Warehouse/Warehouse/SM_Rackshield_2984/SM_Rackshield_02\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Rank-0: policy_step=161632, average_reward_env_0=-11.073860346686859\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Rank-0: policy_step=161704, average_reward_env_0=101.67903857522727\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Rank-0: policy_step=161832, average_reward_env_0=287.4281926775075\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Rank-0: policy_step=161888, average_reward_env_0=162.40788383814584\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Arrive at the goal! - agent:  8  - stage:  6\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Rank-0: policy_step=161960, average_reward_env_0=39.612343240864405\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Rank-0: policy_step=162072, average_reward_env_0=129.23589903172453\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Arrive at the goal! - agent:  5  - stage:  6\n",
      "Arrive at the goal! - agent:  7  - stage:  6\n",
      "Arrive at the goal! - agent:  6  - stage:  6\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Rank-0: policy_step=162240, average_reward_env_0=-10.772431365314137\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Arrive at the goal! - agent:  1  - stage:  6\n",
      "Arrive at the goal! - agent:  2  - stage:  6\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Rank-0: policy_step=162472, average_reward_env_0=43.8469784192838\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Rank-0: policy_step=162544, average_reward_env_0=101.06847830110179\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Rank-0: policy_step=162552, average_reward_env_0=35.5395040463754\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Rank-0: policy_step=162576, average_reward_env_0=379.93235300134984\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Rank-0: policy_step=162664, average_reward_env_0=295.8854439407492\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Arrive at the goal! - agent:  7  - stage:  6\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Rank-0: policy_step=162760, average_reward_env_0=105.1120734855064\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Rank-0: policy_step=162840, average_reward_env_0=29.12376736874487\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Arrive at the goal! - agent:  7  - stage:  6\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Rank-0: policy_step=163152, average_reward_env_0=88.47196332828803\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Rank-0: policy_step=163232, average_reward_env_0=92.57400147278989\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Rank-0: policy_step=163264, average_reward_env_0=248.96553309358535\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Rank-0: policy_step=163344, average_reward_env_0=74.1172464966346\n",
      "Collision detected between /World/Worker_3 and /World/Warehouse/Table_08\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Rank-0: policy_step=163392, average_reward_env_0=151.53299753233378\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Rank-0: policy_step=163456, average_reward_env_0=21.16437032325779\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Arrive at the goal! - agent:  1  - stage:  6\n",
      "Arrive at the goal! - agent:  2  - stage:  6\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Rank-0: policy_step=163664, average_reward_env_0=-4.329493658326545\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Arrive at the goal! - agent:  8  - stage:  6\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Rank-0: policy_step=163760, average_reward_env_0=225.6529133000281\n",
      "Arrive at the goal! - agent:  7  - stage:  6\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Rank-0: policy_step=164072, average_reward_env_0=16.064625947661977\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Rank-0: policy_step=164216, average_reward_env_0=135.34202933230998\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Rank-0: policy_step=164256, average_reward_env_0=-6.024559960431763\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Arrive at the goal! - agent:  7  - stage:  6\n",
      "Arrive at the goal! - agent:  7  - stage:  6\n",
      "Arrive at the goal! - agent:  5  - stage:  6\n",
      "Arrive at the goal! - agent:  4  - stage:  6\n",
      "Arrive at the goal! - agent:  5  - stage:  6\n",
      "Arrive at the goal! - agent:  2  - stage:  6\n",
      "Arrive at the goal! - agent:  4  - stage:  6\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Rank-0: policy_step=164560, average_reward_env_0=370.86707769673785\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Rank-0: policy_step=164672, average_reward_env_0=113.01461502757033\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Rank-0: policy_step=164728, average_reward_env_0=14.862720958522894\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Collision detected between /World/Worker_4 and /World/Warehouse/Warehouse/GroundPlane/CollisionPlane\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Rank-0: policy_step=164832, average_reward_env_0=167.40133212960106\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Arrive at the goal! - agent:  1  - stage:  6\n",
      "Arrive at the goal! - agent:  4  - stage:  6\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Rank-0: policy_step=165528, average_reward_env_0=130.60447446693854\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Rank-0: policy_step=165752, average_reward_env_0=111.30548770242382\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Rank-0: policy_step=165776, average_reward_env_0=148.59852463858562\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Rank-0: policy_step=165792, average_reward_env_0=18.123472834533292\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Rank-0: policy_step=165808, average_reward_env_0=103.92716228230216\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Arrive at the goal! - agent:  7  - stage:  6\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Rank-0: policy_step=166048, average_reward_env_0=222.16230734889893\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Arrive at the goal! - agent:  7  - stage:  6\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Rank-0: policy_step=166200, average_reward_env_0=113.81796843548277\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Arrive at the goal! - agent:  6  - stage:  6\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Rank-0: policy_step=166256, average_reward_env_0=192.84344506357115\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Rank-0: policy_step=166352, average_reward_env_0=358.11318489112017\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Rank-0: policy_step=166400, average_reward_env_0=22.78705361423978\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Rank-0: policy_step=166408, average_reward_env_0=169.00690293227\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Arrive at the goal! - agent:  2  - stage:  6\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Rank-0: policy_step=166552, average_reward_env_0=149.32068187661625\n",
      "Arrive at the goal! - agent:  1  - stage:  6\n",
      "Arrive at the goal! - agent:  2  - stage:  6\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Rank-0: policy_step=166688, average_reward_env_0=101.29230003311656\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Rank-0: policy_step=166984, average_reward_env_0=101.80047504659649\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Arrive at the goal! - agent:  4  - stage:  6\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Rank-0: policy_step=167064, average_reward_env_0=159.61274356108564\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Arrive at the goal! - agent:  7  - stage:  6\n",
      "Collision detected between /World/Worker_3 and /World/Warehouse/Table_09\n",
      "Max step number - agent:  3\n",
      "Arrive at the goal! - agent:  6  - stage:  6\n",
      "Rank-0: policy_step=167464, average_reward_env_0=7.665534727979603\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Rank-0: policy_step=167512, average_reward_env_0=119.82290624953612\n",
      "Collision detected between /World/Worker_1 and /World/Warehouse/Warehouse/GroundPlane/CollisionPlane\n",
      "Arrive at the goal! - agent:  8  - stage:  6\n",
      "Arrive at the goal! - agent:  1  - stage:  6\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Rank-0: policy_step=167816, average_reward_env_0=108.34801107105937\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Arrive at the goal! - agent:  7  - stage:  6\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Rank-0: policy_step=168064, average_reward_env_1=105.32935501441166\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Collision detected between /World/Worker_4 and /World/Worker_3\n",
      "Collision detected between /World/Worker_4 and /World/Worker_3\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Rank-0: policy_step=168096, average_reward_env_0=348.5400162930118\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Arrive at the goal! - agent:  2  - stage:  6\n",
      "Arrive at the goal! - agent:  3  - stage:  6\n",
      "Arrive at the goal! - agent:  1  - stage:  6\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Rank-0: policy_step=168248, average_reward_env_0=148.81928455786124\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Collision detected between /World/Worker_2 and /World/Worker_1\n",
      "Collision detected between /World/Worker_2 and /World/Worker_1\n",
      "Collision detected between /World/Worker_2 and /World/Worker_1\n",
      "Collision detected between /World/Worker_2 and /World/Worker_1\n",
      "Arrive at the goal! - agent:  5  - stage:  6\n",
      "Collision detected between /World/Worker_1 and /World/Warehouse/Forklift_B01_PR_V_NVD_01\n",
      "Arrive at the goal! - agent:  1  - stage:  6\n",
      "Collision detected between /World/Worker_1 and /World/Warehouse/Forklift_B01_PR_V_NVD_01\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Rank-0: policy_step=168440, average_reward_env_0=91.63329893752001\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Rank-0: policy_step=168560, average_reward_env_0=132.9564219551735\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Arrive at the goal! - agent:  5  - stage:  6\n",
      "Rank-0: policy_step=168568, average_reward_env_0=16.52517478339105\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Rank-0: policy_step=168600, average_reward_env_0=116.83186116772873\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Rank-0: policy_step=168648, average_reward_env_0=301.60743936576915\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Rank-0: policy_step=168752, average_reward_env_0=116.90814438801058\n",
      "Arrive at the goal! - agent:  4  - stage:  6\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Rank-0: policy_step=168848, average_reward_env_0=97.74732281007552\n",
      "Arrive at the goal! - agent:  5  - stage:  6\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Rank-0: policy_step=169008, average_reward_env_0=90.334265343791\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Arrive at the goal! - agent:  5  - stage:  6\n",
      "Arrive at the goal! - agent:  2  - stage:  6\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Rank-0: policy_step=169200, average_reward_env_0=121.26530304026107\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Rank-0: policy_step=169296, average_reward_env_0=16.546839279146162\n",
      "Arrive at the goal! - agent:  1  - stage:  6\n",
      "Arrive at the goal! - agent:  3  - stage:  6\n",
      "Max step number - agent:  6\n",
      "Rank-0: policy_step=169608, average_reward_env_0=39.39527925871841\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Rank-0: policy_step=169696, average_reward_env_0=91.44438968266795\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Arrive at the goal! - agent:  1  - stage:  6\n",
      "Max step number - agent:  1\n",
      "Rank-0: policy_step=169752, average_reward_env_0=208.02202728083603\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Rank-0: policy_step=169832, average_reward_env_0=20.04393931242285\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Arrive at the goal! - agent:  7  - stage:  6\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Rank-0: policy_step=170248, average_reward_env_0=181.0475240172083\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Rank-0: policy_step=170336, average_reward_env_0=19.30624719051391\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Rank-0: policy_step=170384, average_reward_env_0=119.035959779058\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Rank-0: policy_step=170440, average_reward_env_0=87.11384115869087\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Rank-0: policy_step=170448, average_reward_env_0=89.4383642771296\n",
      "Arrive at the goal! - agent:  2  - stage:  6\n",
      "Arrive at the goal! - agent:  6  - stage:  6\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Rank-0: policy_step=170672, average_reward_env_0=81.45519364650427\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Rank-0: policy_step=170880, average_reward_env_0=292.6851910080816\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collision detected between /World/Worker_2 and /World/Warehouse/Table_08\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Rank-0: policy_step=171056, average_reward_env_0=113.24675586024992\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Rank-0: policy_step=171184, average_reward_env_0=260.61120597377055\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Rank-0: policy_step=171312, average_reward_env_0=138.6699329767583\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Arrive at the goal! - agent:  1  - stage:  6\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Rank-0: policy_step=171400, average_reward_env_0=103.32380430698947\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Rank-0: policy_step=171632, average_reward_env_0=234.70085853605062\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Rank-0: policy_step=171744, average_reward_env_0=126.35667803288734\n",
      "Collision detected between /World/Worker_4 and /World/Worker_2\n",
      "Collision detected between /World/Worker_4 and /World/Worker_2\n",
      "Collision detected between /World/Worker_4 and /World/Worker_2\n",
      "Collision detected between /World/Worker_4 and /World/Worker_2\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Rank-0: policy_step=171848, average_reward_env_0=77.72069894039242\n",
      "Collision detected between /World/Worker_2 and /World/Warehouse/Forklift_B01_PR_V_NVD_01\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Rank-0: policy_step=171960, average_reward_env_0=70.16843785032923\n",
      "Collision detected between /World/Worker_2 and /World/Warehouse/Warehouse/SM_FireExtinguisher_1929/SM_FireExtinguisher_02\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Arrive at the goal! - agent:  7  - stage:  6\n",
      "Arrive at the goal! - agent:  6  - stage:  6\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Rank-0: policy_step=172288, average_reward_env_0=56.867480222694155\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Rank-0: policy_step=172352, average_reward_env_0=115.63804271251493\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Arrive at the goal! - agent:  2  - stage:  6\n",
      "Arrive at the goal! - agent:  5  - stage:  6\n",
      "Arrive at the goal! - agent:  3  - stage:  6\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Rank-0: policy_step=173104, average_reward_env_0=70.73128674779032\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Rank-0: policy_step=173112, average_reward_env_0=231.08983654840995\n",
      "Arrive at the goal! - agent:  6  - stage:  6\n",
      "Arrive at the goal! - agent:  1  - stage:  6\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Rank-0: policy_step=173328, average_reward_env_0=31.46212352680343\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Rank-0: policy_step=173352, average_reward_env_0=85.91873432725778\n",
      "Max step number - agent:  1\n",
      "Rank-0: policy_step=173456, average_reward_env_0=206.85766579866961\n",
      "Arrive at the goal! - agent:  3  - stage:  6\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Rank-0: policy_step=173784, average_reward_env_0=33.7508189472593\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Rank-0: policy_step=173928, average_reward_env_0=79.6995059876163\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Rank-0: policy_step=173992, average_reward_env_0=68.45239077347722\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Rank-0: policy_step=174000, average_reward_env_0=60.889792171955236\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Rank-0: policy_step=174184, average_reward_env_0=118.6066181730644\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Arrive at the goal! - agent:  1  - stage:  6\n",
      "Arrive at the goal! - agent:  7  - stage:  6\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Rank-0: policy_step=174496, average_reward_env_0=109.17630219140831\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Max step number - agent:  5\n",
      "Rank-0: policy_step=174608, average_reward_env_0=121.3221501947575\n",
      "Arrive at the goal! - agent:  2  - stage:  6\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Rank-0: policy_step=174800, average_reward_env_0=34.623898628671405\n",
      "Arrive at the goal! - agent:  4  - stage:  6\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Arrive at the goal! - agent:  6  - stage:  6\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Arrive at the goal! - agent:  6  - stage:  6\n",
      "Arrive at the goal! - agent:  4  - stage:  6\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Arrive at the goal! - agent:  8  - stage:  6\n",
      "Rank-0: policy_step=175296, average_reward_env_0=232.13630336829073\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Arrive at the goal! - agent:  4  - stage:  6\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Rank-0: policy_step=175680, average_reward_env_0=84.35070945013838\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Rank-0: policy_step=175776, average_reward_env_0=70.83193354437671\n",
      "Collision detected between /World/Worker_4 and /World/Warehouse/Table_09\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Rank-0: policy_step=175864, average_reward_env_0=33.40077355668488\n",
      "Arrive at the goal! - agent:  8  - stage:  6\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Arrive at the goal! - agent:  2  - stage:  6\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Rank-0: policy_step=176296, average_reward_env_0=217.22607023553996\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Arrive at the goal! - agent:  1  - stage:  6\n",
      "Arrive at the goal! - agent:  8  - stage:  6\n",
      "Arrive at the goal! - agent:  4  - stage:  6\n",
      "Max step number - agent:  1\n",
      "Rank-0: policy_step=176656, average_reward_env_0=225.92842973548716\n",
      "Arrive at the goal! - agent:  5  - stage:  6\n",
      "Arrive at the goal! - agent:  6  - stage:  6\n",
      "Arrive at the goal! - agent:  5  - stage:  6\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Arrive at the goal! - agent:  2  - stage:  6\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Max step number - agent:  8\n",
      "Rank-0: policy_step=177136, average_reward_env_0=107.12892228185774\n",
      "Collision detected between /World/Worker_4 and /World/Warehouse/Rack_02\n",
      "Arrive at the goal! - agent:  7  - stage:  6\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Rank-0: policy_step=177656, average_reward_env_0=91.65393211995276\n",
      "Max step number - agent:  2\n",
      "Rank-0: policy_step=177704, average_reward_env_0=135.2735718892865\n",
      "Arrive at the goal! - agent:  3  - stage:  6\n",
      "Arrive at the goal! - agent:  5  - stage:  6\n",
      "Max step number - agent:  5\n",
      "Rank-0: policy_step=177808, average_reward_env_0=146.96022479804586\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Rank-0: policy_step=178024, average_reward_env_0=216.20946399223783\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Arrive at the goal! - agent:  6  - stage:  6\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Rank-0: policy_step=178168, average_reward_env_0=104.6827212293915\n",
      "Arrive at the goal! - agent:  5  - stage:  6\n",
      "Collision detected between /World/Worker_4 and /World/Worker_2\n",
      "Collision detected between /World/Worker_4 and /World/Worker_2\n",
      "Collision detected between /World/Worker_4 and /World/Worker_2\n",
      "Collision detected between /World/Worker_4 and /World/Worker_2\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Rank-0: policy_step=178560, average_reward_env_0=129.54975372824939\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Rank-0: policy_step=178568, average_reward_env_0=219.54191057112135\n",
      "Arrive at the goal! - agent:  5  - stage:  6\n",
      "Arrive at the goal! - agent:  2  - stage:  6\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Max step number - agent:  6\n",
      "Rank-0: policy_step=178984, average_reward_env_0=86.64329766390901\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Rank-0: policy_step=179008, average_reward_env_0=204.02487256409952\n",
      "Max step number - agent:  3\n",
      "Rank-0: policy_step=179064, average_reward_env_0=48.89497737169482\n",
      "Collision detected between /World/Worker_3 and /World/Worker_2\n",
      "Collision detected between /World/Worker_3 and /World/Worker_2\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Rank-0: policy_step=179096, average_reward_env_0=81.06345998885693\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Rank-0: policy_step=179136, average_reward_env_0=127.05711603703415\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Rank-0: policy_step=179176, average_reward_env_0=205.2011312268341\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Rank-0: policy_step=179192, average_reward_env_0=101.20023926136739\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Rank-0: policy_step=179240, average_reward_env_0=153.97514372676696\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Rank-0: policy_step=179248, average_reward_env_0=91.04021211658916\n",
      "Arrive at the goal! - agent:  5  - stage:  6\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Rank-0: policy_step=179448, average_reward_env_0=191.45080978793732\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Rank-0: policy_step=179520, average_reward_env_0=93.63696923385454\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Rank-0: policy_step=179552, average_reward_env_0=120.36057437996864\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Collision detected between /World/Worker_1 and /World/Warehouse/Warehouse/SM_Rackshield_2984/SM_Rackshield_02\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Rank-0: policy_step=179576, average_reward_env_0=49.72836022334808\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Rank-0: policy_step=179648, average_reward_env_0=178.75491049905767\n",
      "Arrive at the goal! - agent:  8  - stage:  6\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Arrive at the goal! - agent:  8  - stage:  6\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Rank-0: policy_step=180096, average_reward_env_0=195.74221228714498\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Rank-0: policy_step=180160, average_reward_env_0=98.25807790080663\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Collision detected between /World/Worker_2 and /World/Worker_1\n",
      "Collision detected between /World/Worker_2 and /World/Worker_1\n",
      "Collision detected between /World/Worker_2 and /World/Worker_1\n",
      "Collision detected between /World/Worker_2 and /World/Worker_1\n",
      "Arrive at the goal! - agent:  3  - stage:  6\n",
      "Arrive at the goal! - agent:  2  - stage:  6\n",
      "Arrive at the goal! - agent:  5  - stage:  6\n",
      "Arrive at the goal! - agent:  3  - stage:  6\n",
      "Arrive at the goal! - agent:  4  - stage:  6\n",
      "Arrive at the goal! - agent:  7  - stage:  6\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Rank-0: policy_step=180768, average_reward_env_0=155.6631920954146\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Rank-0: policy_step=180776, average_reward_env_0=120.3042829847359\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Arrive at the goal! - agent:  6  - stage:  6\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Rank-0: policy_step=180880, average_reward_env_0=185.40729119030348\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Arrive at the goal! - agent:  5  - stage:  6\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Rank-0: policy_step=181048, average_reward_env_0=96.35367058448763\n",
      "Arrive at the goal! - agent:  3  - stage:  6\n",
      "Arrive at the goal! - agent:  8  - stage:  6\n",
      "Arrive at the goal! - agent:  6  - stage:  6\n",
      "Arrive at the goal! - agent:  4  - stage:  6\n",
      "Arrive at the goal! - agent:  2  - stage:  6\n",
      "Arrive at the goal! - agent:  3  - stage:  6\n",
      "Arrive at the goal! - agent:  1  - stage:  6\n",
      "Collision detected between /World/Worker_2 and /World/Warehouse/Table_03\n",
      "Arrive at the goal! - agent:  2  - stage:  6\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Rank-0: policy_step=181784, average_reward_env_0=104.18364009833472\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Rank-0: policy_step=182056, average_reward_env_0=181.6214920733735\n",
      "Arrive at the goal! - agent:  5  - stage:  6\n",
      "Max step number - agent:  6\n",
      "Rank-0: policy_step=182304, average_reward_env_0=98.94579127923448\n",
      "Arrive at the goal! - agent:  8  - stage:  6\n",
      "Arrive at the goal! - agent:  4  - stage:  6\n",
      "Arrive at the goal! - agent:  7  - stage:  6\n",
      "Max step number - agent:  3\n",
      "Rank-0: policy_step=182776, average_reward_env_0=79.63790290016532\n",
      "Collision detected between /World/Worker_4 and /World/Worker_3\n",
      "Collision detected between /World/Worker_4 and /World/Worker_3\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Rank-0: policy_step=182888, average_reward_env_0=187.65893763568727\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Rank-0: policy_step=182920, average_reward_env_0=178.3667324531545\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Rank-0: policy_step=183032, average_reward_env_0=176.7059594651754\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Rank-0: policy_step=183048, average_reward_env_0=104.64408294024943\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Arrive at the goal! - agent:  2  - stage:  6\n",
      "Arrive at the goal! - agent:  8  - stage:  6\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Rank-0: policy_step=183496, average_reward_env_0=112.86858338467711\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Rank-0: policy_step=183560, average_reward_env_0=91.66593187348305\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Rank-0: policy_step=183576, average_reward_env_0=139.26306847116066\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Rank-0: policy_step=183648, average_reward_env_0=171.31262567188537\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Rank-0: policy_step=183760, average_reward_env_0=169.17910898239532\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Rank-0: policy_step=183912, average_reward_env_0=83.292267489004\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Rank-0: policy_step=183920, average_reward_env_0=97.15107916632186\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Max step number - agent:  5\n",
      "Rank-0: policy_step=183976, average_reward_env_0=172.68530547587764\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Rank-0: policy_step=184096, average_reward_env_0=107.03847130686567\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Rank-0: policy_step=184208, average_reward_env_0=133.8622328350265\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Rank-0: policy_step=184224, average_reward_env_0=79.8516254001446\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Arrive at the goal! - agent:  7  - stage:  6\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Rank-0: policy_step=184448, average_reward_env_0=164.23780142427086\n",
      "Arrive at the goal! - agent:  5  - stage:  6\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Rank-0: policy_step=184496, average_reward_env_0=162.85110827462222\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Arrive at the goal! - agent:  2  - stage:  6\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Rank-0: policy_step=184848, average_reward_env_0=80.57101280297097\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Rank-0: policy_step=184976, average_reward_env_0=132.0340240586603\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Arrive at the goal! - agent:  7  - stage:  6\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Rank-0: policy_step=185040, average_reward_env_0=103.94296236989092\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Rank-0: policy_step=185064, average_reward_env_0=159.78774374876835\n",
      "Arrive at the goal! - agent:  4  - stage:  6\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Rank-0: policy_step=185400, average_reward_env_0=91.77978660298362\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Arrive at the goal! - agent:  7  - stage:  6\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Rank-0: policy_step=186056, average_reward_env_0=92.23850697168413\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Rank-0: policy_step=186104, average_reward_env_0=185.07989047354783\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Rank-0: policy_step=186112, average_reward_env_0=108.38550025894939\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Arrive at the goal! - agent:  3  - stage:  6\n",
      "Collision detected between /World/Worker_2 and /World/Warehouse/Warehouse/SM_Rackshield_2984/SM_Rackshield_02\n",
      "Collision detected between /World/Worker_2 and /World/Warehouse/Forklift_B01_PR_V_NVD_01\n",
      "Collision detected between /World/Worker_2 and /World/Warehouse/Forklift_B01_PR_V_NVD_01\n",
      "Collision detected between /World/Worker_3 and /World/Worker_2\n",
      "Collision detected between /World/Worker_3 and /World/Worker_2\n",
      "Collision detected between /World/Worker_3 and /World/Worker_2\n",
      "Collision detected between /World/Worker_3 and /World/Worker_2\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Rank-0: policy_step=186608, average_reward_env_0=134.31796813015615\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Rank-0: policy_step=186648, average_reward_env_0=155.94702606232073\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Collision detected between /World/Worker_1 and /World/Warehouse/Table_03\n",
      "Collision detected between /World/Worker_1 and /World/Warehouse/Warehouse/Box_43762/SM_CardBoxC_01/SM_CardBoxC_01\n",
      "Collision detected between /World/Worker_1 and /World/Warehouse/Warehouse/SM_RackFrame_217/SM_RackFrame_03/SM_RackFrame_03/Section0\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Rank-0: policy_step=186728, average_reward_env_0=88.40884469194718\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Rank-0: policy_step=186760, average_reward_env_0=93.80546446626076\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Rank-0: policy_step=186840, average_reward_env_0=177.9844281144614\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Rank-0: policy_step=187112, average_reward_env_0=97.77095290452515\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Max step number - agent:  4\n",
      "Rank-0: policy_step=187128, average_reward_env_0=115.19551849371437\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Rank-0: policy_step=187136, average_reward_env_0=134.66568048143895\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Rank-0: policy_step=187216, average_reward_env_0=110.71746532518746\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Rank-0: policy_step=187264, average_reward_env_0=146.29787571690568\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Rank-0: policy_step=187368, average_reward_env_0=164.68827631934315\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Rank-0: policy_step=187376, average_reward_env_0=106.50058093047933\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Rank-0: policy_step=187432, average_reward_env_0=131.02122864607756\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Collision detected between /World/Worker_4 and /World/Warehouse/Warehouse/GroundPlane/CollisionPlane\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Rank-0: policy_step=187472, average_reward_env_0=118.11111266823873\n",
      "Collision detected between /World/Worker_2 and /World/Warehouse/Warehouse/SM_Rackshield_2984/SM_Rackshield_02\n",
      "Arrive at the goal! - agent:  6  - stage:  6\n",
      "Arrive at the goal! - agent:  5  - stage:  6\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Rank-0: policy_step=187680, average_reward_env_0=84.27869998359637\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Rank-0: policy_step=187728, average_reward_env_0=87.55713232876037\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Arrive at the goal! - agent:  3  - stage:  6\n",
      "Arrive at the goal! - agent:  8  - stage:  6\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Rank-0: policy_step=187856, average_reward_env_0=100.54975286273964\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Rank-0: policy_step=187904, average_reward_env_0=163.1805181032143\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collision detected between /World/Worker_2 and /World/Worker_1\n",
      "Collision detected between /World/Worker_2 and /World/Worker_1\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Rank-0: policy_step=188192, average_reward_env_0=83.99939627977126\n",
      "Arrive at the goal! - agent:  2  - stage:  6\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Rank-0: policy_step=188352, average_reward_env_0=108.27571015671042\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Rank-0: policy_step=188368, average_reward_env_0=140.4104607850203\n",
      "Arrive at the goal! - agent:  3  - stage:  6\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Rank-0: policy_step=188504, average_reward_env_0=139.6217558821774\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Rank-0: policy_step=188560, average_reward_env_0=91.07358681754756\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Rank-0: policy_step=188720, average_reward_env_0=132.8682069036165\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Arrive at the goal! - agent:  4  - stage:  6\n",
      "Arrive at the goal! - agent:  2  - stage:  6\n",
      "Arrive at the goal! - agent:  1  - stage:  6\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Rank-0: policy_step=188904, average_reward_env_0=102.40842523404754\n",
      "Collision detected between /World/Worker_2 and /World/Warehouse/Warehouse/SM_Rackshield_2984/SM_Rackshield_02\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Rank-0: policy_step=188960, average_reward_env_0=83.4191649919489\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Rank-0: policy_step=189080, average_reward_env_0=89.64427989944873\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Rank-0: policy_step=189120, average_reward_env_0=132.82424650956594\n",
      "Arrive at the goal! - agent:  7  - stage:  6\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Rank-0: policy_step=189240, average_reward_env_0=81.35786416394589\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Rank-0: policy_step=189496, average_reward_env_0=165.9636573815095\n",
      "Collision detected between /World/Worker_2 and /World/Warehouse/Warehouse/SM_Rackshield_2984/SM_Rackshield_02\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Rank-0: policy_step=189920, average_reward_env_0=105.43657322078805\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Collision detected between /World/Worker_4 and /World/Worker_3\n",
      "Collision detected between /World/Worker_4 and /World/Worker_3\n",
      "Max step number - agent:  3\n",
      "Collision detected between /World/Worker_4 and /World/Worker_3\n",
      "Collision detected between /World/Worker_4 and /World/Worker_3\n",
      "Rank-0: policy_step=189936, average_reward_env_0=107.29393237759932\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Arrive at the goal! - agent:  1  - stage:  6\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Rank-0: policy_step=190216, average_reward_env_0=132.24695512967864\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Rank-0: policy_step=190256, average_reward_env_0=104.3303101110639\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Arrive at the goal! - agent:  5  - stage:  6\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Rank-0: policy_step=190416, average_reward_env_0=147.3838834276676\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Rank-0: policy_step=190440, average_reward_env_0=108.74956716363222\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Arrive at the goal! - agent:  4  - stage:  6\n",
      "Collision detected between /World/Worker_4 and /World/Worker_1\n",
      "Collision detected between /World/Worker_4 and /World/Worker_1\n",
      "Collision detected between /World/Worker_4 and /World/Worker_1\n",
      "Collision detected between /World/Worker_4 and /World/Worker_1\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Rank-0: policy_step=190792, average_reward_env_0=109.92906877195428\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Rank-0: policy_step=190888, average_reward_env_0=107.73271539443192\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Arrive at the goal! - agent:  3  - stage:  6\n",
      "Arrive at the goal! - agent:  8  - stage:  6\n",
      "Arrive at the goal! - agent:  6  - stage:  6\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Rank-0: policy_step=191224, average_reward_env_0=143.92650361618934\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Rank-0: policy_step=191296, average_reward_env_0=139.60970211960816\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Rank-0: policy_step=191376, average_reward_env_0=138.5054743644644\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Rank-0: policy_step=191560, average_reward_env_0=131.7876156857976\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Rank-0: policy_step=191712, average_reward_env_0=139.6700592834695\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Rank-0: policy_step=191856, average_reward_env_0=99.38551579820567\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Arrive at the goal! - agent:  2  - stage:  6\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Rank-0: policy_step=192104, average_reward_env_0=104.40404170645942\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Arrive at the goal! - agent:  2  - stage:  6\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Rank-0: policy_step=192328, average_reward_env_0=119.10576326922434\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Rank-0: policy_step=192424, average_reward_env_0=102.62198476429673\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Max step number - agent:  8\n",
      "Rank-0: policy_step=192440, average_reward_env_0=81.13573346762001\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Collision detected between /World/Worker_3 and /World/Warehouse/Table_12\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Rank-0: policy_step=192848, average_reward_env_0=121.73066838221389\n",
      "Arrive at the goal! - agent:  5  - stage:  6\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Rank-0: policy_step=193064, average_reward_env_0=108.99643006395607\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Arrive at the goal! - agent:  7  - stage:  6\n",
      "Arrive at the goal! - agent:  6  - stage:  6\n",
      "Arrive at the goal! - agent:  3  - stage:  6\n",
      "Arrive at the goal! - agent:  5  - stage:  6\n",
      "Collision detected between /World/Worker_4 and /World/Worker_2\n",
      "Collision detected between /World/Worker_4 and /World/Worker_2\n",
      "Collision detected between /World/Worker_4 and /World/Worker_2\n",
      "Collision detected between /World/Worker_4 and /World/Worker_2\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Rank-0: policy_step=193696, average_reward_env_0=103.42864195029857\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Rank-0: policy_step=193776, average_reward_env_0=108.5705680057877\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Arrive at the goal! - agent:  7  - stage:  6\n",
      "Arrive at the goal! - agent:  1  - stage:  6\n",
      "Arrive at the goal! - agent:  6  - stage:  6\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collision detected between /World/Worker_2 and /World/Worker_1\n",
      "Collision detected between /World/Worker_2 and /World/Worker_1\n",
      "Collision detected between /World/Worker_2 and /World/Worker_1\n",
      "Collision detected between /World/Worker_2 and /World/Worker_1\n",
      "Max step number - agent:  3\n",
      "Rank-0: policy_step=194088, average_reward_env_0=124.18886833485132\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Rank-0: policy_step=194120, average_reward_env_0=80.70616914375628\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Rank-0: policy_step=194216, average_reward_env_0=102.80702919322397\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Rank-0: policy_step=194232, average_reward_env_0=102.69664192163243\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Rank-0: policy_step=194264, average_reward_env_0=108.83680020901109\n",
      "Collision detected between /World/Worker_4 and /World/Worker_1\n",
      "Collision detected between /World/Worker_4 and /World/Worker_1\n",
      "Collision detected between /World/Worker_4 and /World/Worker_1\n",
      "Collision detected between /World/Worker_4 and /World/Worker_1\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Rank-0: policy_step=194480, average_reward_env_0=122.55468457461447\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Rank-0: policy_step=194600, average_reward_env_0=84.17105977201055\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Rank-0: policy_step=194632, average_reward_env_0=107.19378094224257\n",
      "Arrive at the goal! - agent:  5  - stage:  6\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Rank-0: policy_step=194824, average_reward_env_0=71.21493327666586\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Arrive at the goal! - agent:  7  - stage:  6\n",
      "Max step number - agent:  5\n",
      "Rank-0: policy_step=194920, average_reward_env_0=163.52478366112152\n",
      "Collision detected between /World/Worker_4 and /World/Warehouse/Rack_02\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Rank-0: policy_step=194976, average_reward_env_0=122.61364063844536\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Rank-0: policy_step=195000, average_reward_env_0=83.17003203572591\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Arrive at the goal! - agent:  6  - stage:  6\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Rank-0: policy_step=195240, average_reward_env_0=156.04819997377245\n",
      "Arrive at the goal! - agent:  2  - stage:  6\n",
      "Arrive at the goal! - agent:  8  - stage:  6\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Rank-0: policy_step=195416, average_reward_env_0=76.704374180833\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Rank-0: policy_step=195424, average_reward_env_0=106.32484508505135\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Rank-0: policy_step=195488, average_reward_env_0=116.25611862503014\n",
      "Rank-0: policy_step=195488, average_reward_env_1=89.82421921950333\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Rank-0: policy_step=195528, average_reward_env_0=75.84352621112185\n",
      "Rank-0: policy_step=195528, average_reward_env_1=87.31513140036039\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Rank-0: policy_step=195600, average_reward_env_0=74.60244847767326\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Rank-0: policy_step=195664, average_reward_env_0=154.43437192337367\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Arrive at the goal! - agent:  1  - stage:  6\n",
      "Arrive at the goal! - agent:  7  - stage:  6\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Arrive at the goal! - agent:  3  - stage:  6\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Rank-0: policy_step=196080, average_reward_env_0=70.65581521144485\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Rank-0: policy_step=196136, average_reward_env_0=80.71369212337265\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Rank-0: policy_step=196288, average_reward_env_0=141.3315130042948\n",
      "Collision detected between /World/Worker_4 and /World/Warehouse/Rack_02\n",
      "Arrive at the goal! - agent:  2  - stage:  6\n",
      "Arrive at the goal! - agent:  8  - stage:  6\n",
      "Arrive at the goal! - agent:  4  - stage:  6\n",
      "Arrive at the goal! - agent:  6  - stage:  6\n",
      "Arrive at the goal! - agent:  3  - stage:  6\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Arrive at the goal! - agent:  4  - stage:  6\n",
      "Arrive at the goal! - agent:  2  - stage:  6\n",
      "Arrive at the goal! - agent:  1  - stage:  6\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Rank-0: policy_step=197352, average_reward_env_0=133.59977032095662\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Arrive at the goal! - agent:  7  - stage:  6\n",
      "Max step number - agent:  1\n",
      "Rank-0: policy_step=197832, average_reward_env_0=122.77898579747483\n",
      "Collision detected between /World/Worker_4 and /World/Warehouse/Warehouse/Box_3581/SM_CardBoxB_01/SM_CardBoxB_01\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Rank-0: policy_step=197976, average_reward_env_0=87.17765732314659\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Arrive at the goal! - agent:  8  - stage:  6\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Rank-0: policy_step=198024, average_reward_env_0=72.87324709779713\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Arrive at the goal! - agent:  6  - stage:  6\n",
      "Arrive at the goal! - agent:  3  - stage:  6\n",
      "Arrive at the goal! - agent:  4  - stage:  6\n",
      "Max step number - agent:  2\n",
      "Rank-0: policy_step=198624, average_reward_env_0=125.9589064669679\n",
      "Max step number - agent:  6\n",
      "Rank-0: policy_step=198688, average_reward_env_0=133.7147953120699\n",
      "Arrive at the goal! - agent:  5  - stage:  6\n",
      "Max step number - agent:  4\n",
      "Rank-0: policy_step=198800, average_reward_env_0=101.62730240985911\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Rank-0: policy_step=198968, average_reward_env_0=126.37585537000996\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Arrive at the goal! - agent:  8  - stage:  6\n",
      "Arrive at the goal! - agent:  7  - stage:  6\n",
      "Collision detected between /World/Worker_4 and /World/Warehouse/Warehouse/SM_Rackshield_3844/SM_Rackshield_02\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Rank-0: policy_step=199192, average_reward_env_0=117.70388404795864\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Rank-0: policy_step=199256, average_reward_env_0=117.45832551840572\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Rank-0: policy_step=199296, average_reward_env_0=153.73838746142133\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Rank-0: policy_step=199312, average_reward_env_0=127.77852882277492\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Rank-0: policy_step=199344, average_reward_env_0=96.98501418284552\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Arrive at the goal! - agent:  6  - stage:  6\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Rank-0: policy_step=199536, average_reward_env_0=98.39028928751492\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Rank-0: policy_step=199656, average_reward_env_0=139.76577863109222\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Rank-0: policy_step=199728, average_reward_env_0=92.70757723916026\n",
      "Arrive at the goal! - agent:  5  - stage:  6\n",
      "Arrive at the goal! - agent:  1  - stage:  6\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Arrive at the goal! - agent:  3  - stage:  6\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Rank-0: policy_step=200192, average_reward_env_0=81.58967878497532\n",
      "Arrive at the goal! - agent:  1  - stage:  6\n",
      "Arrive at the goal! - agent:  6  - stage:  6\n",
      "Max step number - agent:  3\n",
      "Rank-0: policy_step=200560, average_reward_env_0=151.44339693144903\n",
      "Arrive at the goal! - agent:  8  - stage:  6\n",
      "Arrive at the goal! - agent:  7  - stage:  6\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Arrive at the goal! - agent:  2  - stage:  6\n",
      "Arrive at the goal! - agent:  3  - stage:  6\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Rank-0: policy_step=201128, average_reward_env_0=97.29855366164408\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Arrive at the goal! - agent:  1  - stage:  6\n",
      "Arrive at the goal! - agent:  3  - stage:  6\n",
      "Arrive at the goal! - agent:  6  - stage:  6\n",
      "Arrive at the goal! - agent:  1  - stage:  6\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Rank-0: policy_step=201480, average_reward_env_0=151.31697443705724\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Collision detected between /World/Worker_2 and /World/Warehouse/Table_12\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Rank-0: policy_step=201608, average_reward_env_0=116.89072382432886\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Arrive at the goal! - agent:  5  - stage:  6\n",
      "Arrive at the goal! - agent:  7  - stage:  6\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Rank-0: policy_step=201864, average_reward_env_0=140.58622811009658\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Arrive at the goal! - agent:  4  - stage:  6\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Rank-0: policy_step=201952, average_reward_env_0=144.4852094182873\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Arrive at the goal! - agent:  8  - stage:  6\n",
      "Collision detected between /World/Worker_1 and /World/Warehouse/Warehouse/GroundPlane/CollisionPlane\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Rank-0: policy_step=202160, average_reward_env_0=113.263494188976\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Rank-0: policy_step=202344, average_reward_env_0=89.20948250267931\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Rank-0: policy_step=202408, average_reward_env_0=142.53921072751803\n",
      "Max step number - agent:  5\n",
      "Rank-0: policy_step=202504, average_reward_env_0=167.7837949115415\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Rank-0: policy_step=202584, average_reward_env_0=105.85269421049041\n",
      "Arrive at the goal! - agent:  5  - stage:  6\n",
      "Arrive at the goal! - agent:  6  - stage:  6\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Rank-0: policy_step=202728, average_reward_env_0=109.60431982553676\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Arrive at the goal! - agent:  2  - stage:  6\n",
      "Arrive at the goal! - agent:  5  - stage:  6\n",
      "Max step number - agent:  6\n",
      "Rank-0: policy_step=202856, average_reward_env_0=166.50016847805836\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Arrive at the goal! - agent:  4  - stage:  6\n",
      "Arrive at the goal! - agent:  3  - stage:  6\n",
      "Arrive at the goal! - agent:  3  - stage:  6\n",
      "Arrive at the goal! - agent:  8  - stage:  6\n",
      "Collision detected between /World/Worker_4 and /World/Warehouse/Warehouse/SM_Rackshield_1074/SM_Rackshield_02\n",
      "Arrive at the goal! - agent:  6  - stage:  6\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Rank-0: policy_step=203384, average_reward_env_0=153.07956947268147\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Arrive at the goal! - agent:  7  - stage:  6\n",
      "Collision detected between /World/Worker_2 and /World/Warehouse/Table_03\n",
      "Arrive at the goal! - agent:  5  - stage:  6\n",
      "Arrive at the goal! - agent:  1  - stage:  6\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Rank-0: policy_step=203928, average_reward_env_0=108.25599325685984\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collision detected between /World/Worker_2 and /World/Worker_1\n",
      "Collision detected between /World/Worker_2 and /World/Worker_1\n",
      "Collision detected between /World/Worker_2 and /World/Worker_1\n",
      "Collision detected between /World/Worker_2 and /World/Worker_1\n",
      "Arrive at the goal! - agent:  2  - stage:  6\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Rank-0: policy_step=204240, average_reward_env_1=150.04167443528502\n",
      "Arrive at the goal! - agent:  7  - stage:  6\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Rank-0: policy_step=204552, average_reward_env_0=110.46936636442507\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Arrive at the goal! - agent:  6  - stage:  6\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Rank-0: policy_step=204608, average_reward_env_0=181.66496202482685\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Arrive at the goal! - agent:  8  - stage:  6\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Rank-0: policy_step=204728, average_reward_env_0=93.51189485161005\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Rank-0: policy_step=204744, average_reward_env_0=124.46237206865162\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Max step number - agent:  2\n",
      "Rank-0: policy_step=204816, average_reward_env_0=137.73723021450294\n",
      "Arrive at the goal! - agent:  1  - stage:  6\n",
      "Arrive at the goal! - agent:  5  - stage:  6\n",
      "Arrive at the goal! - agent:  8  - stage:  6\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Arrive at the goal! - agent:  8  - stage:  6\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Rank-0: policy_step=205176, average_reward_env_0=183.07515220605265\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Rank-0: policy_step=205216, average_reward_env_0=135.9884215786446\n",
      "Arrive at the goal! - agent:  8  - stage:  6\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Rank-0: policy_step=205288, average_reward_env_0=92.48390101731542\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Arrive at the goal! - agent:  7  - stage:  6\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Rank-0: policy_step=205296, average_reward_env_0=180.64127562398286\n",
      "Rank-0: policy_step=205296, average_reward_env_2=135.44070868137194\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Arrive at the goal! - agent:  3  - stage:  6\n",
      "Arrive at the goal! - agent:  1  - stage:  6\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Rank-0: policy_step=205552, average_reward_env_0=111.906782027568\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Rank-0: policy_step=205600, average_reward_env_0=176.6874577374999\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Rank-0: policy_step=205624, average_reward_env_0=128.13564411490458\n",
      "Arrive at the goal! - agent:  5  - stage:  6\n",
      "Max step number - agent:  5\n",
      "Rank-0: policy_step=205704, average_reward_env_0=188.00090151286275\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Rank-0: policy_step=205856, average_reward_env_0=106.77930887446904\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Rank-0: policy_step=205872, average_reward_env_0=109.27724638591494\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Rank-0: policy_step=205928, average_reward_env_0=106.17005669887394\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Rank-0: policy_step=205936, average_reward_env_0=85.51038108051169\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Arrive at the goal! - agent:  7  - stage:  6\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Rank-0: policy_step=206088, average_reward_env_0=90.0226653548765\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Rank-0: policy_step=206112, average_reward_env_0=81.44816197583836\n",
      "Rank-0: policy_step=206112, average_reward_env_1=109.0798817625354\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Rank-0: policy_step=206232, average_reward_env_0=180.23755172559996\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Rank-0: policy_step=206416, average_reward_env_0=89.05616116141299\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Arrive at the goal! - agent:  7  - stage:  6\n",
      "Arrive at the goal! - agent:  3  - stage:  6\n",
      "Arrive at the goal! - agent:  6  - stage:  6\n",
      "Arrive at the goal! - agent:  8  - stage:  6\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Rank-0: policy_step=206808, average_reward_env_0=112.22577479015726\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Rank-0: policy_step=207008, average_reward_env_0=78.21433334998946\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Rank-0: policy_step=207096, average_reward_env_0=180.08724937110026\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Rank-0: policy_step=207120, average_reward_env_0=173.60484409640267\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Arrive at the goal! - agent:  1  - stage:  6\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Rank-0: policy_step=207192, average_reward_env_1=134.34692606665337\n",
      "Arrive at the goal! - agent:  3  - stage:  6\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Rank-0: policy_step=207352, average_reward_env_0=88.08970289394274\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Arrive at the goal! - agent:  5  - stage:  6\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Rank-0: policy_step=207424, average_reward_env_0=114.66356218420442\n",
      "Max step number - agent:  3\n",
      "Arrive at the goal! - agent:  6  - stage:  6\n",
      "Rank-0: policy_step=207440, average_reward_env_0=174.0576388938584\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Rank-0: policy_step=207576, average_reward_env_0=75.27848602916156\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Rank-0: policy_step=207600, average_reward_env_0=110.5081846011176\n",
      "Arrive at the goal! - agent:  7  - stage:  6\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Rank-0: policy_step=207760, average_reward_env_0=107.27946353751078\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Rank-0: policy_step=207904, average_reward_env_0=106.48757424076472\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Arrive at the goal! - agent:  4  - stage:  6\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Arrive at the goal! - agent:  2  - stage:  6\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Rank-0: policy_step=208136, average_reward_env_0=85.79332700568341\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Rank-0: policy_step=208216, average_reward_env_0=70.68919787948555\n",
      "Arrive at the goal! - agent:  4  - stage:  6\n",
      "Arrive at the goal! - agent:  7  - stage:  6\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Rank-0: policy_step=208488, average_reward_env_0=179.66163308160722\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Rank-0: policy_step=208592, average_reward_env_0=109.5436925739262\n",
      "Arrive at the goal! - agent:  3  - stage:  6\n",
      "Arrive at the goal! - agent:  6  - stage:  6\n",
      "Arrive at the goal! - agent:  2  - stage:  6\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Arrive at the goal! - agent:  1  - stage:  6\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collision detected between /World/Worker_2 and /World/Warehouse/Warehouse/GroundPlane/CollisionPlane\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Rank-0: policy_step=209096, average_reward_env_0=112.22070434079178\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Rank-0: policy_step=209152, average_reward_env_0=177.64363767879698\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Rank-0: policy_step=209224, average_reward_env_0=105.5119170624128\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Rank-0: policy_step=209248, average_reward_env_0=112.74998788048296\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Arrive at the goal! - agent:  4  - stage:  6\n",
      "Arrive at the goal! - agent:  3  - stage:  6\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Rank-0: policy_step=209560, average_reward_env_0=81.2395243751402\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Arrive at the goal! - agent:  4  - stage:  6\n",
      "Arrive at the goal! - agent:  7  - stage:  6\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Rank-0: policy_step=209624, average_reward_env_0=100.04008526355634\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Collision detected between /World/Worker_4 and /World/Worker_2\n",
      "Collision detected between /World/Worker_4 and /World/Worker_2\n",
      "Collision detected between /World/Worker_4 and /World/Worker_2\n",
      "Collision detected between /World/Worker_4 and /World/Worker_2\n",
      "Arrive at the goal! - agent:  1  - stage:  6\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Rank-0: policy_step=209904, average_reward_env_0=116.73505629157307\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Rank-0: policy_step=209912, average_reward_env_0=77.75084910459584\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Rank-0: policy_step=209928, average_reward_env_0=96.27661016285161\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Rank-0: policy_step=209968, average_reward_env_0=160.9566634636925\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Arrive at the goal! - agent:  5  - stage:  6\n",
      "Arrive at the goal! - agent:  7  - stage:  6\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Rank-0: policy_step=210136, average_reward_env_0=179.6312854779332\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Rank-0: policy_step=210168, average_reward_env_0=86.95479410836306\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Rank-0: policy_step=210224, average_reward_env_0=85.73093292070908\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Rank-0: policy_step=210288, average_reward_env_0=74.9614710417203\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Rank-0: policy_step=210320, average_reward_env_0=174.3474291684478\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Collision detected between /World/Worker_1 and /World/Warehouse/Warehouse/SM_FireExtinguisher_1925/SM_FireExtinguisher_02\n",
      "Collision detected between /World/Worker_1 and /World/Warehouse/Warehouse/Box_3551/SM_CardBoxC_01/SM_CardBoxC_01\n",
      "Arrive at the goal! - agent:  5  - stage:  6\n",
      "Arrive at the goal! - agent:  2  - stage:  6\n",
      "Arrive at the goal! - agent:  6  - stage:  6\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Rank-0: policy_step=210560, average_reward_env_0=83.35341456994249\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Rank-0: policy_step=210632, average_reward_env_0=86.19440055314253\n",
      "Collision detected between /World/Worker_2 and /World/Warehouse/Warehouse/SM_Rackshield_2984/SM_Rackshield_02\n",
      "Collision detected between /World/Worker_2 and /World/Warehouse/Warehouse/Box_29066/SM_CardBoxD_04/SM_CardBoxD_04\n",
      "Collision detected between /World/Worker_2 and /World/Warehouse/Warehouse/Box_29122/SM_CardBoxD_04/SM_CardBoxD_04\n",
      "Arrive at the goal! - agent:  2  - stage:  6\n",
      "Arrive at the goal! - agent:  3  - stage:  6\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Arrive at the goal! - agent:  7  - stage:  6\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Rank-0: policy_step=211248, average_reward_env_0=177.29249535761772\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Arrive at the goal! - agent:  4  - stage:  6\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Rank-0: policy_step=211272, average_reward_env_0=162.0005253711633\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Rank-0: policy_step=211288, average_reward_env_0=174.72291507693433\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Rank-0: policy_step=211304, average_reward_env_0=81.87863234186091\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Rank-0: policy_step=211456, average_reward_env_0=157.61653822569505\n",
      "Collision detected between /World/Worker_1 and /World/Warehouse/Table_09\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Rank-0: policy_step=211544, average_reward_env_0=86.28642549209408\n",
      "Max step number - agent:  5\n",
      "Rank-0: policy_step=211696, average_reward_env_0=203.15023774251998\n",
      "Arrive at the goal! - agent:  7  - stage:  6\n",
      "Arrive at the goal! - agent:  1  - stage:  6\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Rank-0: policy_step=211952, average_reward_env_0=84.21942432869888\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Arrive at the goal! - agent:  3  - stage:  6\n",
      "Collision detected between /World/Worker_2 and /World/Warehouse/Warehouse/SM_Rackshield_2984/SM_Rackshield_02\n",
      "Arrive at the goal! - agent:  7  - stage:  6\n",
      "Arrive at the goal! - agent:  8  - stage:  6\n",
      "Arrive at the goal! - agent:  8  - stage:  6\n",
      "Arrive at the goal! - agent:  7  - stage:  6\n",
      "Arrive at the goal! - agent:  6  - stage:  6\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Rank-0: policy_step=212656, average_reward_env_0=89.88784460623272\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Rank-0: policy_step=212744, average_reward_env_0=82.2153097591527\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Rank-0: policy_step=212896, average_reward_env_0=165.51380509069813\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Rank-0: policy_step=212992, average_reward_env_0=98.05364786533052\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Arrive at the goal! - agent:  4  - stage:  6\n",
      "Max step number - agent:  7\n",
      "Rank-0: policy_step=213112, average_reward_env_0=148.12428445917988\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Rank-0: policy_step=213184, average_reward_env_0=189.08565558838342\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Arrive at the goal! - agent:  2  - stage:  6\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Rank-0: policy_step=213312, average_reward_env_0=136.97302906900183\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Rank-0: policy_step=213432, average_reward_env_0=83.20962831324427\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Rank-0: policy_step=213664, average_reward_env_0=76.31910628230294\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Rank-0: policy_step=213704, average_reward_env_0=138.88276182801687\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Rank-0: policy_step=213912, average_reward_env_0=161.97720122280327\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Rank-0: policy_step=213960, average_reward_env_0=92.24472376996383\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Rank-0: policy_step=214056, average_reward_env_0=76.63016119229455\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Rank-0: policy_step=214136, average_reward_env_0=99.45838439709647\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Arrive at the goal! - agent:  5  - stage:  6\n",
      "Arrive at the goal! - agent:  3  - stage:  6\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Rank-0: policy_step=214304, average_reward_env_0=143.12607490720896\n",
      "Arrive at the goal! - agent:  7  - stage:  6\n",
      "Arrive at the goal! - agent:  6  - stage:  6\n",
      "Arrive at the goal! - agent:  1  - stage:  6\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collision detected between /World/Worker_4 and /World/Worker_1\n",
      "Collision detected between /World/Worker_4 and /World/Worker_1\n",
      "Collision detected between /World/Worker_4 and /World/Worker_1\n",
      "Collision detected between /World/Worker_4 and /World/Worker_1\n",
      "Arrive at the goal! - agent:  7  - stage:  6\n",
      "Arrive at the goal! - agent:  8  - stage:  6\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Rank-0: policy_step=215120, average_reward_env_0=173.80479959324913\n",
      "Arrive at the goal! - agent:  6  - stage:  6\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Rank-0: policy_step=215256, average_reward_env_0=79.0271224725405\n",
      "Arrive at the goal! - agent:  3  - stage:  6\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Rank-0: policy_step=215296, average_reward_env_0=142.44360740715817\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Rank-0: policy_step=215368, average_reward_env_0=146.90230995032408\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Collision detected between /World/Worker_3 and /World/Warehouse/Table_03\n",
      "Collision detected between /World/Worker_3 and /World/Warehouse/Warehouse/SM_SignCVer_164/SM_SignCVer_10\n",
      "Collision detected between /World/Worker_3 and /World/Warehouse/Warehouse/SM_RackFrame_3840/SM_RackFrame_03/SM_RackFrame_03/Section1\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Rank-0: policy_step=215616, average_reward_env_0=78.65082993500658\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Rank-0: policy_step=215624, average_reward_env_0=158.59701185045284\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Rank-0: policy_step=215704, average_reward_env_0=156.3399555655385\n",
      "Arrive at the goal! - agent:  2  - stage:  6\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Rank-0: policy_step=215776, average_reward_env_0=132.32554106646015\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Rank-0: policy_step=216072, average_reward_env_0=142.1060633205411\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Arrive at the goal! - agent:  3  - stage:  6\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Rank-0: policy_step=216400, average_reward_env_0=132.5509254778168\n",
      "Collision detected between /World/Worker_3 and /World/Warehouse/Warehouse/SM_Rackshield_2984/SM_Rackshield_02\n",
      "Arrive at the goal! - agent:  5  - stage:  6\n",
      "Collision detected between /World/Worker_3 and /World/Warehouse/Forklift_B01_PR_V_NVD_01\n",
      "Max step number - agent:  8\n",
      "Rank-0: policy_step=216640, average_reward_env_0=100.27991613961225\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Rank-0: policy_step=216848, average_reward_env_0=105.55518242719711\n",
      "Arrive at the goal! - agent:  1  - stage:  6\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Arrive at the goal! - agent:  7  - stage:  6\n",
      "Max step number - agent:  1\n",
      "Rank-0: policy_step=217344, average_reward_env_0=116.96660871729533\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Rank-0: policy_step=217472, average_reward_env_0=107.1279266063534\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Rank-0: policy_step=217544, average_reward_env_0=115.10726087989245\n",
      "Arrive at the goal! - agent:  1  - stage:  6\n",
      "Collision detected between /World/Worker_4 and /World/Warehouse/Rack_04\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Arrive at the goal! - agent:  4  - stage:  6\n",
      "Arrive at the goal! - agent:  6  - stage:  6\n",
      "Arrive at the goal! - agent:  7  - stage:  6\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Arrive at the goal! - agent:  5  - stage:  6\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Rank-0: policy_step=218048, average_reward_env_0=87.66347544262251\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Arrive at the goal! - agent:  8  - stage:  6\n",
      "Arrive at the goal! - agent:  4  - stage:  6\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Rank-0: policy_step=218288, average_reward_env_0=157.0679578948626\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Arrive at the goal! - agent:  6  - stage:  6\n",
      "Arrive at the goal! - agent:  1  - stage:  6\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Rank-0: policy_step=218536, average_reward_env_0=156.90751238212775\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Rank-0: policy_step=218544, average_reward_env_0=166.89274912215157\n",
      "Max step number - agent:  4\n",
      "Rank-0: policy_step=218824, average_reward_env_0=94.93729515193021\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Rank-0: policy_step=218840, average_reward_env_0=124.12256730395768\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Arrive at the goal! - agent:  2  - stage:  6\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Arrive at the goal! - agent:  7  - stage:  6\n",
      "Max step number - agent:  7\n",
      "Rank-0: policy_step=219288, average_reward_env_0=146.32233496689184\n",
      "Arrive at the goal! - agent:  6  - stage:  6\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Rank-0: policy_step=219296, average_reward_env_1=114.57414936182248\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Arrive at the goal! - agent:  6  - stage:  6\n",
      "Max step number - agent:  6\n",
      "Rank-0: policy_step=219600, average_reward_env_0=162.2646632154339\n",
      "Arrive at the goal! - agent:  3  - stage:  6\n",
      "Collision detected between /World/Worker_2 and /World/Warehouse/Warehouse/GroundPlane/CollisionPlane\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Rank-0: policy_step=219680, average_reward_env_0=160.53225450315244\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Collision detected between /World/Worker_2 and /World/Warehouse/Warehouse/SM_FloorDecal_StripeFull_4m106/SM_FloorDecal_StripeFull_4m\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Rank-0: policy_step=219864, average_reward_env_0=148.0866460127997\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Arrive at the goal! - agent:  5  - stage:  6\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Arrive at the goal! - agent:  8  - stage:  6\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Rank-0: policy_step=220120, average_reward_env_0=160.19442358633384\n",
      "Collision detected between /World/Worker_1 and /World/Warehouse/Table_08\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Rank-0: policy_step=220184, average_reward_env_0=84.17776743173621\n",
      "Collision detected between /World/Worker_4 and /World/Worker_2\n",
      "Collision detected between /World/Worker_4 and /World/Worker_2\n",
      "Collision detected between /World/Worker_4 and /World/Worker_2\n",
      "Collision detected between /World/Worker_4 and /World/Worker_2\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Rank-0: policy_step=220320, average_reward_env_0=133.47629191951455\n",
      "Collision detected between /World/Worker_4 and /World/Warehouse/Warehouse/SM_FireExtinguisher_1929/SM_FireExtinguisher_02\n",
      "Arrive at the goal! - agent:  8  - stage:  6\n",
      "Collision detected between /World/Worker_1 and /World/Warehouse/Forklift_B01_PR_V_NVD_01\n",
      "Collision detected between /World/Worker_1 and /World/Warehouse/Forklift_B01_PR_V_NVD_01\n",
      "Arrive at the goal! - agent:  1  - stage:  6\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Rank-0: policy_step=220816, average_reward_env_1=148.41346316731722\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Arrive at the goal! - agent:  2  - stage:  6\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Rank-0: policy_step=220912, average_reward_env_0=111.69123558357563\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Arrive at the goal! - agent:  7  - stage:  6\n",
      "Arrive at the goal! - agent:  2  - stage:  6\n",
      "Max step number - agent:  2\n",
      "Rank-0: policy_step=221256, average_reward_env_0=113.79022221767656\n",
      "Arrive at the goal! - agent:  4  - stage:  6\n",
      "Arrive at the goal! - agent:  6  - stage:  6\n",
      "Arrive at the goal! - agent:  3  - stage:  6\n",
      "Arrive at the goal! - agent:  7  - stage:  6\n",
      "Collision detected between /World/Worker_4 and /World/Worker_3\n",
      "Collision detected between /World/Worker_4 and /World/Worker_3\n",
      "Collision detected between /World/Worker_4 and /World/Worker_3\n",
      "Collision detected between /World/Worker_4 and /World/Worker_3\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Rank-0: policy_step=221880, average_reward_env_0=108.07008971989198\n",
      "Arrive at the goal! - agent:  1  - stage:  6\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Rank-0: policy_step=221928, average_reward_env_0=157.44546883656136\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Max step number - agent:  1\n",
      "Rank-0: policy_step=222048, average_reward_env_0=146.4791673339521\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Arrive at the goal! - agent:  1  - stage:  6\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Rank-0: policy_step=222280, average_reward_env_0=122.04855671029465\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Rank-0: policy_step=222312, average_reward_env_0=170.82163403436033\n",
      "Collision detected between /World/Worker_3 and /World/Worker_2\n",
      "Collision detected between /World/Worker_3 and /World/Worker_2\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Arrive at the goal! - agent:  7  - stage:  6\n",
      "Collision detected between /World/Worker_3 and /World/Warehouse/Warehouse/SM_Rackshield_4492/SM_Rackshield_02\n",
      "Collision detected between /World/Worker_3 and /World/Warehouse/Warehouse/SM_Rackshield_4492/SM_Rackshield_02\n",
      "Arrive at the goal! - agent:  5  - stage:  6\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Rank-0: policy_step=222592, average_reward_env_0=151.6672605076439\n",
      "Arrive at the goal! - agent:  4  - stage:  6\n",
      "Arrive at the goal! - agent:  7  - stage:  6\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Rank-0: policy_step=222752, average_reward_env_0=107.79889644959088\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Arrive at the goal! - agent:  8  - stage:  6\n",
      "Arrive at the goal! - agent:  3  - stage:  6\n",
      "Arrive at the goal! - agent:  3  - stage:  6\n",
      "Max step number - agent:  4\n",
      "Rank-0: policy_step=223384, average_reward_env_0=108.61264935378068\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Arrive at the goal! - agent:  5  - stage:  6\n",
      "Rank-0: policy_step=223496, average_reward_env_0=138.58992572858295\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Arrive at the goal! - agent:  7  - stage:  6\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Rank-0: policy_step=223688, average_reward_env_0=134.13719405642743\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Collision detected between /World/Worker_4 and /World/Worker_1\n",
      "Collision detected between /World/Worker_4 and /World/Worker_1\n",
      "Collision detected between /World/Worker_4 and /World/Warehouse/Warehouse/SM_Rackshield_3841/SM_Rackshield_02\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Rank-0: policy_step=223824, average_reward_env_0=170.91132851014282\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Arrive at the goal! - agent:  2  - stage:  6\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Max step number - agent:  7\n",
      "Arrive at the goal! - agent:  8  - stage:  6\n",
      "Rank-0: policy_step=224024, average_reward_env_0=182.7603728283728\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Rank-0: policy_step=224104, average_reward_env_0=116.88028032866805\n",
      "Arrive at the goal! - agent:  1  - stage:  6\n",
      "Arrive at the goal! - agent:  8  - stage:  6\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Rank-0: policy_step=224176, average_reward_env_0=168.37003886806332\n",
      "Arrive at the goal! - agent:  3  - stage:  6\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Rank-0: policy_step=224352, average_reward_env_0=173.07599094205435\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Rank-0: policy_step=224400, average_reward_env_0=138.43535344790186\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Arrive at the goal! - agent:  6  - stage:  6\n",
      "Arrive at the goal! - agent:  7  - stage:  6\n",
      "Arrive at the goal! - agent:  4  - stage:  6\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Rank-0: policy_step=224912, average_reward_env_0=174.5564536772212\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Rank-0: policy_step=224952, average_reward_env_0=132.55336738074965\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Arrive at the goal! - agent:  5  - stage:  6\n",
      "Max step number - agent:  8\n",
      "Rank-0: policy_step=225088, average_reward_env_0=123.45160680110807\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Rank-0: policy_step=225120, average_reward_env_0=171.2700249931434\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Rank-0: policy_step=225136, average_reward_env_0=123.44204373253758\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Arrive at the goal! - agent:  3  - stage:  6\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Rank-0: policy_step=225248, average_reward_env_0=117.8391823837373\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Arrive at the goal! - agent:  7  - stage:  6\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Rank-0: policy_step=225360, average_reward_env_0=159.9951599734837\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Collision detected between /World/Worker_4 and /World/Warehouse/Warehouse/SM_Rackshield_2984/SM_Rackshield_02\n",
      "Arrive at the goal! - agent:  1  - stage:  6\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Rank-0: policy_step=225696, average_reward_env_0=108.57421384259207\n",
      "Arrive at the goal! - agent:  8  - stage:  6\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Rank-0: policy_step=225896, average_reward_env_0=127.87803989113691\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Arrive at the goal! - agent:  5  - stage:  6\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Arrive at the goal! - agent:  6  - stage:  6\n",
      "Arrive at the goal! - agent:  8  - stage:  6\n",
      "Arrive at the goal! - agent:  4  - stage:  6\n",
      "Arrive at the goal! - agent:  7  - stage:  6\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Rank-0: policy_step=226424, average_reward_env_0=172.66026105702647\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Max step number - agent:  4\n",
      "Rank-0: policy_step=226584, average_reward_env_0=131.83559472378423\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Rank-0: policy_step=226736, average_reward_env_0=131.08849023766857\n",
      "Rank-0: policy_step=226736, average_reward_env_1=192.4230227395341\n",
      "Arrive at the goal! - agent:  5  - stage:  6\n",
      "Arrive at the goal! - agent:  6  - stage:  6\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Rank-0: policy_step=226976, average_reward_env_0=129.40375727343744\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Rank-0: policy_step=227048, average_reward_env_0=89.6847499742224\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Arrive at the goal! - agent:  2  - stage:  6\n",
      "Arrive at the goal! - agent:  8  - stage:  6\n",
      "Arrive at the goal! - agent:  1  - stage:  6\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Rank-0: policy_step=227088, average_reward_env_1=149.23527638156\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Rank-0: policy_step=227096, average_reward_env_0=92.28788000930965\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Rank-0: policy_step=227104, average_reward_env_0=178.5408639892394\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Arrive at the goal! - agent:  6  - stage:  6\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Rank-0: policy_step=227152, average_reward_env_0=190.20590405244886\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Rank-0: policy_step=227160, average_reward_env_0=129.8228080718075\n",
      "Collision detected between /World/Worker_4 and /World/Worker_2\n",
      "Collision detected between /World/Worker_4 and /World/Worker_2\n",
      "Collision detected between /World/Worker_4 and /World/Worker_2\n",
      "Collision detected between /World/Worker_4 and /World/Worker_2\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Rank-0: policy_step=227296, average_reward_env_0=157.751201144678\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Arrive at the goal! - agent:  3  - stage:  6\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Rank-0: policy_step=227336, average_reward_env_0=147.82316753128083\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Rank-0: policy_step=227736, average_reward_env_0=92.54901923020432\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Collision detected between /World/Worker_3 and /World/Warehouse/Table_08\n",
      "Arrive at the goal! - agent:  8  - stage:  6\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Arrive at the goal! - agent:  8  - stage:  6\n",
      "Collision detected between /World/Worker_3 and /World/Worker_1\n",
      "Collision detected between /World/Worker_3 and /World/Worker_1\n",
      "Collision detected between /World/Worker_3 and /World/Worker_1\n",
      "Collision detected between /World/Worker_3 and /World/Worker_1\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Rank-0: policy_step=228032, average_reward_env_0=129.0348352330639\n",
      "Collision detected between /World/Worker_3 and /World/Warehouse/Table_02\n",
      "Collision detected between /World/Worker_3 and /World/Warehouse/Table_02\n",
      "Arrive at the goal! - agent:  7  - stage:  6\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Rank-0: policy_step=228128, average_reward_env_0=196.8543824326827\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Rank-0: policy_step=228136, average_reward_env_0=198.7812003925913\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Rank-0: policy_step=228192, average_reward_env_0=138.15496079832238\n",
      "Arrive at the goal! - agent:  3  - stage:  6\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Rank-0: policy_step=228264, average_reward_env_0=131.61815662613898\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Rank-0: policy_step=228496, average_reward_env_0=106.76120451314654\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Rank-0: policy_step=228504, average_reward_env_0=86.18989249517438\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Rank-0: policy_step=228552, average_reward_env_0=186.48424181316557\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Rank-0: policy_step=229080, average_reward_env_0=127.15150539860733\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Max step number - agent:  8\n",
      "Rank-0: policy_step=229104, average_reward_env_0=147.68860158030728\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Rank-0: policy_step=229112, average_reward_env_0=164.88492428002445\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Arrive at the goal! - agent:  5  - stage:  6\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Rank-0: policy_step=229304, average_reward_env_0=134.23439527545844\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Rank-0: policy_step=229336, average_reward_env_0=68.01311023671174\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Rank-0: policy_step=229560, average_reward_env_0=109.61117295286444\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Arrive at the goal! - agent:  4  - stage:  6\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Rank-0: policy_step=229664, average_reward_env_1=94.99870634608726\n",
      "Arrive at the goal! - agent:  3  - stage:  6\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Rank-0: policy_step=229752, average_reward_env_0=191.13365271729867\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Rank-0: policy_step=229760, average_reward_env_0=194.71864421254554\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Rank-0: policy_step=229832, average_reward_env_0=115.81839376083586\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Rank-0: policy_step=229952, average_reward_env_0=70.77478362276415\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Arrive at the goal! - agent:  1  - stage:  6\n",
      "Arrive at the goal! - agent:  8  - stage:  6\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Rank-0: policy_step=230384, average_reward_env_0=196.51995662340215\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Rank-0: policy_step=230392, average_reward_env_0=71.3004571822002\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Rank-0: policy_step=230432, average_reward_env_0=124.12899597832704\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Rank-0: policy_step=230576, average_reward_env_0=190.19623752434526\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Rank-0: policy_step=230696, average_reward_env_0=193.62210115268502\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Rank-0: policy_step=230744, average_reward_env_0=118.37733912386737\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Rank-0: policy_step=230768, average_reward_env_0=96.85298987256088\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Rank-0: policy_step=230824, average_reward_env_0=154.06420343715968\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Rank-0: policy_step=230872, average_reward_env_0=154.01895779886993\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Arrive at the goal! - agent:  3  - stage:  6\n",
      "Arrive at the goal! - agent:  8  - stage:  6\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Rank-0: policy_step=231216, average_reward_env_0=67.68502391241704\n",
      "Arrive at the goal! - agent:  3  - stage:  6\n",
      "Arrive at the goal! - agent:  6  - stage:  6\n",
      "Arrive at the goal! - agent:  7  - stage:  6\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Rank-0: policy_step=231688, average_reward_env_0=117.26845085112964\n",
      "Arrive at the goal! - agent:  7  - stage:  6\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Rank-0: policy_step=231904, average_reward_env_0=141.241968209058\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Arrive at the goal! - agent:  8  - stage:  6\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Arrive at the goal! - agent:  5  - stage:  6\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Rank-0: policy_step=232096, average_reward_env_0=106.0932924737914\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Rank-0: policy_step=232144, average_reward_env_0=105.27484156298607\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Rank-0: policy_step=232160, average_reward_env_0=105.49582822422343\n",
      "Collision detected between /World/Worker_3 and /World/Warehouse/Table_09\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Arrive at the goal! - agent:  1  - stage:  6\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Rank-0: policy_step=232176, average_reward_env_1=164.7881657784852\n",
      "Arrive at the goal! - agent:  8  - stage:  6\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Rank-0: policy_step=232360, average_reward_env_0=123.50990206722066\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Rank-0: policy_step=232416, average_reward_env_0=104.22048386389908\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Rank-0: policy_step=232480, average_reward_env_0=200.97162765635417\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Rank-0: policy_step=232624, average_reward_env_0=72.41113771125244\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Rank-0: policy_step=232648, average_reward_env_0=100.61260816831043\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Rank-0: policy_step=232696, average_reward_env_0=202.140944268116\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Rank-0: policy_step=232808, average_reward_env_0=101.14071928792288\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Rank-0: policy_step=232920, average_reward_env_0=122.68219546911149\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Rank-0: policy_step=232984, average_reward_env_0=99.31527697240799\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Rank-0: policy_step=233000, average_reward_env_0=128.01159480501272\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Rank-0: policy_step=233336, average_reward_env_0=124.4423602337716\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Arrive at the goal! - agent:  5  - stage:  6\n",
      "Arrive at the goal! - agent:  3  - stage:  6\n",
      "Collision detected between /World/Worker_4 and /World/Warehouse/Warehouse/GroundPlane/CollisionPlane\n",
      "Arrive at the goal! - agent:  7  - stage:  6\n",
      "Arrive at the goal! - agent:  7  - stage:  6\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Rank-0: policy_step=233608, average_reward_env_0=116.7709263362755\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Rank-0: policy_step=233712, average_reward_env_0=76.88138268140888\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Arrive at the goal! - agent:  4  - stage:  6\n",
      "Arrive at the goal! - agent:  1  - stage:  6\n",
      "Arrive at the goal! - agent:  1  - stage:  6\n",
      "Arrive at the goal! - agent:  8  - stage:  6\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Arrive at the goal! - agent:  6  - stage:  6\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Rank-0: policy_step=234088, average_reward_env_0=108.92794558861534\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Arrive at the goal! - agent:  5  - stage:  6\n",
      "Arrive at the goal! - agent:  3  - stage:  6\n",
      "Arrive at the goal! - agent:  7  - stage:  6\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Rank-0: policy_step=234616, average_reward_env_0=164.1135734027761\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Arrive at the goal! - agent:  1  - stage:  6\n",
      "Arrive at the goal! - agent:  6  - stage:  6\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Rank-0: policy_step=234832, average_reward_env_0=101.18865666925961\n",
      "Rank-0: policy_step=234832, average_reward_env_1=125.37189563254617\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Arrive at the goal! - agent:  5  - stage:  6\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Rank-0: policy_step=234880, average_reward_env_0=145.99335737882495\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Rank-0: policy_step=234952, average_reward_env_0=140.48927859505855\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Arrive at the goal! - agent:  5  - stage:  6\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Rank-0: policy_step=235216, average_reward_env_0=221.656770012837\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Rank-0: policy_step=235440, average_reward_env_0=126.08731970683169\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Arrive at the goal! - agent:  3  - stage:  6\n",
      "Collision detected between /World/Worker_2 and /World/Warehouse/Table_09\n",
      "Collision detected between /World/Worker_3 and /World/Worker_1\n",
      "Collision detected between /World/Worker_3 and /World/Worker_1\n",
      "Arrive at the goal! - agent:  4  - stage:  6\n",
      "Collision detected between /World/Worker_2 and /World/Warehouse/Warehouse/SM_FloorDecal_StripeFull_4m108/SM_FloorDecal_StripeFull_4m\n",
      "Max step number - agent:  5\n",
      "Rank-0: policy_step=235896, average_reward_env_0=220.43344796482327\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Rank-0: policy_step=236200, average_reward_env_0=219.29560194358498\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Rank-0: policy_step=236208, average_reward_env_0=73.74703891864213\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Rank-0: policy_step=236248, average_reward_env_0=193.49439208421663\n",
      "Arrive at the goal! - agent:  6  - stage:  6\n",
      "Collision detected between /World/Worker_4 and /World/Worker_3\n",
      "Collision detected between /World/Worker_4 and /World/Worker_3\n",
      "Collision detected between /World/Worker_4 and /World/Worker_3\n",
      "Collision detected between /World/Worker_4 and /World/Worker_3\n",
      "Collision detected between /World/Worker_1 and /World/Warehouse/Rack_07\n",
      "Arrive at the goal! - agent:  2  - stage:  6\n",
      "Arrive at the goal! - agent:  7  - stage:  6\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Rank-0: policy_step=236528, average_reward_env_0=119.10014260394459\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Rank-0: policy_step=236688, average_reward_env_0=123.69451667563003\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Collision detected between /World/Worker_3 and /World/Worker_1\n",
      "Collision detected between /World/Worker_3 and /World/Worker_1\n",
      "Arrive at the goal! - agent:  3  - stage:  6\n",
      "Arrive at the goal! - agent:  1  - stage:  6\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Arrive at the goal! - agent:  7  - stage:  6\n",
      "Arrive at the goal! - agent:  6  - stage:  6\n",
      "Arrive at the goal! - agent:  5  - stage:  6\n",
      "Collision detected between /World/Worker_1 and /World/Warehouse/Rack_02\n",
      "Arrive at the goal! - agent:  8  - stage:  6\n",
      "Arrive at the goal! - agent:  4  - stage:  6\n",
      "Arrive at the goal! - agent:  1  - stage:  6\n",
      "Arrive at the goal! - agent:  7  - stage:  6\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Rank-0: policy_step=237864, average_reward_env_0=125.51799251264381\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Arrive at the goal! - agent:  6  - stage:  6\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Rank-0: policy_step=238000, average_reward_env_0=129.92285487271243\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Max step number - agent:  6\n",
      "Rank-0: policy_step=238040, average_reward_env_0=127.86102648411244\n",
      "Arrive at the goal! - agent:  6  - stage:  6\n",
      "Max step number - agent:  3\n",
      "Rank-0: policy_step=238088, average_reward_env_0=156.91519757724683\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Rank-0: policy_step=238120, average_reward_env_0=122.92292799169138\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Rank-0: policy_step=238136, average_reward_env_0=138.53008398030005\n",
      "Arrive at the goal! - agent:  5  - stage:  6\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Rank-0: policy_step=238360, average_reward_env_0=129.00059209167313\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Max step number - agent:  7\n",
      "Rank-0: policy_step=238416, average_reward_env_0=238.1362350791727\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Rank-0: policy_step=238496, average_reward_env_0=156.62063342711713\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Rank-0: policy_step=238552, average_reward_env_0=211.0316762045708\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Rank-0: policy_step=238568, average_reward_env_0=220.9557881031817\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Rank-0: policy_step=238824, average_reward_env_0=112.68545650395632\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Rank-0: policy_step=238864, average_reward_env_0=140.3816705278369\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Arrive at the goal! - agent:  4  - stage:  6\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Rank-0: policy_step=239040, average_reward_env_0=139.32649714346115\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Rank-0: policy_step=239208, average_reward_env_0=77.96763589334584\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Rank-0: policy_step=239240, average_reward_env_0=130.53866605895337\n",
      "Arrive at the goal! - agent:  8  - stage:  6\n",
      "Arrive at the goal! - agent:  5  - stage:  6\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Rank-0: policy_step=239528, average_reward_env_0=149.82443163153104\n",
      "Arrive at the goal! - agent:  3  - stage:  6\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Rank-0: policy_step=239792, average_reward_env_0=218.31956856749457\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Arrive at the goal! - agent:  3  - stage:  6\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Rank-0: policy_step=239960, average_reward_env_0=115.29164968411514\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Rank-0: policy_step=240080, average_reward_env_0=153.9995051266736\n",
      "Rank-0: policy_step=240080, average_reward_env_1=209.5673156494482\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Arrive at the goal! - agent:  2  - stage:  6\n",
      "Arrive at the goal! - agent:  6  - stage:  6\n",
      "Arrive at the goal! - agent:  1  - stage:  6\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Rank-0: policy_step=240504, average_reward_env_0=135.9004021484465\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Rank-0: policy_step=240512, average_reward_env_0=129.28172295730917\n",
      "Arrive at the goal! - agent:  7  - stage:  6\n",
      "Arrive at the goal! - agent:  7  - stage:  6\n",
      "Arrive at the goal! - agent:  5  - stage:  6\n",
      "Arrive at the goal! - agent:  2  - stage:  6\n",
      "Collision detected between /World/Worker_3 and /World/Warehouse/Table_09\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Rank-0: policy_step=240784, average_reward_env_0=135.23586980444406\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Rank-0: policy_step=240824, average_reward_env_0=91.6857371525984\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Rank-0: policy_step=240992, average_reward_env_0=136.66001145970148\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Collision detected between /World/Worker_3 and /World/Warehouse/Warehouse/SM_WallA_6M18/SM_WallA_6M/SM_WallA_6M/Section0\n",
      "Collision detected between /World/Worker_3 and /World/Warehouse/Warehouse/SM_WallA_6M18/SM_WallA_6M/SM_WallA_6M/Section0\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Rank-0: policy_step=241208, average_reward_env_0=113.6051551185601\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Rank-0: policy_step=241232, average_reward_env_0=137.51655945319723\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Rank-0: policy_step=241288, average_reward_env_0=139.89097570680602\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Rank-0: policy_step=241360, average_reward_env_0=112.48432783161641\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Rank-0: policy_step=241480, average_reward_env_0=110.9931656810788\n",
      "Arrive at the goal! - agent:  7  - stage:  6\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Rank-0: policy_step=241768, average_reward_env_0=95.62230315485296\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Arrive at the goal! - agent:  8  - stage:  6\n",
      "Arrive at the goal! - agent:  2  - stage:  6\n",
      "Arrive at the goal! - agent:  5  - stage:  6\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Arrive at the goal! - agent:  4  - stage:  6\n",
      "Arrive at the goal! - agent:  2  - stage:  6\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Rank-0: policy_step=242176, average_reward_env_1=143.78533566851016\n",
      "Collision detected between /World/Worker_3 and /World/Warehouse/Table_03\n",
      "Arrive at the goal! - agent:  3  - stage:  6\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Rank-0: policy_step=242312, average_reward_env_0=106.02541747251342\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Arrive at the goal! - agent:  1  - stage:  6\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Rank-0: policy_step=242640, average_reward_env_0=236.15190627942252\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Rank-0: policy_step=242768, average_reward_env_0=142.92235585721696\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Arrive at the goal! - agent:  7  - stage:  6\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Max step number - agent:  7\n",
      "Rank-0: policy_step=243288, average_reward_env_0=238.62276966209134\n",
      "Collision detected between /World/Worker_2 and /World/Warehouse/Warehouse/GroundPlane/CollisionPlane\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Rank-0: policy_step=243376, average_reward_env_0=116.53290627282526\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Arrive at the goal! - agent:  2  - stage:  6\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Arrive at the goal! - agent:  4  - stage:  6\n",
      "Rank-0: policy_step=243744, average_reward_env_0=139.94136792542008\n",
      "Arrive at the goal! - agent:  5  - stage:  6\n",
      "Arrive at the goal! - agent:  6  - stage:  6\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Max step number - agent:  4\n",
      "Rank-0: policy_step=243984, average_reward_env_0=140.8695981574162\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Rank-0: policy_step=244080, average_reward_env_0=118.83901123013543\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Rank-0: policy_step=244096, average_reward_env_0=232.5452360661937\n",
      "Arrive at the goal! - agent:  2  - stage:  6\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Rank-0: policy_step=244320, average_reward_env_0=126.79052463540391\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Rank-0: policy_step=244376, average_reward_env_0=108.27881640570729\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Rank-0: policy_step=244472, average_reward_env_0=94.4802238034867\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Rank-0: policy_step=244496, average_reward_env_0=210.21739532761683\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Arrive at the goal! - agent:  4  - stage:  6\n",
      "Rank-0: policy_step=244520, average_reward_env_0=79.02731826847558\n",
      "Arrive at the goal! - agent:  8  - stage:  6\n",
      "Arrive at the goal! - agent:  1  - stage:  6\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Rank-0: policy_step=244864, average_reward_env_0=78.01607797840632\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Arrive at the goal! - agent:  6  - stage:  6\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Rank-0: policy_step=245112, average_reward_env_0=207.62514541085932\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Rank-0: policy_step=245352, average_reward_env_0=139.94110403841722\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Arrive at the goal! - agent:  3  - stage:  6\n",
      "Collision detected between /World/Worker_4 and /World/Warehouse/Warehouse/GroundPlane/CollisionPlane\n",
      "Arrive at the goal! - agent:  1  - stage:  6\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Rank-0: policy_step=245512, average_reward_env_0=123.41572461244276\n",
      "Arrive at the goal! - agent:  8  - stage:  6\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Rank-0: policy_step=245688, average_reward_env_0=130.2810152471262\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Max step number - agent:  5\n",
      "Rank-0: policy_step=245848, average_reward_env_0=239.4104854779065\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Max step number - agent:  8\n",
      "Rank-0: policy_step=245976, average_reward_env_0=154.4174058523042\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Rank-0: policy_step=246016, average_reward_env_0=153.94114082465836\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Arrive at the goal! - agent:  6  - stage:  6\n",
      "Arrive at the goal! - agent:  2  - stage:  6\n",
      "Arrive at the goal! - agent:  2  - stage:  6\n",
      "Arrive at the goal! - agent:  1  - stage:  6\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Rank-0: policy_step=246312, average_reward_env_0=161.56071822852485\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Rank-0: policy_step=246320, average_reward_env_0=170.1639054030169\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Arrive at the goal! - agent:  7  - stage:  6\n",
      "Arrive at the goal! - agent:  3  - stage:  6\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Rank-0: policy_step=246488, average_reward_env_0=130.96536025069656\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Rank-0: policy_step=246504, average_reward_env_0=165.91137887903815\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Collision detected between /World/Worker_2 and /World/Warehouse/Table_12\n",
      "Arrive at the goal! - agent:  6  - stage:  6\n",
      "Collision detected between /World/Worker_4 and /World/Warehouse/Table_08\n",
      "Arrive at the goal! - agent:  5  - stage:  6\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Arrive at the goal! - agent:  8  - stage:  6\n",
      "Rank-0: policy_step=247120, average_reward_env_0=113.01807016184893\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Rank-0: policy_step=247368, average_reward_env_0=130.34443141007532\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Max step number - agent:  6\n",
      "Rank-0: policy_step=247528, average_reward_env_0=151.56271908965402\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Rank-0: policy_step=247600, average_reward_env_0=229.60541229007245\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Arrive at the goal! - agent:  7  - stage:  6\n",
      "Rank-0: policy_step=247680, average_reward_env_0=149.58564302971345\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Rank-0: policy_step=247720, average_reward_env_0=130.30553052497112\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Rank-0: policy_step=247848, average_reward_env_0=107.86740116952289\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Rank-0: policy_step=247864, average_reward_env_0=114.37289233603738\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Arrive at the goal! - agent:  2  - stage:  6\n",
      "Max step number - agent:  2\n",
      "Rank-0: policy_step=248072, average_reward_env_0=104.7486146496434\n",
      "Collision detected between /World/Worker_3 and /World/Worker_2\n",
      "Collision detected between /World/Worker_3 and /World/Worker_2\n",
      "Arrive at the goal! - agent:  6  - stage:  6\n",
      "Max step number - agent:  7\n",
      "Rank-0: policy_step=248320, average_reward_env_0=195.61047501842427\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Rank-0: policy_step=248384, average_reward_env_0=127.91589723858198\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Rank-0: policy_step=248400, average_reward_env_0=106.12426700142167\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Rank-0: policy_step=248432, average_reward_env_0=166.45385465821\n",
      "Arrive at the goal! - agent:  1  - stage:  6\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Rank-0: policy_step=248568, average_reward_env_0=103.11790755294766\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Rank-0: policy_step=248592, average_reward_env_0=148.14605321929506\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Arrive at the goal! - agent:  8  - stage:  6\n",
      "Arrive at the goal! - agent:  1  - stage:  6\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Rank-0: policy_step=248760, average_reward_env_0=139.72277806337922\n",
      "Arrive at the goal! - agent:  3  - stage:  6\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Rank-0: policy_step=248824, average_reward_env_0=80.60077526096326\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Rank-0: policy_step=249208, average_reward_env_0=196.278875160422\n",
      "Arrive at the goal! - agent:  6  - stage:  6\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Rank-0: policy_step=249272, average_reward_env_0=202.11472547702886\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Arrive at the goal! - agent:  3  - stage:  6\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Rank-0: policy_step=249384, average_reward_env_0=201.13630455087133\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Rank-0: policy_step=249448, average_reward_env_0=129.7286135763721\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Rank-0: policy_step=249664, average_reward_env_0=165.0634638103254\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Rank-0: policy_step=249672, average_reward_env_0=87.25651528969075\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Rank-0: policy_step=249728, average_reward_env_0=136.78256404080446\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Rank-0: policy_step=249808, average_reward_env_0=157.79477815253674\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Arrive at the goal! - agent:  7  - stage:  6\n",
      "Arrive at the goal! - agent:  5  - stage:  6\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Rank-0: policy_step=250200, average_reward_env_0=193.89595785139392\n",
      "Arrive at the goal! - agent:  2  - stage:  6\n",
      "Arrive at the goal! - agent:  2  - stage:  6\n",
      "Arrive at the goal! - agent:  6  - stage:  6\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Rank-0: policy_step=250560, average_reward_env_0=91.23092665540261\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Rank-0: policy_step=250592, average_reward_env_0=107.47152264563096\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Rank-0: policy_step=250600, average_reward_env_0=131.10219527151878\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Rank-0: policy_step=250640, average_reward_env_0=106.07143044766644\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Rank-0: policy_step=250696, average_reward_env_0=106.1027052756222\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Rank-0: policy_step=250800, average_reward_env_0=68.89686918560072\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Rank-0: policy_step=250816, average_reward_env_0=141.5827415624644\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Rank-0: policy_step=250888, average_reward_env_0=192.84788601608113\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Arrive at the goal! - agent:  1  - stage:  6\n",
      "Arrive at the goal! - agent:  1  - stage:  6\n",
      "Arrive at the goal! - agent:  4  - stage:  6\n",
      "Arrive at the goal! - agent:  1  - stage:  6\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Rank-0: policy_step=251024, average_reward_env_0=108.46005109786867\n",
      "Arrive at the goal! - agent:  5  - stage:  6\n",
      "Arrive at the goal! - agent:  2  - stage:  6\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Rank-0: policy_step=251368, average_reward_env_0=103.09733993941947\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Arrive at the goal! - agent:  8  - stage:  6\n",
      "Rank-0: policy_step=251480, average_reward_env_0=152.55219446504788\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Rank-0: policy_step=251488, average_reward_env_0=216.5142048541723\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Collision detected between /World/Worker_3 and /World/Warehouse/Table_09\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Rank-0: policy_step=251672, average_reward_env_0=208.0497469280143\n",
      "Arrive at the goal! - agent:  7  - stage:  6\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Rank-0: policy_step=251920, average_reward_env_0=173.87352204770548\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Arrive at the goal! - agent:  8  - stage:  6\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Rank-0: policy_step=252016, average_reward_env_0=185.47654674177275\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Rank-0: policy_step=252072, average_reward_env_0=149.36366783764603\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Arrive at the goal! - agent:  4  - stage:  6\n",
      "Arrive at the goal! - agent:  3  - stage:  6\n",
      "Arrive at the goal! - agent:  2  - stage:  6\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Rank-0: policy_step=252544, average_reward_env_0=97.9567909215828\n",
      "Arrive at the goal! - agent:  5  - stage:  6\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Rank-0: policy_step=252584, average_reward_env_0=171.25927160785312\n",
      "Arrive at the goal! - agent:  6  - stage:  6\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Rank-0: policy_step=252704, average_reward_env_0=132.75687522077237\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Arrive at the goal! - agent:  8  - stage:  6\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Rank-0: policy_step=253336, average_reward_env_0=189.37373275714938\n",
      "Arrive at the goal! - agent:  4  - stage:  6\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Arrive at the goal! - agent:  7  - stage:  6\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Rank-0: policy_step=253440, average_reward_env_0=178.461697344584\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Arrive at the goal! - agent:  2  - stage:  6\n",
      "Collision detected between /World/Worker_1 and /World/Warehouse/Warehouse/GroundPlane/CollisionPlane\n",
      "Arrive at the goal! - agent:  3  - stage:  6\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Rank-0: policy_step=253736, average_reward_env_0=101.76079550530935\n",
      "Arrive at the goal! - agent:  1  - stage:  6\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Rank-0: policy_step=253792, average_reward_env_0=138.07502798267748\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Arrive at the goal! - agent:  7  - stage:  6\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collision detected between /World/Worker_4 and /World/Worker_1\n",
      "Collision detected between /World/Worker_4 and /World/Worker_1\n",
      "Max step number - agent:  4\n",
      "Collision detected between /World/Worker_4 and /World/Worker_1\n",
      "Collision detected between /World/Worker_4 and /World/Worker_1\n",
      "Rank-0: policy_step=254008, average_reward_env_0=93.43523311279529\n",
      "Arrive at the goal! - agent:  5  - stage:  6\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Rank-0: policy_step=254120, average_reward_env_0=186.6157109957968\n",
      "Arrive at the goal! - agent:  5  - stage:  6\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Rank-0: policy_step=254320, average_reward_env_0=154.87053767400232\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Rank-0: policy_step=254472, average_reward_env_0=154.96535497242903\n",
      "Collision detected between /World/Worker_4 and /World/Warehouse/Rack_07\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Arrive at the goal! - agent:  8  - stage:  6\n",
      "Rank-0: policy_step=254552, average_reward_env_0=103.43424866590632\n",
      "Max step number - agent:  8\n",
      "Rank-0: policy_step=254576, average_reward_env_0=105.77736845871421\n",
      "Arrive at the goal! - agent:  6  - stage:  6\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Rank-0: policy_step=254824, average_reward_env_0=159.34173115654679\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Arrive at the goal! - agent:  4  - stage:  6\n",
      "Arrive at the goal! - agent:  8  - stage:  6\n",
      "Arrive at the goal! - agent:  5  - stage:  6\n",
      "Arrive at the goal! - agent:  3  - stage:  6\n",
      "Arrive at the goal! - agent:  4  - stage:  6\n",
      "Arrive at the goal! - agent:  5  - stage:  6\n",
      "Arrive at the goal! - agent:  3  - stage:  6\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Rank-0: policy_step=255656, average_reward_env_0=139.42055931545903\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Rank-0: policy_step=255800, average_reward_env_0=138.26979320318185\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Max step number - agent:  3\n",
      "Rank-0: policy_step=255912, average_reward_env_0=148.515303042959\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Rank-0: policy_step=256056, average_reward_env_0=162.14362342481115\n",
      "Arrive at the goal! - agent:  2  - stage:  6\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Rank-0: policy_step=256216, average_reward_env_0=116.54204878161474\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Rank-0: policy_step=256312, average_reward_env_0=154.56174521745962\n",
      "Arrive at the goal! - agent:  4  - stage:  6\n",
      "Collision detected between /World/Worker_2 and /World/Warehouse/Warehouse/GroundPlane/CollisionPlane\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Rank-0: policy_step=256480, average_reward_env_0=147.03858773246756\n",
      "Collision detected between /World/Worker_1 and /World/Warehouse/Rack_02\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Rank-0: policy_step=256616, average_reward_env_0=190.59713759518928\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Rank-0: policy_step=256632, average_reward_env_0=133.82370399764125\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Rank-0: policy_step=256720, average_reward_env_0=139.77670073568044\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Arrive at the goal! - agent:  7  - stage:  6\n",
      "Arrive at the goal! - agent:  7  - stage:  6\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Arrive at the goal! - agent:  8  - stage:  6\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Rank-0: policy_step=257064, average_reward_env_0=115.55862782220115\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Arrive at the goal! - agent:  2  - stage:  6\n",
      "Max step number - agent:  4\n",
      "Rank-0: policy_step=257208, average_reward_env_0=120.87590338334171\n",
      "Arrive at the goal! - agent:  5  - stage:  6\n",
      "Arrive at the goal! - agent:  7  - stage:  6\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Rank-0: policy_step=257336, average_reward_env_0=134.21992889026626\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Arrive at the goal! - agent:  1  - stage:  6\n",
      "Arrive at the goal! - agent:  3  - stage:  6\n",
      "Arrive at the goal! - agent:  1  - stage:  6\n",
      "Max step number - agent:  2\n",
      "Rank-0: policy_step=257752, average_reward_env_1=121.9535741562726\n",
      "Arrive at the goal! - agent:  7  - stage:  6\n",
      "Arrive at the goal! - agent:  6  - stage:  6\n",
      "Arrive at the goal! - agent:  5  - stage:  6\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Rank-0: policy_step=258104, average_reward_env_0=151.65251973308546\n",
      "Rank-0: policy_step=258104, average_reward_env_1=199.78594132760935\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Collision detected between /World/Worker_2 and /World/Warehouse/Warehouse/SM_Rackshield_3841/SM_Rackshield_02\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Rank-0: policy_step=258336, average_reward_env_0=189.38152192044268\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Collision detected between /World/Worker_4 and /World/Warehouse/Table_09\n",
      "Arrive at the goal! - agent:  1  - stage:  6\n",
      "Arrive at the goal! - agent:  4  - stage:  6\n",
      "Arrive at the goal! - agent:  8  - stage:  6\n",
      "Arrive at the goal! - agent:  8  - stage:  6\n",
      "Arrive at the goal! - agent:  6  - stage:  6\n",
      "Arrive at the goal! - agent:  4  - stage:  6\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Rank-0: policy_step=258736, average_reward_env_0=132.09030986230817\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Rank-0: policy_step=258816, average_reward_env_0=175.48830611931004\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Arrive at the goal! - agent:  7  - stage:  6\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Arrive at the goal! - agent:  2  - stage:  6\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Rank-0: policy_step=259240, average_reward_env_0=155.990869158308\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Rank-0: policy_step=259312, average_reward_env_0=117.65696832779084\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Rank-0: policy_step=259376, average_reward_env_0=158.34525134596686\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Arrive at the goal! - agent:  2  - stage:  6\n",
      "Arrive at the goal! - agent:  2  - stage:  6\n",
      "Arrive at the goal! - agent:  5  - stage:  6\n",
      "Max step number - agent:  7\n",
      "Rank-0: policy_step=259840, average_reward_env_0=164.75535250672002\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Rank-0: policy_step=259984, average_reward_env_0=111.40308911585123\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collision detected between /World/Worker_4 and /World/Warehouse/Table_03\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Rank-0: policy_step=260136, average_reward_env_0=144.91457868297516\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Arrive at the goal! - agent:  4  - stage:  6\n",
      "Max step number - agent:  4\n",
      "Rank-0: policy_step=260408, average_reward_env_0=144.72337133271716\n",
      "Arrive at the goal! - agent:  1  - stage:  6\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Rank-0: policy_step=260616, average_reward_env_0=166.5238845963588\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Rank-0: policy_step=260632, average_reward_env_0=169.89151695263396\n",
      "Arrive at the goal! - agent:  6  - stage:  6\n",
      "Arrive at the goal! - agent:  8  - stage:  6\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Rank-0: policy_step=260768, average_reward_env_0=166.83139065096216\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Rank-0: policy_step=260800, average_reward_env_0=139.86049139524462\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Rank-0: policy_step=260872, average_reward_env_0=138.28494332776702\n",
      "Arrive at the goal! - agent:  1  - stage:  6\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Rank-0: policy_step=260952, average_reward_env_0=137.32032244332711\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Rank-0: policy_step=261216, average_reward_env_0=166.56860454863852\n",
      "Arrive at the goal! - agent:  3  - stage:  6\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Collision detected between /World/Worker_3 and /World/Warehouse/Warehouse/GroundPlane/CollisionPlane\n",
      "Collision detected between /World/Worker_3 and /World/Warehouse/Warehouse/SM_PaletteA_3055/SM_PaletteA_01\n",
      "Collision detected between /World/Worker_3 and /World/Warehouse/Warehouse/Box_19299/SM_CardBoxA_01/SM_CardBoxA_01\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Rank-0: policy_step=261368, average_reward_env_0=119.36579774712294\n",
      "Arrive at the goal! - agent:  2  - stage:  6\n",
      "Arrive at the goal! - agent:  4  - stage:  6\n",
      "Collision detected between /World/Worker_2 and /World/Worker_1\n",
      "Collision detected between /World/Worker_2 and /World/Worker_1\n",
      "Collision detected between /World/Worker_2 and /World/Worker_1\n",
      "Collision detected between /World/Worker_2 and /World/Worker_1\n",
      "Collision detected between /World/Worker_2 and /World/Warehouse/Forklift_B01_PR_V_NVD_01\n",
      "Arrive at the goal! - agent:  6  - stage:  6\n",
      "Arrive at the goal! - agent:  3  - stage:  6\n",
      "Arrive at the goal! - agent:  2  - stage:  6\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Rank-0: policy_step=262024, average_reward_env_0=120.58067412700147\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Rank-0: policy_step=262032, average_reward_env_0=125.00137045340104\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Rank-0: policy_step=262168, average_reward_env_0=155.09236060349676\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Rank-0: policy_step=262192, average_reward_env_0=169.08758980037896\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Arrive at the goal! - agent:  4  - stage:  6\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Rank-0: policy_step=262312, average_reward_env_0=144.88036184875068\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Collision detected between /World/Worker_4 and /World/Worker_3\n",
      "Collision detected between /World/Worker_4 and /World/Worker_3\n",
      "Collision detected between /World/Worker_4 and /World/Worker_3\n",
      "Collision detected between /World/Worker_4 and /World/Worker_3\n",
      "Arrive at the goal! - agent:  6  - stage:  6\n",
      "Collision detected between /World/Worker_4 and /World/Warehouse/Warehouse/Box_37276/SM_CardBoxC_01/SM_CardBoxC_01\n",
      "Collision detected between /World/Worker_4 and /World/Warehouse/Warehouse/SM_RackShelf_3822/SM_RackShelf_01/SM_RackShelf_01/Section1\n",
      "Collision detected between /World/Worker_4 and /World/Worker_1\n",
      "Collision detected between /World/Worker_4 and /World/Worker_1\n",
      "Collision detected between /World/Worker_1 and /World/Warehouse/Warehouse/SM_Rackshield_3841/SM_Rackshield_02\n",
      "Collision detected between /World/Worker_4 and /World/Worker_1\n",
      "Collision detected between /World/Worker_1 and /World/Warehouse/Warehouse/Box_43762/SM_CardBoxC_01/SM_CardBoxC_01\n",
      "Collision detected between /World/Worker_1 and /World/Warehouse/Warehouse/Box_43762/SM_CardBoxC_01/SM_CardBoxC_01\n",
      "Arrive at the goal! - agent:  5  - stage:  6\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Rank-0: policy_step=262864, average_reward_env_0=168.96317653757185\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Arrive at the goal! - agent:  8  - stage:  6\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Arrive at the goal! - agent:  6  - stage:  6\n",
      "Arrive at the goal! - agent:  1  - stage:  6\n",
      "Arrive at the goal! - agent:  3  - stage:  6\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Rank-0: policy_step=263296, average_reward_env_0=163.18927434382138\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Rank-0: policy_step=263320, average_reward_env_0=188.61947422845117\n",
      "Arrive at the goal! - agent:  2  - stage:  6\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Rank-0: policy_step=263416, average_reward_env_0=169.35321784679664\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Arrive at the goal! - agent:  1  - stage:  6\n",
      "Arrive at the goal! - agent:  1  - stage:  6\n",
      "Collision detected between /World/Worker_3 and /World/Worker_2\n",
      "Collision detected between /World/Worker_3 and /World/Worker_2\n",
      "Collision detected between /World/Worker_3 and /World/Worker_2\n",
      "Collision detected between /World/Worker_3 and /World/Worker_2\n",
      "Arrive at the goal! - agent:  3  - stage:  6\n",
      "Arrive at the goal! - agent:  7  - stage:  6\n",
      "Arrive at the goal! - agent:  8  - stage:  6\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Rank-0: policy_step=264008, average_reward_env_0=154.2106947315982\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Rank-0: policy_step=264136, average_reward_env_0=163.91219641527425\n",
      "Arrive at the goal! - agent:  4  - stage:  6\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Rank-0: policy_step=264192, average_reward_env_0=168.41108655811212\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Rank-0: policy_step=264296, average_reward_env_0=128.36098739029117\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Arrive at the goal! - agent:  1  - stage:  6\n",
      "Arrive at the goal! - agent:  4  - stage:  6\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Rank-0: policy_step=264544, average_reward_env_0=126.29547209645098\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Rank-0: policy_step=264752, average_reward_env_0=169.66316283744973\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Rank-0: policy_step=264832, average_reward_env_0=163.70927955492414\n",
      "Arrive at the goal! - agent:  3  - stage:  6\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Rank-0: policy_step=264840, average_reward_env_1=165.4656695003447\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Rank-0: policy_step=264952, average_reward_env_0=162.19890913162263\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Arrive at the goal! - agent:  2  - stage:  6\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Rank-0: policy_step=265216, average_reward_env_0=181.84633925219953\n",
      "Max step number - agent:  3\n",
      "Rank-0: policy_step=265240, average_reward_env_0=139.31255895244337\n",
      "Max step number - agent:  1\n",
      "Rank-0: policy_step=265376, average_reward_env_0=184.89112844070988\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Rank-0: policy_step=265504, average_reward_env_0=123.13675543270536\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Rank-0: policy_step=265600, average_reward_env_0=162.52926338503022\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Arrive at the goal! - agent:  8  - stage:  6\n",
      "Arrive at the goal! - agent:  6  - stage:  6\n",
      "Collision detected between /World/Worker_3 and /World/Warehouse/Table_03\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Rank-0: policy_step=266040, average_reward_env_0=185.41291012014162\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Arrive at the goal! - agent:  4  - stage:  6\n",
      "Arrive at the goal! - agent:  7  - stage:  6\n",
      "Arrive at the goal! - agent:  2  - stage:  6\n",
      "Arrive at the goal! - agent:  7  - stage:  6\n",
      "Arrive at the goal! - agent:  5  - stage:  6\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Rank-0: policy_step=266464, average_reward_env_0=166.16493052465307\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Arrive at the goal! - agent:  6  - stage:  6\n",
      "Arrive at the goal! - agent:  3  - stage:  6\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Rank-0: policy_step=266784, average_reward_env_0=165.1334289622902\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Collision detected between /World/Worker_4 and /World/Warehouse/Table_08\n",
      "Arrive at the goal! - agent:  7  - stage:  6\n",
      "Arrive at the goal! - agent:  4  - stage:  6\n",
      "Collision detected between /World/Worker_3 and /World/Worker_1\n",
      "Collision detected between /World/Worker_3 and /World/Worker_1\n",
      "Arrive at the goal! - agent:  8  - stage:  6\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Rank-0: policy_step=266904, average_reward_env_0=161.40944869266693\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Arrive at the goal! - agent:  1  - stage:  6\n",
      "Arrive at the goal! - agent:  5  - stage:  6\n",
      "Arrive at the goal! - agent:  4  - stage:  6\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Rank-0: policy_step=267208, average_reward_env_1=185.7435323349513\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Rank-0: policy_step=267352, average_reward_env_0=141.43926216179014\n",
      "Arrive at the goal! - agent:  5  - stage:  6\n",
      "Arrive at the goal! - agent:  7  - stage:  6\n",
      "Arrive at the goal! - agent:  3  - stage:  6\n",
      "Max step number - agent:  8\n",
      "Rank-0: policy_step=267744, average_reward_env_0=145.46062733611421\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Rank-0: policy_step=267776, average_reward_env_0=180.43553551863067\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Arrive at the goal! - agent:  7  - stage:  6\n",
      "Arrive at the goal! - agent:  4  - stage:  6\n",
      "Arrive at the goal! - agent:  5  - stage:  6\n",
      "Max step number - agent:  4\n",
      "Rank-0: policy_step=268160, average_reward_env_0=188.43646562707528\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Rank-0: policy_step=268216, average_reward_env_0=152.0657522631409\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Rank-0: policy_step=268640, average_reward_env_0=153.3063261974344\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Arrive at the goal! - agent:  2  - stage:  6\n",
      "Arrive at the goal! - agent:  7  - stage:  6\n",
      "Arrive at the goal! - agent:  4  - stage:  6\n",
      "Max step number - agent:  3\n",
      "Rank-0: policy_step=268704, average_reward_env_0=143.79838857780717\n",
      "Arrive at the goal! - agent:  6  - stage:  6\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Rank-0: policy_step=268816, average_reward_env_0=166.9286768127589\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Arrive at the goal! - agent:  1  - stage:  6\n",
      "Arrive at the goal! - agent:  8  - stage:  6\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Rank-0: policy_step=269152, average_reward_env_0=145.67695243679375\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Rank-0: policy_step=269184, average_reward_env_0=190.64290049587595\n",
      "Max step number - agent:  1\n",
      "Rank-0: policy_step=269248, average_reward_env_0=190.3145101086077\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Rank-0: policy_step=269304, average_reward_env_0=188.09366753512572\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Rank-0: policy_step=269328, average_reward_env_0=145.82115597344477\n",
      "Arrive at the goal! - agent:  4  - stage:  6\n",
      "Arrive at the goal! - agent:  7  - stage:  6\n",
      "Arrive at the goal! - agent:  3  - stage:  6\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Rank-0: policy_step=269664, average_reward_env_0=128.05635986133038\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Rank-0: policy_step=269768, average_reward_env_0=200.07409452426845\n",
      "Collision detected between /World/Worker_2 and /World/Warehouse/Table_12\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Rank-0: policy_step=269912, average_reward_env_0=165.91254091894217\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Rank-0: policy_step=269944, average_reward_env_0=137.28681653505643\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Rank-0: policy_step=270072, average_reward_env_0=128.27437447079373\n",
      "Collision detected between /World/Worker_4 and /World/Warehouse/Warehouse/SM_FireExtinguisher_1925/SM_FireExtinguisher_02\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Rank-0: policy_step=270200, average_reward_env_0=183.01462410132427\n",
      "Arrive at the goal! - agent:  7  - stage:  6\n",
      "Max step number - agent:  7\n",
      "Rank-0: policy_step=270416, average_reward_env_0=194.76730681144042\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Rank-0: policy_step=270520, average_reward_env_0=202.27929517462997\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Rank-0: policy_step=270568, average_reward_env_0=169.1583564018751\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Rank-0: policy_step=270640, average_reward_env_0=131.0671548343134\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Rank-0: policy_step=270704, average_reward_env_0=160.57889518757352\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Rank-0: policy_step=270720, average_reward_env_0=201.39570892939273\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Arrive at the goal! - agent:  1  - stage:  6\n",
      "Arrive at the goal! - agent:  3  - stage:  6\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Rank-0: policy_step=271104, average_reward_env_0=180.50269870139877\n",
      "Arrive at the goal! - agent:  7  - stage:  6\n",
      "Arrive at the goal! - agent:  7  - stage:  6\n",
      "Arrive at the goal! - agent:  3  - stage:  6\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Rank-0: policy_step=271616, average_reward_env_0=137.81098442953905\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Rank-0: policy_step=271888, average_reward_env_0=181.0416751716601\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Arrive at the goal! - agent:  4  - stage:  6\n",
      "Arrive at the goal! - agent:  5  - stage:  6\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Rank-0: policy_step=272040, average_reward_env_0=182.63271615699662\n",
      "Arrive at the goal! - agent:  3  - stage:  6\n",
      "Arrive at the goal! - agent:  4  - stage:  6\n",
      "Arrive at the goal! - agent:  2  - stage:  6\n",
      "Arrive at the goal! - agent:  6  - stage:  6\n",
      "Arrive at the goal! - agent:  5  - stage:  6\n",
      "Collision detected between /World/Worker_4 and /World/Warehouse/Warehouse/SM_Rackshield_29/SM_Rackshield_02\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Rank-0: policy_step=272480, average_reward_env_0=183.40568446160884\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Rank-0: policy_step=272512, average_reward_env_0=138.3175490432074\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Rank-0: policy_step=272520, average_reward_env_0=179.43537939410646\n",
      "Max step number - agent:  8\n",
      "Rank-0: policy_step=272528, average_reward_env_0=143.11784587173779\n",
      "Arrive at the goal! - agent:  5  - stage:  6\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Rank-0: policy_step=272744, average_reward_env_0=142.08309687740234\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Rank-0: policy_step=272752, average_reward_env_0=183.87304641150254\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Arrive at the goal! - agent:  2  - stage:  6\n",
      "Arrive at the goal! - agent:  4  - stage:  6\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Rank-0: policy_step=272968, average_reward_env_0=172.85101990928217\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Rank-0: policy_step=273000, average_reward_env_0=142.34691127269997\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Arrive at the goal! - agent:  3  - stage:  6\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Rank-0: policy_step=273112, average_reward_env_0=142.92927319024093\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Rank-0: policy_step=273192, average_reward_env_0=223.21684679317303\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Rank-0: policy_step=273216, average_reward_env_0=134.33608883261567\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Arrive at the goal! - agent:  5  - stage:  6\n",
      "Max step number - agent:  6\n",
      "Rank-0: policy_step=273400, average_reward_env_0=201.3551757539872\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Rank-0: policy_step=273520, average_reward_env_0=164.97157688149036\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Rank-0: policy_step=273720, average_reward_env_0=133.62394671333416\n",
      "Max step number - agent:  5\n",
      "Rank-0: policy_step=273848, average_reward_env_0=142.6432625086229\n",
      "Arrive at the goal! - agent:  5  - stage:  6\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Arrive at the goal! - agent:  4  - stage:  6\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Arrive at the goal! - agent:  8  - stage:  6\n",
      "Arrive at the goal! - agent:  3  - stage:  6\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Rank-0: policy_step=274424, average_reward_env_0=164.28835819446675\n",
      "Arrive at the goal! - agent:  7  - stage:  6\n",
      "Arrive at the goal! - agent:  1  - stage:  6\n",
      "Collision detected between /World/Worker_3 and /World/Warehouse/Warehouse/GroundPlane/CollisionPlane\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Rank-0: policy_step=274864, average_reward_env_0=225.8442455802906\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Rank-0: policy_step=274896, average_reward_env_0=204.0437290171551\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Rank-0: policy_step=275000, average_reward_env_0=160.71695310748296\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Rank-0: policy_step=275040, average_reward_env_0=119.12913276652068\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Rank-0: policy_step=275256, average_reward_env_0=190.30393311477332\n",
      "Arrive at the goal! - agent:  6  - stage:  6\n",
      "Arrive at the goal! - agent:  1  - stage:  6\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Rank-0: policy_step=275576, average_reward_env_0=133.65004290957683\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Arrive at the goal! - agent:  4  - stage:  6\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Rank-0: policy_step=275752, average_reward_env_0=159.65996379464968\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Rank-0: policy_step=275768, average_reward_env_0=124.6210924177061\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Arrive at the goal! - agent:  7  - stage:  6\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Rank-0: policy_step=275824, average_reward_env_0=123.03921945780509\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Rank-0: policy_step=275848, average_reward_env_0=222.84317795146947\n",
      "Arrive at the goal! - agent:  3  - stage:  6\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Arrive at the goal! - agent:  6  - stage:  6\n",
      "Arrive at the goal! - agent:  1  - stage:  6\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Max step number - agent:  1\n",
      "Rank-0: policy_step=276176, average_reward_env_0=186.13189970675194\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Rank-0: policy_step=276232, average_reward_env_0=144.84061191189514\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Arrive at the goal! - agent:  8  - stage:  6\n",
      "Max step number - agent:  3\n",
      "Rank-0: policy_step=276312, average_reward_env_0=162.6540033986609\n",
      "Collision detected between /World/Worker_4 and /World/Warehouse/Warehouse/SM_FloorDecal_StripeFull_4m78/SM_FloorDecal_StripeFull_4m\n",
      "Arrive at the goal! - agent:  5  - stage:  6\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Rank-0: policy_step=276400, average_reward_env_0=124.74133861655251\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Collision detected between /World/Worker_4 and /World/Warehouse/Warehouse/SM_FloorDecal_StripeFull_4m106/SM_FloorDecal_StripeFull_4m\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Rank-0: policy_step=276664, average_reward_env_0=216.93398244131404\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Rank-0: policy_step=276744, average_reward_env_0=200.2404953830027\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Rank-0: policy_step=276856, average_reward_env_0=126.15084486623437\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Arrive at the goal! - agent:  8  - stage:  6\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Arrive at the goal! - agent:  2  - stage:  6\n",
      "Arrive at the goal! - agent:  8  - stage:  6\n",
      "Arrive at the goal! - agent:  5  - stage:  6\n",
      "Arrive at the goal! - agent:  1  - stage:  6\n",
      "Arrive at the goal! - agent:  3  - stage:  6\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Rank-0: policy_step=277528, average_reward_env_0=177.27482237919173\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Rank-0: policy_step=277648, average_reward_env_0=171.24391830689063\n",
      "Arrive at the goal! - agent:  1  - stage:  6\n",
      "Arrive at the goal! - agent:  4  - stage:  6\n",
      "Arrive at the goal! - agent:  5  - stage:  6\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Rank-0: policy_step=277976, average_reward_env_0=128.8627329761766\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Rank-0: policy_step=278048, average_reward_env_0=156.6339117376816\n",
      "Arrive at the goal! - agent:  7  - stage:  6\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Rank-0: policy_step=278248, average_reward_env_0=172.25340086999327\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Collision detected between /World/Worker_3 and /World/Worker_2\n",
      "Collision detected between /World/Worker_3 and /World/Worker_2\n",
      "Collision detected between /World/Worker_3 and /World/Worker_2\n",
      "Collision detected between /World/Worker_3 and /World/Worker_2\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Rank-0: policy_step=278328, average_reward_env_0=160.79869073737572\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Arrive at the goal! - agent:  1  - stage:  6\n",
      "Arrive at the goal! - agent:  5  - stage:  6\n",
      "Arrive at the goal! - agent:  7  - stage:  6\n",
      "Arrive at the goal! - agent:  6  - stage:  6\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Rank-0: policy_step=278800, average_reward_env_0=132.16852046125098\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Arrive at the goal! - agent:  4  - stage:  6\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Rank-0: policy_step=279112, average_reward_env_0=186.20877642106902\n",
      "Collision detected between /World/Worker_2 and /World/Warehouse/Rack_03\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Rank-0: policy_step=279208, average_reward_env_0=161.59010445597613\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Arrive at the goal! - agent:  8  - stage:  6\n",
      "Arrive at the goal! - agent:  6  - stage:  6\n",
      "Arrive at the goal! - agent:  4  - stage:  6\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Rank-0: policy_step=279632, average_reward_env_0=177.4384592203486\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Collision detected between /World/Worker_4 and /World/Worker_1\n",
      "Collision detected between /World/Worker_4 and /World/Worker_1\n",
      "Collision detected between /World/Worker_4 and /World/Worker_1\n",
      "Collision detected between /World/Worker_4 and /World/Worker_1\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Rank-0: policy_step=279856, average_reward_env_0=164.44236566131025\n",
      "Arrive at the goal! - agent:  6  - stage:  6\n",
      "Max step number - agent:  4\n",
      "Rank-0: policy_step=279952, average_reward_env_0=202.10843938520287\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Arrive at the goal! - agent:  5  - stage:  6\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Max step number - agent:  5\n",
      "Rank-0: policy_step=280064, average_reward_env_0=162.78106191667825\n",
      "Rank-0: policy_step=280064, average_reward_env_1=141.696350758239\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Arrive at the goal! - agent:  7  - stage:  6\n",
      "Arrive at the goal! - agent:  5  - stage:  6\n",
      "Arrive at the goal! - agent:  5  - stage:  6\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Rank-0: policy_step=280360, average_reward_env_0=193.4543519974272\n",
      "Collision detected between /World/Worker_3 and /World/Worker_1\n",
      "Collision detected between /World/Worker_3 and /World/Worker_1\n",
      "Arrive at the goal! - agent:  8  - stage:  6\n",
      "Arrive at the goal! - agent:  6  - stage:  6\n",
      "Max step number - agent:  6\n",
      "Rank-0: policy_step=280736, average_reward_env_0=204.23769654517687\n",
      "Max step number - agent:  6\n",
      "Rank-0: policy_step=280744, average_reward_env_0=232.23972957605042\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Rank-0: policy_step=280784, average_reward_env_0=170.44924364355938\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Arrive at the goal! - agent:  6  - stage:  6\n",
      "Collision detected between /World/Worker_2 and /World/Warehouse/Warehouse/SM_Rackshield_3841/SM_Rackshield_02\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Rank-0: policy_step=281192, average_reward_env_0=164.07453755032984\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Rank-0: policy_step=281200, average_reward_env_0=172.0081241899203\n",
      "Arrive at the goal! - agent:  2  - stage:  6\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Rank-0: policy_step=281328, average_reward_env_0=196.7064891514411\n",
      "Arrive at the goal! - agent:  7  - stage:  6\n",
      "Arrive at the goal! - agent:  5  - stage:  6\n",
      "Arrive at the goal! - agent:  6  - stage:  6\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Rank-0: policy_step=281904, average_reward_env_0=239.1716267825751\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Arrive at the goal! - agent:  8  - stage:  6\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Max step number - agent:  8\n",
      "Rank-0: policy_step=282000, average_reward_env_0=155.59351294388102\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Rank-0: policy_step=282008, average_reward_env_0=230.14338259159186\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Arrive at the goal! - agent:  4  - stage:  6\n",
      "Collision detected between /World/Worker_2 and /World/Warehouse/Warehouse/SM_RackFrame_3840/SM_RackFrame_03/SM_RackFrame_03/Section1\n",
      "Collision detected between /World/Worker_2 and /World/Warehouse/Warehouse/SM_Rackshield_3841/SM_Rackshield_02\n",
      "Collision detected between /World/Worker_2 and /World/Worker_1\n",
      "Collision detected between /World/Worker_2 and /World/Worker_1\n",
      "Collision detected between /World/Worker_2 and /World/Worker_1\n",
      "Collision detected between /World/Worker_2 and /World/Worker_1\n",
      "Arrive at the goal! - agent:  3  - stage:  6\n",
      "Arrive at the goal! - agent:  8  - stage:  6\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Rank-0: policy_step=282464, average_reward_env_0=173.846590774338\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Arrive at the goal! - agent:  8  - stage:  6\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Rank-0: policy_step=282616, average_reward_env_0=150.34431564785254\n",
      "Arrive at the goal! - agent:  5  - stage:  6\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Rank-0: policy_step=282688, average_reward_env_0=204.30391618638106\n",
      "Rank-0: policy_step=282688, average_reward_env_1=166.28928616080296\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Arrive at the goal! - agent:  7  - stage:  6\n",
      "Arrive at the goal! - agent:  1  - stage:  6\n",
      "Max step number - agent:  7\n",
      "Rank-0: policy_step=282840, average_reward_env_0=203.79595112724712\n",
      "Arrive at the goal! - agent:  3  - stage:  6\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collision detected between /World/Worker_3 and /World/Warehouse/Rack_02\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collision detected between /World/Worker_3 and /World/Warehouse/Warehouse/SM_WallA_6M18/SM_WallA_6M/SM_WallA_6M/Section0\n",
      "Collision detected between /World/Worker_3 and /World/Warehouse/Warehouse/SM_WallA_6M18/SM_WallA_6M/SM_WallA_6M/Section1\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Rank-0: policy_step=283096, average_reward_env_0=217.15149721670917\n",
      "Collision detected between /World/Worker_3 and /World/Warehouse/Rack_04\n",
      "Arrive at the goal! - agent:  8  - stage:  6\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Rank-0: policy_step=283424, average_reward_env_0=180.22443571898665\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Rank-0: policy_step=283856, average_reward_env_0=169.96678497706714\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Rank-0: policy_step=283992, average_reward_env_0=161.06902577738865\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Arrive at the goal! - agent:  7  - stage:  6\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Rank-0: policy_step=284032, average_reward_env_0=151.2609094549799\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Arrive at the goal! - agent:  4  - stage:  6\n",
      "Arrive at the goal! - agent:  1  - stage:  6\n",
      "Arrive at the goal! - agent:  8  - stage:  6\n",
      "Arrive at the goal! - agent:  3  - stage:  6\n",
      "Arrive at the goal! - agent:  6  - stage:  6\n",
      "Arrive at the goal! - agent:  8  - stage:  6\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Rank-0: policy_step=284280, average_reward_env_0=137.5998098008566\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Rank-0: policy_step=284392, average_reward_env_0=120.80075839427383\n",
      "Max step number - agent:  1\n",
      "Rank-0: policy_step=284400, average_reward_env_0=190.8930877804512\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Rank-0: policy_step=284552, average_reward_env_0=114.15590817219243\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Rank-0: policy_step=284608, average_reward_env_0=162.35163229045972\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Rank-0: policy_step=284784, average_reward_env_0=212.37303648314392\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Arrive at the goal! - agent:  4  - stage:  6\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Rank-0: policy_step=284912, average_reward_env_0=215.2785426546469\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Arrive at the goal! - agent:  6  - stage:  6\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Arrive at the goal! - agent:  8  - stage:  6\n",
      "Rank-0: policy_step=285152, average_reward_env_0=188.84625094455924\n",
      "Max step number - agent:  8\n",
      "Rank-0: policy_step=285200, average_reward_env_0=192.47859956524726\n",
      "Arrive at the goal! - agent:  7  - stage:  6\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Rank-0: policy_step=285512, average_reward_env_0=208.1490439299622\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Rank-0: policy_step=285520, average_reward_env_0=217.47424684904487\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Arrive at the goal! - agent:  1  - stage:  6\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Rank-0: policy_step=285784, average_reward_env_0=215.54768325436402\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Arrive at the goal! - agent:  5  - stage:  6\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Max step number - agent:  7\n",
      "Rank-0: policy_step=286040, average_reward_env_0=207.18070342207892\n",
      "Collision detected between /World/Worker_3 and /World/Warehouse/Table_03\n",
      "Arrive at the goal! - agent:  3  - stage:  6\n",
      "Arrive at the goal! - agent:  8  - stage:  6\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Rank-0: policy_step=286240, average_reward_env_0=185.2847439546927\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Rank-0: policy_step=286320, average_reward_env_0=108.3096778893751\n",
      "Rank-0: policy_step=286320, average_reward_env_1=165.6951744990218\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Arrive at the goal! - agent:  3  - stage:  6\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Rank-0: policy_step=286464, average_reward_env_0=185.66819125817895\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Rank-0: policy_step=286584, average_reward_env_0=186.1735964791359\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Collision detected between /World/Worker_4 and /World/Warehouse/Warehouse/GroundPlane/CollisionPlane\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Rank-0: policy_step=286760, average_reward_env_0=145.39143032173573\n",
      "Collision detected between /World/Worker_3 and /World/Warehouse/Table_08\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Rank-0: policy_step=286776, average_reward_env_0=199.95611913208376\n",
      "Arrive at the goal! - agent:  6  - stage:  6\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Rank-0: policy_step=286800, average_reward_env_0=168.9687237196106\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Arrive at the goal! - agent:  3  - stage:  6\n",
      "Arrive at the goal! - agent:  4  - stage:  6\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Rank-0: policy_step=286976, average_reward_env_0=217.97803468512626\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Rank-0: policy_step=287304, average_reward_env_0=133.37613128624136\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Arrive at the goal! - agent:  4  - stage:  6\n",
      "Rank-0: policy_step=287328, average_reward_env_0=167.96731727122966\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Rank-0: policy_step=287336, average_reward_env_0=218.97467570525131\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Rank-0: policy_step=287568, average_reward_env_0=202.726119414868\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Arrive at the goal! - agent:  6  - stage:  6\n",
      "Arrive at the goal! - agent:  7  - stage:  6\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Rank-0: policy_step=287736, average_reward_env_0=192.12561159756413\n",
      "Rank-0: policy_step=287736, average_reward_env_1=214.99886317233555\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Arrive at the goal! - agent:  6  - stage:  6\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Rank-0: policy_step=287968, average_reward_env_0=207.88953333444144\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Rank-0: policy_step=288152, average_reward_env_0=224.14823773348309\n",
      "Arrive at the goal! - agent:  5  - stage:  6\n",
      "Arrive at the goal! - agent:  4  - stage:  6\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Rank-0: policy_step=288232, average_reward_env_1=206.25837561169968\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Rank-0: policy_step=288312, average_reward_env_0=139.6930871248331\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Rank-0: policy_step=288344, average_reward_env_0=214.92707036958782\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Arrive at the goal! - agent:  2  - stage:  6\n",
      "Arrive at the goal! - agent:  1  - stage:  6\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Rank-0: policy_step=288864, average_reward_env_0=210.7652995914294\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Rank-0: policy_step=288928, average_reward_env_0=206.83520692202936\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Arrive at the goal! - agent:  5  - stage:  6\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collision detected between /World/Worker_3 and /World/Warehouse/Table_12\n",
      "Arrive at the goal! - agent:  6  - stage:  6\n",
      "Arrive at the goal! - agent:  1  - stage:  6\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Rank-0: policy_step=289280, average_reward_env_0=101.14452765259419\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Rank-0: policy_step=289392, average_reward_env_0=189.05970979090253\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Arrive at the goal! - agent:  4  - stage:  6\n",
      "Collision detected between /World/Worker_2 and /World/Warehouse/Warehouse/GroundPlane/CollisionPlane\n",
      "Collision detected between /World/Worker_2 and /World/Warehouse/Warehouse/SM_Rackshield_4492/SM_Rackshield_02\n",
      "Collision detected between /World/Worker_2 and /World/Warehouse/Warehouse/Box_32606/SM_CardBoxB_01/SM_CardBoxB_01\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Rank-0: policy_step=289648, average_reward_env_0=208.13561947505968\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Collision detected between /World/Worker_3 and /World/Worker_2\n",
      "Collision detected between /World/Worker_3 and /World/Worker_2\n",
      "Collision detected between /World/Worker_3 and /World/Worker_2\n",
      "Collision detected between /World/Worker_3 and /World/Worker_2\n",
      "Collision detected between /World/Worker_3 and /World/Warehouse/Warehouse/SM_FloorDecal_StripeFull_4m67_1162/SM_FloorDecal_StripeFull_4m\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Rank-0: policy_step=289960, average_reward_env_0=207.06524820455476\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Arrive at the goal! - agent:  5  - stage:  6\n",
      "Arrive at the goal! - agent:  5  - stage:  6\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Rank-0: policy_step=290272, average_reward_env_0=206.85402610038645\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Rank-0: policy_step=290368, average_reward_env_0=204.6785379127701\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Rank-0: policy_step=290432, average_reward_env_0=210.9692130498155\n",
      "Arrive at the goal! - agent:  5  - stage:  6\n",
      "Max step number - agent:  1\n",
      "Rank-0: policy_step=290536, average_reward_env_0=177.38091932661052\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Rank-0: policy_step=290656, average_reward_env_0=174.81852878573164\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Arrive at the goal! - agent:  8  - stage:  6\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Rank-0: policy_step=290688, average_reward_env_0=83.02783190847283\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Collision detected between /World/Worker_3 and /World/Warehouse/Table_09\n",
      "Arrive at the goal! - agent:  6  - stage:  6\n",
      "Arrive at the goal! - agent:  3  - stage:  6\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Rank-0: policy_step=291064, average_reward_env_0=172.3199191097751\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Rank-0: policy_step=291088, average_reward_env_0=200.03710702461314\n",
      "Collision detected between /World/Worker_3 and /World/Worker_2\n",
      "Collision detected between /World/Worker_3 and /World/Worker_2\n",
      "Collision detected between /World/Worker_3 and /World/Worker_2\n",
      "Collision detected between /World/Worker_3 and /World/Worker_2\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Rank-0: policy_step=291272, average_reward_env_0=176.93637880899144\n",
      "Arrive at the goal! - agent:  4  - stage:  6\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Rank-0: policy_step=291440, average_reward_env_0=166.35841897632602\n",
      "Arrive at the goal! - agent:  6  - stage:  6\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Rank-0: policy_step=291632, average_reward_env_0=191.55803001295723\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Arrive at the goal! - agent:  3  - stage:  6\n",
      "Arrive at the goal! - agent:  8  - stage:  6\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Rank-0: policy_step=292088, average_reward_env_0=176.89707164186024\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Rank-0: policy_step=292224, average_reward_env_0=89.393287711458\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Rank-0: policy_step=292248, average_reward_env_0=166.29997795199904\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Rank-0: policy_step=292264, average_reward_env_0=89.49974436518889\n",
      "Arrive at the goal! - agent:  1  - stage:  6\n",
      "Arrive at the goal! - agent:  6  - stage:  6\n",
      "Collision detected between /World/Worker_4 and /World/Worker_1\n",
      "Collision detected between /World/Worker_4 and /World/Worker_1\n",
      "Collision detected between /World/Worker_4 and /World/Worker_1\n",
      "Collision detected between /World/Worker_4 and /World/Worker_1\n",
      "Arrive at the goal! - agent:  4  - stage:  6\n",
      "Arrive at the goal! - agent:  5  - stage:  6\n",
      "Collision detected between /World/Worker_2 and /World/Warehouse/Warehouse/SM_Rackshield_3841/SM_Rackshield_02\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Rank-0: policy_step=292952, average_reward_env_0=184.55415079931134\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Rank-0: policy_step=293128, average_reward_env_0=191.8617546352233\n",
      "Max step number - agent:  8\n",
      "Rank-0: policy_step=293168, average_reward_env_0=198.42012777337132\n",
      "Arrive at the goal! - agent:  2  - stage:  6\n",
      "Arrive at the goal! - agent:  3  - stage:  6\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Rank-0: policy_step=293360, average_reward_env_0=178.26640136226825\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Arrive at the goal! - agent:  6  - stage:  6\n",
      "Arrive at the goal! - agent:  7  - stage:  6\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Rank-0: policy_step=293840, average_reward_env_0=172.33628706866836\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Arrive at the goal! - agent:  7  - stage:  6\n",
      "Arrive at the goal! - agent:  1  - stage:  6\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Rank-0: policy_step=294160, average_reward_env_0=160.69391874050413\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Rank-0: policy_step=294224, average_reward_env_0=205.45898080816528\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Rank-0: policy_step=294240, average_reward_env_0=169.72511968822295\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Arrive at the goal! - agent:  8  - stage:  6\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Rank-0: policy_step=294368, average_reward_env_0=175.79181541331994\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Rank-0: policy_step=294464, average_reward_env_0=91.67880894050624\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Arrive at the goal! - agent:  3  - stage:  6\n",
      "Arrive at the goal! - agent:  4  - stage:  6\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Rank-0: policy_step=294856, average_reward_env_0=200.04838300893357\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Arrive at the goal! - agent:  7  - stage:  6\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Rank-0: policy_step=295136, average_reward_env_0=83.32333617826745\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Rank-0: policy_step=295192, average_reward_env_0=156.51604469266786\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Arrive at the goal! - agent:  3  - stage:  6\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Rank-0: policy_step=295640, average_reward_env_0=179.2895593858558\n",
      "Arrive at the goal! - agent:  4  - stage:  6\n",
      "Arrive at the goal! - agent:  1  - stage:  6\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Rank-0: policy_step=295920, average_reward_env_0=161.98272761492672\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Arrive at the goal! - agent:  8  - stage:  6\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Rank-0: policy_step=296136, average_reward_env_0=173.80661076080887\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Arrive at the goal! - agent:  2  - stage:  6\n",
      "Arrive at the goal! - agent:  7  - stage:  6\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Rank-0: policy_step=296280, average_reward_env_0=206.7575786133003\n",
      "Collision detected between /World/Worker_4 and /World/Worker_3\n",
      "Collision detected between /World/Worker_4 and /World/Worker_3\n",
      "Collision detected between /World/Worker_4 and /World/Worker_3\n",
      "Collision detected between /World/Worker_4 and /World/Worker_3\n",
      "Max step number - agent:  7\n",
      "Rank-0: policy_step=296328, average_reward_env_0=222.52740866117907\n",
      "Arrive at the goal! - agent:  6  - stage:  6\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Rank-0: policy_step=296424, average_reward_env_0=164.69865849981406\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Rank-0: policy_step=296544, average_reward_env_0=162.0450761845573\n",
      "Arrive at the goal! - agent:  7  - stage:  6\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Rank-0: policy_step=296760, average_reward_env_0=186.5596789959986\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Rank-0: policy_step=296880, average_reward_env_0=184.48592322930173\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Rank-0: policy_step=296912, average_reward_env_0=224.65204132259984\n",
      "Arrive at the goal! - agent:  1  - stage:  6\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Rank-0: policy_step=296952, average_reward_env_1=85.3891748685767\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Max step number - agent:  1\n",
      "Rank-0: policy_step=297072, average_reward_env_0=192.30247407152274\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Rank-0: policy_step=297272, average_reward_env_0=185.80208211495483\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Rank-0: policy_step=297328, average_reward_env_0=203.88771584300946\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Arrive at the goal! - agent:  2  - stage:  6\n",
      "Arrive at the goal! - agent:  5  - stage:  6\n",
      "Arrive at the goal! - agent:  3  - stage:  6\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Rank-0: policy_step=297536, average_reward_env_0=188.67607376754694\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Rank-0: policy_step=297552, average_reward_env_0=85.17982157678307\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Max step number - agent:  3\n",
      "Rank-0: policy_step=297568, average_reward_env_0=190.20723903365698\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Rank-0: policy_step=297576, average_reward_env_0=136.79779272942937\n",
      "Arrive at the goal! - agent:  4  - stage:  6\n",
      "Arrive at the goal! - agent:  3  - stage:  6\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Rank-0: policy_step=297800, average_reward_env_0=228.2538092241084\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Arrive at the goal! - agent:  1  - stage:  6\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Rank-0: policy_step=298032, average_reward_env_0=82.00261110122243\n",
      "Collision detected between /World/Worker_2 and /World/Warehouse/Table_08\n",
      "Collision detected between /World/Worker_2 and /World/Warehouse/Forklift_B01_PR_V_NVD_01\n",
      "Collision detected between /World/Worker_2 and /World/Warehouse/Forklift_B01_PR_V_NVD_01\n",
      "Arrive at the goal! - agent:  2  - stage:  6\n",
      "Arrive at the goal! - agent:  5  - stage:  6\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Rank-0: policy_step=298824, average_reward_env_0=127.05538932725194\n",
      "Arrive at the goal! - agent:  1  - stage:  6\n",
      "Arrive at the goal! - agent:  3  - stage:  6\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Rank-0: policy_step=298968, average_reward_env_0=166.19606840490619\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Arrive at the goal! - agent:  6  - stage:  6\n",
      "Arrive at the goal! - agent:  8  - stage:  6\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Rank-0: policy_step=299216, average_reward_env_0=231.18799696955367\n",
      "Arrive at the goal! - agent:  3  - stage:  6\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Rank-0: policy_step=299480, average_reward_env_0=203.73705884256452\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Collision detected between /World/Worker_2 and /World/Warehouse/Warehouse/SM_Rackshield_2984/SM_Rackshield_02\n",
      "Arrive at the goal! - agent:  6  - stage:  6\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Rank-0: policy_step=299680, average_reward_env_0=196.94511538653322\n",
      "Arrive at the goal! - agent:  2  - stage:  6\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Rank-0: policy_step=299760, average_reward_env_0=190.56054824502394\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Rank-0: policy_step=299768, average_reward_env_0=162.86248780517454\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Rank-0: policy_step=299840, average_reward_env_0=174.20003419193662\n",
      "Arrive at the goal! - agent:  3  - stage:  6\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Rank-0: policy_step=299880, average_reward_env_1=187.45800477637596\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Collision detected between /World/Worker_4 and /World/Worker_3\n",
      "Collision detected between /World/Worker_4 and /World/Worker_3\n",
      "Collision detected between /World/Worker_4 and /World/Worker_3\n",
      "Collision detected between /World/Worker_4 and /World/Worker_3\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Rank-0: policy_step=299952, average_reward_env_0=177.802890742428\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collision detected between /World/Worker_4 and /World/Warehouse/Warehouse/SM_PaletteA_3055/SM_PaletteA_01\n",
      "Arrive at the goal! - agent:  2  - stage:  6\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Rank-0: policy_step=300096, average_reward_env_0=195.14908017202623\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Arrive at the goal! - agent:  2  - stage:  6\n",
      "Arrive at the goal! - agent:  2  - stage:  6\n",
      "Max step number - agent:  1\n",
      "Rank-0: policy_step=300272, average_reward_env_0=210.4245439827804\n",
      "Arrive at the goal! - agent:  6  - stage:  6\n",
      "Arrive at the goal! - agent:  5  - stage:  6\n",
      "Arrive at the goal! - agent:  8  - stage:  6\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Rank-0: policy_step=300616, average_reward_env_0=101.78169991141398\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Rank-0: policy_step=300696, average_reward_env_0=130.54390196592593\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Rank-0: policy_step=300720, average_reward_env_0=181.35722809450598\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Arrive at the goal! - agent:  4  - stage:  6\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Rank-0: policy_step=300856, average_reward_env_0=159.73971638294665\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Rank-0: policy_step=300936, average_reward_env_0=185.95921755434108\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Arrive at the goal! - agent:  7  - stage:  6\n",
      "Arrive at the goal! - agent:  1  - stage:  6\n",
      "Arrive at the goal! - agent:  5  - stage:  6\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Rank-0: policy_step=301200, average_reward_env_1=197.85400264877825\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Arrive at the goal! - agent:  2  - stage:  6\n",
      "Arrive at the goal! - agent:  4  - stage:  6\n",
      "Arrive at the goal! - agent:  6  - stage:  6\n",
      "Collision detected between /World/Worker_2 and /World/Warehouse/Warehouse/SM_Rackshield_3841/SM_Rackshield_02\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Rank-0: policy_step=301560, average_reward_env_0=188.51328073351695\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Rank-0: policy_step=301584, average_reward_env_0=191.65753989902453\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Rank-0: policy_step=301648, average_reward_env_0=187.76270297240495\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Rank-0: policy_step=301736, average_reward_env_0=106.5151464518417\n",
      "Collision detected between /World/Worker_1 and /World/Warehouse/Table_03\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Rank-0: policy_step=301928, average_reward_env_0=169.64415424017773\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Arrive at the goal! - agent:  1  - stage:  6\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Rank-0: policy_step=302320, average_reward_env_0=198.39316068314528\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Arrive at the goal! - agent:  7  - stage:  6\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Rank-0: policy_step=302384, average_reward_env_0=161.72139326197862\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Rank-0: policy_step=302608, average_reward_env_0=144.92287047829\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Rank-0: policy_step=302720, average_reward_env_0=166.91190223909507\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Rank-0: policy_step=302736, average_reward_env_0=190.1646538979711\n",
      "Arrive at the goal! - agent:  3  - stage:  6\n",
      "Arrive at the goal! - agent:  5  - stage:  6\n",
      "Arrive at the goal! - agent:  7  - stage:  6\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Rank-0: policy_step=302952, average_reward_env_0=123.98725877556144\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Rank-0: policy_step=303208, average_reward_env_0=214.033116405608\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Rank-0: policy_step=303296, average_reward_env_0=156.38243548655376\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Collision detected between /World/Worker_3 and /World/Warehouse/Table_08\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Rank-0: policy_step=303488, average_reward_env_0=108.2023089175036\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Rank-0: policy_step=303552, average_reward_env_0=114.61074868059062\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Arrive at the goal! - agent:  5  - stage:  6\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Rank-0: policy_step=303712, average_reward_env_0=157.30882630868348\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Rank-0: policy_step=303752, average_reward_env_0=106.66344557956602\n",
      "Arrive at the goal! - agent:  6  - stage:  6\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Rank-0: policy_step=303824, average_reward_env_0=148.50582692174726\n",
      "Arrive at the goal! - agent:  3  - stage:  6\n",
      "Collision detected between /World/Worker_4 and /World/Worker_3\n",
      "Collision detected between /World/Worker_4 and /World/Worker_3\n",
      "Collision detected between /World/Worker_4 and /World/Worker_3\n",
      "Collision detected between /World/Worker_4 and /World/Worker_3\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Max step number - agent:  5\n",
      "Rank-0: policy_step=304056, average_reward_env_0=186.4089524289614\n",
      "Arrive at the goal! - agent:  2  - stage:  6\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Rank-0: policy_step=304280, average_reward_env_0=95.90006152566875\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Rank-0: policy_step=304328, average_reward_env_0=177.26715612422845\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Arrive at the goal! - agent:  1  - stage:  6\n",
      "Arrive at the goal! - agent:  6  - stage:  6\n",
      "Arrive at the goal! - agent:  1  - stage:  6\n",
      "Max step number - agent:  3\n",
      "Rank-0: policy_step=304760, average_reward_env_0=190.58220757533607\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Arrive at the goal! - agent:  4  - stage:  6\n",
      "Arrive at the goal! - agent:  5  - stage:  6\n",
      "Arrive at the goal! - agent:  8  - stage:  6\n",
      "Arrive at the goal! - agent:  6  - stage:  6\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Rank-0: policy_step=305456, average_reward_env_0=96.2713702572339\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Rank-0: policy_step=305712, average_reward_env_0=142.95374549699764\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Arrive at the goal! - agent:  4  - stage:  6\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Max step number - agent:  6\n",
      "Rank-0: policy_step=305928, average_reward_env_0=191.90919926444352\n",
      "Arrive at the goal! - agent:  1  - stage:  6\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Rank-0: policy_step=306016, average_reward_env_0=177.22419341868678\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Rank-0: policy_step=306096, average_reward_env_0=189.4629478008152\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Arrive at the goal! - agent:  8  - stage:  6\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Arrive at the goal! - agent:  3  - stage:  6\n",
      "Rank-0: policy_step=306216, average_reward_env_0=93.63594498970983\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Rank-0: policy_step=306360, average_reward_env_0=198.2294213076928\n",
      "Max step number - agent:  1\n",
      "Rank-0: policy_step=306416, average_reward_env_0=210.52184938146777\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Rank-0: policy_step=306616, average_reward_env_0=196.41109527259388\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Rank-0: policy_step=306680, average_reward_env_0=209.6118790019229\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Collision detected between /World/Worker_2 and /World/Warehouse/Warehouse/GroundPlane/CollisionPlane\n",
      "Arrive at the goal! - agent:  3  - stage:  6\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Rank-0: policy_step=306808, average_reward_env_0=166.95580994734198\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Arrive at the goal! - agent:  4  - stage:  6\n",
      "Max step number - agent:  4\n",
      "Rank-0: policy_step=306952, average_reward_env_0=129.7173192341436\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Max step number - agent:  8\n",
      "Rank-0: policy_step=307024, average_reward_env_0=169.1150000364238\n",
      "Arrive at the goal! - agent:  3  - stage:  6\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Rank-0: policy_step=307208, average_reward_env_0=151.18257380943598\n",
      "Arrive at the goal! - agent:  7  - stage:  6\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Rank-0: policy_step=307544, average_reward_env_0=191.53254874391183\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Rank-0: policy_step=307584, average_reward_env_0=169.44876636414975\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Rank-0: policy_step=307672, average_reward_env_0=180.9755615615082\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Arrive at the goal! - agent:  3  - stage:  6\n",
      "Arrive at the goal! - agent:  2  - stage:  6\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collision detected between /World/Worker_3 and /World/Worker_1\n",
      "Collision detected between /World/Worker_3 and /World/Worker_1\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Rank-0: policy_step=308016, average_reward_env_0=123.19405639888679\n",
      "Arrive at the goal! - agent:  5  - stage:  6\n",
      "Arrive at the goal! - agent:  4  - stage:  6\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Rank-0: policy_step=308248, average_reward_env_0=180.3177569179379\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Rank-0: policy_step=308256, average_reward_env_0=209.12921021635123\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Arrive at the goal! - agent:  8  - stage:  6\n",
      "Arrive at the goal! - agent:  1  - stage:  6\n",
      "Arrive at the goal! - agent:  2  - stage:  6\n",
      "Arrive at the goal! - agent:  7  - stage:  6\n",
      "Arrive at the goal! - agent:  3  - stage:  6\n",
      "Arrive at the goal! - agent:  3  - stage:  6\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Rank-0: policy_step=308840, average_reward_env_0=116.55156077590827\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Arrive at the goal! - agent:  1  - stage:  6\n",
      "Max step number - agent:  7\n",
      "Rank-0: policy_step=308936, average_reward_env_0=164.53299890608315\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Arrive at the goal! - agent:  5  - stage:  6\n",
      "Arrive at the goal! - agent:  4  - stage:  6\n",
      "Arrive at the goal! - agent:  8  - stage:  6\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Rank-0: policy_step=309400, average_reward_env_0=207.4281546038275\n",
      "Rank-0: policy_step=309400, average_reward_env_1=118.89600390429308\n",
      "Arrive at the goal! - agent:  3  - stage:  6\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Max step number - agent:  2\n",
      "Rank-0: policy_step=309424, average_reward_env_0=116.96329492597879\n",
      "Collision detected between /World/Worker_4 and /World/Worker_2\n",
      "Collision detected between /World/Worker_4 and /World/Worker_2\n",
      "Collision detected between /World/Worker_3 and /World/Warehouse/Table_03\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Rank-0: policy_step=309496, average_reward_env_0=153.50187538876276\n",
      "Collision detected between /World/Worker_3 and /World/Warehouse/Rack_02\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Rank-0: policy_step=309984, average_reward_env_0=190.4356536259324\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Max step number - agent:  5\n",
      "Rank-0: policy_step=310016, average_reward_env_0=180.92420008507196\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Rank-0: policy_step=310032, average_reward_env_0=149.17917408434266\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Arrive at the goal! - agent:  2  - stage:  6\n",
      "Arrive at the goal! - agent:  6  - stage:  6\n",
      "Arrive at the goal! - agent:  8  - stage:  6\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Rank-0: policy_step=310368, average_reward_env_0=181.665074887792\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Arrive at the goal! - agent:  3  - stage:  6\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Rank-0: policy_step=310584, average_reward_env_0=196.20508121973134\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Rank-0: policy_step=310616, average_reward_env_0=125.02137006448949\n",
      "Arrive at the goal! - agent:  1  - stage:  6\n",
      "Arrive at the goal! - agent:  5  - stage:  6\n",
      "Arrive at the goal! - agent:  7  - stage:  6\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Rank-0: policy_step=310720, average_reward_env_0=195.50194671984573\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Arrive at the goal! - agent:  3  - stage:  6\n",
      "Arrive at the goal! - agent:  5  - stage:  6\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Rank-0: policy_step=311072, average_reward_env_0=116.49454298001692\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Rank-0: policy_step=311120, average_reward_env_0=125.09465035629194\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Rank-0: policy_step=311144, average_reward_env_0=111.13304559101563\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Arrive at the goal! - agent:  4  - stage:  6\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Rank-0: policy_step=311680, average_reward_env_0=132.44013458684697\n",
      "Arrive at the goal! - agent:  3  - stage:  6\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Rank-0: policy_step=311800, average_reward_env_0=111.11819570769902\n",
      "Arrive at the goal! - agent:  1  - stage:  6\n",
      "Arrive at the goal! - agent:  8  - stage:  6\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Arrive at the goal! - agent:  7  - stage:  6\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Arrive at the goal! - agent:  5  - stage:  6\n",
      "Arrive at the goal! - agent:  7  - stage:  6\n",
      "Arrive at the goal! - agent:  5  - stage:  6\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Rank-0: policy_step=312288, average_reward_env_0=98.68158150614198\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Rank-0: policy_step=312336, average_reward_env_0=224.11811233107798\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Arrive at the goal! - agent:  4  - stage:  6\n",
      "Arrive at the goal! - agent:  7  - stage:  6\n",
      "Max step number - agent:  7\n",
      "Rank-0: policy_step=312696, average_reward_env_0=182.85200782221168\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Rank-0: policy_step=312704, average_reward_env_0=206.5786090482341\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Arrive at the goal! - agent:  1  - stage:  6\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Rank-0: policy_step=312904, average_reward_env_0=220.0890422296442\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Rank-0: policy_step=312920, average_reward_env_0=197.93907797665267\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Rank-0: policy_step=312936, average_reward_env_0=136.07274643943208\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Rank-0: policy_step=312992, average_reward_env_0=196.5013433658777\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Rank-0: policy_step=313000, average_reward_env_0=119.98076168942944\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Rank-0: policy_step=313064, average_reward_env_0=98.49803951029577\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Rank-0: policy_step=313072, average_reward_env_0=172.1255559921883\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Rank-0: policy_step=313232, average_reward_env_0=97.403885824904\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Arrive at the goal! - agent:  3  - stage:  6\n",
      "Arrive at the goal! - agent:  5  - stage:  6\n",
      "Arrive at the goal! - agent:  4  - stage:  6\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Rank-0: policy_step=313448, average_reward_env_0=158.76179913033863\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Arrive at the goal! - agent:  1  - stage:  6\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Rank-0: policy_step=313504, average_reward_env_0=193.54356916186126\n",
      "Arrive at the goal! - agent:  3  - stage:  6\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Max step number - agent:  5\n",
      "Rank-0: policy_step=313576, average_reward_env_0=208.32324140344463\n",
      "Arrive at the goal! - agent:  8  - stage:  6\n",
      "Arrive at the goal! - agent:  2  - stage:  6\n",
      "Arrive at the goal! - agent:  4  - stage:  6\n",
      "Max step number - agent:  1\n",
      "Rank-0: policy_step=313928, average_reward_env_0=220.67196457870307\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Arrive at the goal! - agent:  8  - stage:  6\n",
      "Arrive at the goal! - agent:  6  - stage:  6\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Arrive at the goal! - agent:  7  - stage:  6\n",
      "Arrive at the goal! - agent:  5  - stage:  6\n",
      "Arrive at the goal! - agent:  3  - stage:  6\n",
      "Collision detected between /World/Worker_2 and /World/Warehouse/Table_09\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Rank-0: policy_step=314176, average_reward_env_0=105.37013816469909\n",
      "Arrive at the goal! - agent:  8  - stage:  6\n",
      "Collision detected between /World/Worker_1 and /World/Warehouse/Warehouse/GroundPlane/CollisionPlane\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Rank-0: policy_step=314536, average_reward_env_0=108.33940799339301\n",
      "Arrive at the goal! - agent:  4  - stage:  6\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Rank-0: policy_step=314560, average_reward_env_0=197.68755065799166\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Rank-0: policy_step=314680, average_reward_env_0=104.35433102867748\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Arrive at the goal! - agent:  7  - stage:  6\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Arrive at the goal! - agent:  8  - stage:  6\n",
      "Arrive at the goal! - agent:  2  - stage:  6\n",
      "Arrive at the goal! - agent:  1  - stage:  6\n",
      "Collision detected between /World/Worker_4 and /World/Warehouse/Warehouse/GroundPlane/CollisionPlane\n",
      "Arrive at the goal! - agent:  5  - stage:  6\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Rank-0: policy_step=315416, average_reward_env_0=117.644558253245\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Rank-0: policy_step=315560, average_reward_env_0=182.20401942406227\n",
      "Rank-0: policy_step=315560, average_reward_env_1=117.67587015312589\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Arrive at the goal! - agent:  3  - stage:  6\n",
      "Arrive at the goal! - agent:  6  - stage:  6\n",
      "Arrive at the goal! - agent:  8  - stage:  6\n",
      "Arrive at the goal! - agent:  1  - stage:  6\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Rank-0: policy_step=315720, average_reward_env_0=226.49750763987865\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Arrive at the goal! - agent:  4  - stage:  6\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Rank-0: policy_step=315904, average_reward_env_0=205.74015371811208\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Rank-0: policy_step=315928, average_reward_env_0=106.96803983963801\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Rank-0: policy_step=315960, average_reward_env_0=202.1755988579179\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Max step number - agent:  4\n",
      "Rank-0: policy_step=316144, average_reward_env_0=157.86021195089194\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Rank-0: policy_step=316168, average_reward_env_0=212.5548128062058\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Rank-0: policy_step=316272, average_reward_env_0=175.18879424091153\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Rank-0: policy_step=316432, average_reward_env_0=135.69443058028935\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Rank-0: policy_step=316456, average_reward_env_0=169.3206926804375\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Arrive at the goal! - agent:  5  - stage:  6\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Rank-0: policy_step=316664, average_reward_env_0=143.24166148344392\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Max step number - agent:  5\n",
      "Rank-0: policy_step=316776, average_reward_env_0=232.7679838061922\n",
      "Arrive at the goal! - agent:  1  - stage:  6\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Arrive at the goal! - agent:  6  - stage:  6\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Rank-0: policy_step=317152, average_reward_env_0=211.55418179154188\n",
      "Arrive at the goal! - agent:  2  - stage:  6\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Rank-0: policy_step=317224, average_reward_env_0=143.75655663216557\n",
      "Arrive at the goal! - agent:  2  - stage:  6\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Rank-0: policy_step=317360, average_reward_env_0=109.39583797550392\n",
      "Arrive at the goal! - agent:  5  - stage:  6\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Arrive at the goal! - agent:  8  - stage:  6\n",
      "Rank-0: policy_step=317504, average_reward_env_0=207.9027185222944\n",
      "Arrive at the goal! - agent:  4  - stage:  6\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Rank-0: policy_step=317632, average_reward_env_0=120.16925555379581\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Rank-0: policy_step=318008, average_reward_env_0=193.5565159724421\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Rank-0: policy_step=318112, average_reward_env_0=212.0789234637849\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Arrive at the goal! - agent:  3  - stage:  6\n",
      "Arrive at the goal! - agent:  5  - stage:  6\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Rank-0: policy_step=318520, average_reward_env_0=89.5509643006401\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Arrive at the goal! - agent:  1  - stage:  6\n",
      "Arrive at the goal! - agent:  8  - stage:  6\n",
      "Arrive at the goal! - agent:  4  - stage:  6\n",
      "Arrive at the goal! - agent:  2  - stage:  6\n",
      "Max step number - agent:  2\n",
      "Rank-0: policy_step=318760, average_reward_env_0=147.9481589819724\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Rank-0: policy_step=318880, average_reward_env_0=143.0678222739276\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Arrive at the goal! - agent:  8  - stage:  6\n",
      "Collision detected between /World/Worker_4 and /World/Warehouse/Warehouse/GroundPlane/CollisionPlane\n",
      "Collision detected between /World/Worker_4 and /World/Worker_3\n",
      "Collision detected between /World/Worker_4 and /World/Worker_3\n",
      "Arrive at the goal! - agent:  7  - stage:  6\n",
      "Arrive at the goal! - agent:  2  - stage:  6\n",
      "Arrive at the goal! - agent:  4  - stage:  6\n",
      "Arrive at the goal! - agent:  1  - stage:  6\n",
      "Arrive at the goal! - agent:  3  - stage:  6\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Rank-0: policy_step=319368, average_reward_env_0=212.7507877093922\n",
      "Arrive at the goal! - agent:  6  - stage:  6\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Rank-0: policy_step=319584, average_reward_env_0=146.98506465448168\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Arrive at the goal! - agent:  5  - stage:  6\n",
      "Collision detected between /World/Worker_4 and /World/Worker_3\n",
      "Collision detected between /World/Worker_4 and /World/Worker_3\n",
      "Collision detected between /World/Worker_4 and /World/Worker_3\n",
      "Collision detected between /World/Worker_4 and /World/Worker_3\n",
      "Collision detected between /World/Worker_4 and /World/Warehouse/Forklift_B01_PR_V_NVD_03\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Rank-0: policy_step=319752, average_reward_env_0=217.0285932611952\n",
      "Max step number - agent:  4\n",
      "Rank-0: policy_step=319872, average_reward_env_0=166.61172680638165\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Rank-0: policy_step=319984, average_reward_env_0=143.68007763838727\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Rank-0: policy_step=320000, average_reward_env_0=124.07444895665411\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Arrive at the goal! - agent:  2  - stage:  6\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Rank-0: policy_step=320080, average_reward_env_0=203.60422917219356\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Rank-0: policy_step=320288, average_reward_env_0=200.3180577862432\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Arrive at the goal! - agent:  5  - stage:  6\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Rank-0: policy_step=320552, average_reward_env_0=99.32452810541098\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Rank-0: policy_step=320584, average_reward_env_0=208.70608335690451\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Rank-0: policy_step=320680, average_reward_env_0=126.00635038779386\n",
      "Collision detected between /World/Worker_4 and /World/Worker_3\n",
      "Collision detected between /World/Worker_4 and /World/Worker_3\n",
      "Collision detected between /World/Worker_4 and /World/Worker_3\n",
      "Collision detected between /World/Worker_4 and /World/Worker_3\n",
      "Arrive at the goal! - agent:  1  - stage:  6\n",
      "Arrive at the goal! - agent:  4  - stage:  6\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Rank-0: policy_step=320856, average_reward_env_0=125.98964615379398\n",
      "Arrive at the goal! - agent:  2  - stage:  6\n",
      "Collision detected between /World/Worker_4 and /World/Worker_1\n",
      "Collision detected between /World/Worker_4 and /World/Worker_1\n",
      "Collision detected between /World/Worker_4 and /World/Worker_1\n",
      "Collision detected between /World/Worker_4 and /World/Worker_1\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Rank-0: policy_step=320960, average_reward_env_0=145.20873391763496\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Max step number - agent:  1\n",
      "Rank-0: policy_step=321208, average_reward_env_0=220.84916969002379\n",
      "Collision detected between /World/Worker_2 and /World/Warehouse/Table_12\n",
      "Arrive at the goal! - agent:  6  - stage:  6\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Rank-0: policy_step=321368, average_reward_env_0=174.6267550173144\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Arrive at the goal! - agent:  8  - stage:  6\n",
      "Arrive at the goal! - agent:  3  - stage:  6\n",
      "Arrive at the goal! - agent:  5  - stage:  6\n",
      "Arrive at the goal! - agent:  4  - stage:  6\n",
      "Collision detected between /World/Worker_3 and /World/Worker_1\n",
      "Collision detected between /World/Worker_3 and /World/Worker_1\n",
      "Collision detected between /World/Worker_3 and /World/Worker_1\n",
      "Collision detected between /World/Worker_3 and /World/Worker_1\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Rank-0: policy_step=321712, average_reward_env_0=213.82147539575178\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Arrive at the goal! - agent:  7  - stage:  6\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Rank-0: policy_step=321880, average_reward_env_0=126.91336344212759\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Rank-0: policy_step=321952, average_reward_env_0=130.47682010159872\n",
      "Max step number - agent:  2\n",
      "Rank-0: policy_step=321960, average_reward_env_0=157.38837484339822\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Arrive at the goal! - agent:  1  - stage:  6\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Arrive at the goal! - agent:  3  - stage:  6\n",
      "Arrive at the goal! - agent:  3  - stage:  6\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Rank-0: policy_step=322320, average_reward_env_0=204.60854297346046\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Rank-0: policy_step=322528, average_reward_env_0=113.23052145710768\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Rank-0: policy_step=322584, average_reward_env_0=109.5006182088864\n",
      "Arrive at the goal! - agent:  6  - stage:  6\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Arrive at the goal! - agent:  4  - stage:  6\n",
      "Arrive at the goal! - agent:  6  - stage:  6\n",
      "Arrive at the goal! - agent:  1  - stage:  6\n",
      "Arrive at the goal! - agent:  2  - stage:  6\n",
      "Arrive at the goal! - agent:  3  - stage:  6\n",
      "Arrive at the goal! - agent:  7  - stage:  6\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Rank-0: policy_step=323344, average_reward_env_0=120.7993708766011\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Arrive at the goal! - agent:  4  - stage:  6\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Rank-0: policy_step=323600, average_reward_env_0=219.6359579716744\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Arrive at the goal! - agent:  2  - stage:  6\n",
      "Arrive at the goal! - agent:  4  - stage:  6\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Rank-0: policy_step=323784, average_reward_env_0=167.26917474956423\n",
      "Max step number - agent:  3\n",
      "Rank-0: policy_step=323792, average_reward_env_0=225.82428450810056\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Rank-0: policy_step=323856, average_reward_env_0=112.2464118562749\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collision detected between /World/Worker_3 and /World/Warehouse/Table_08\n",
      "Arrive at the goal! - agent:  7  - stage:  6\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Rank-0: policy_step=324104, average_reward_env_0=161.33799600931528\n",
      "Rank-0: policy_step=324104, average_reward_env_1=200.0626332718013\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Rank-0: policy_step=324152, average_reward_env_0=100.13378724371586\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Arrive at the goal! - agent:  1  - stage:  6\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Rank-0: policy_step=324288, average_reward_env_0=217.96273364770894\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Arrive at the goal! - agent:  5  - stage:  6\n",
      "Rank-0: policy_step=324360, average_reward_env_0=211.61543065581805\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Rank-0: policy_step=324400, average_reward_env_0=210.95794128014754\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Rank-0: policy_step=324408, average_reward_env_0=211.05530979761815\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Arrive at the goal! - agent:  7  - stage:  6\n",
      "Arrive at the goal! - agent:  8  - stage:  6\n",
      "Arrive at the goal! - agent:  3  - stage:  6\n",
      "Arrive at the goal! - agent:  7  - stage:  6\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Rank-0: policy_step=324872, average_reward_env_0=207.71526526221623\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Rank-0: policy_step=325072, average_reward_env_0=198.67226913853733\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Max step number - agent:  7\n",
      "Rank-0: policy_step=325080, average_reward_env_0=159.09009078848624\n",
      "Arrive at the goal! - agent:  5  - stage:  6\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Rank-0: policy_step=325136, average_reward_env_1=157.50070740320544\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Arrive at the goal! - agent:  8  - stage:  6\n",
      "Rank-0: policy_step=325264, average_reward_env_0=165.36850272502042\n",
      "Rank-0: policy_step=325264, average_reward_env_1=196.2498713340894\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Arrive at the goal! - agent:  8  - stage:  6\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Rank-0: policy_step=325544, average_reward_env_0=199.87022813481792\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Arrive at the goal! - agent:  6  - stage:  6\n",
      "Arrive at the goal! - agent:  1  - stage:  6\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Rank-0: policy_step=325584, average_reward_env_1=114.82536530583984\n",
      "Collision detected between /World/Worker_3 and /World/Warehouse/Warehouse/SM_Rackshield_3841/SM_Rackshield_02\n",
      "Arrive at the goal! - agent:  5  - stage:  6\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Rank-0: policy_step=325808, average_reward_env_0=189.31884794584843\n",
      "Rank-0: policy_step=325808, average_reward_env_1=108.1001726862941\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Rank-0: policy_step=325856, average_reward_env_0=129.4376319589555\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Rank-0: policy_step=325864, average_reward_env_0=192.76218329406277\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Rank-0: policy_step=325888, average_reward_env_0=162.9075670084424\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Rank-0: policy_step=326160, average_reward_env_0=221.0330727633757\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Arrive at the goal! - agent:  2  - stage:  6\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Rank-0: policy_step=326232, average_reward_env_0=144.57288010372324\n",
      "Rank-0: policy_step=326232, average_reward_env_1=104.91765035802075\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Rank-0: policy_step=326352, average_reward_env_0=101.3712934701973\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Rank-0: policy_step=326392, average_reward_env_0=160.1039109473674\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Collision detected between /World/Worker_4 and /World/Warehouse/Table_12\n",
      "Max step number - agent:  6\n",
      "Rank-0: policy_step=326552, average_reward_env_0=131.27154332844665\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Rank-0: policy_step=326704, average_reward_env_0=230.22907406172936\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Rank-0: policy_step=326752, average_reward_env_0=130.49191711403816\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Collision detected between /World/Worker_1 and /World/Warehouse/Warehouse/SM_Rackshield_29/SM_Rackshield_02\n",
      "Arrive at the goal! - agent:  1  - stage:  6\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Rank-0: policy_step=326904, average_reward_env_0=128.65510857693363\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Arrive at the goal! - agent:  7  - stage:  6\n",
      "Arrive at the goal! - agent:  2  - stage:  6\n",
      "Arrive at the goal! - agent:  4  - stage:  6\n",
      "Arrive at the goal! - agent:  3  - stage:  6\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Rank-0: policy_step=327256, average_reward_env_1=111.99020939909653\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Arrive at the goal! - agent:  4  - stage:  6\n",
      "Arrive at the goal! - agent:  5  - stage:  6\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Rank-0: policy_step=327360, average_reward_env_0=235.4709209817234\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Arrive at the goal! - agent:  1  - stage:  6\n",
      "Collision detected between /World/Worker_2 and /World/Warehouse/Warehouse/GroundPlane/CollisionPlane\n",
      "Arrive at the goal! - agent:  2  - stage:  6\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Rank-0: policy_step=327616, average_reward_env_0=189.10058784987882\n",
      "Arrive at the goal! - agent:  4  - stage:  6\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Arrive at the goal! - agent:  6  - stage:  6\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Rank-0: policy_step=327696, average_reward_env_0=144.89481221192491\n",
      "Rank-0: policy_step=327696, average_reward_env_1=234.67427904485118\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Arrive at the goal! - agent:  8  - stage:  6\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Rank-0: policy_step=327760, average_reward_env_0=171.0310053471876\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Rank-0: policy_step=327928, average_reward_env_0=134.62549188788753\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Arrive at the goal! - agent:  6  - stage:  6\n",
      "Arrive at the goal! - agent:  2  - stage:  6\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Rank-0: policy_step=328424, average_reward_env_0=124.84360865043242\n",
      "Arrive at the goal! - agent:  6  - stage:  6\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Rank-0: policy_step=328568, average_reward_env_0=236.53186627301343\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Rank-0: policy_step=328656, average_reward_env_0=133.4077326676211\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Rank-0: policy_step=328736, average_reward_env_0=167.40154015054975\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Rank-0: policy_step=328744, average_reward_env_0=113.58860772333044\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Arrive at the goal! - agent:  8  - stage:  6\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Rank-0: policy_step=328888, average_reward_env_0=213.2204854713023\n",
      "Collision detected between /World/Worker_4 and /World/Worker_1\n",
      "Collision detected between /World/Worker_4 and /World/Worker_1\n",
      "Collision detected between /World/Worker_4 and /World/Worker_1\n",
      "Collision detected between /World/Worker_4 and /World/Worker_1\n",
      "Arrive at the goal! - agent:  1  - stage:  6\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Arrive at the goal! - agent:  4  - stage:  6\n",
      "Collision detected between /World/Worker_4 and /World/Warehouse/Forklift_B01_PR_V_NVD_01\n",
      "Collision detected between /World/Worker_4 and /World/Warehouse/Forklift_B01_PR_V_NVD_01\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Arrive at the goal! - agent:  8  - stage:  6\n",
      "Arrive at the goal! - agent:  3  - stage:  6\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Rank-0: policy_step=329328, average_reward_env_0=130.9867123673772\n",
      "Arrive at the goal! - agent:  6  - stage:  6\n",
      "Arrive at the goal! - agent:  1  - stage:  6\n",
      "Max step number - agent:  8\n",
      "Rank-0: policy_step=329552, average_reward_env_0=126.69135878251349\n",
      "Arrive at the goal! - agent:  5  - stage:  6\n",
      "Arrive at the goal! - agent:  6  - stage:  6\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Rank-0: policy_step=329792, average_reward_env_0=164.69689517419857\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Arrive at the goal! - agent:  7  - stage:  6\n",
      "Arrive at the goal! - agent:  1  - stage:  6\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Arrive at the goal! - agent:  4  - stage:  6\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Rank-0: policy_step=330120, average_reward_env_0=238.0532065555014\n",
      "Arrive at the goal! - agent:  3  - stage:  6\n",
      "Arrive at the goal! - agent:  1  - stage:  6\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Arrive at the goal! - agent:  8  - stage:  6\n",
      "Rank-0: policy_step=330256, average_reward_env_0=167.9622685435653\n",
      "Arrive at the goal! - agent:  1  - stage:  6\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Rank-0: policy_step=330656, average_reward_env_0=132.97333509636238\n",
      "Arrive at the goal! - agent:  4  - stage:  6\n",
      "Arrive at the goal! - agent:  2  - stage:  6\n",
      "Arrive at the goal! - agent:  3  - stage:  6\n",
      "Max step number - agent:  4\n",
      "Rank-0: policy_step=330816, average_reward_env_1=209.53734472480326\n",
      "Max step number - agent:  3\n",
      "Rank-0: policy_step=330904, average_reward_env_0=171.34307052725165\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Arrive at the goal! - agent:  2  - stage:  6\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Rank-0: policy_step=331064, average_reward_env_0=148.2551579869895\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Arrive at the goal! - agent:  5  - stage:  6\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Rank-0: policy_step=331248, average_reward_env_0=130.44566552044887\n",
      "Arrive at the goal! - agent:  7  - stage:  6\n",
      "Arrive at the goal! - agent:  1  - stage:  6\n",
      "Collision detected between /World/Worker_2 and /World/Worker_1\n",
      "Collision detected between /World/Worker_2 and /World/Worker_1\n",
      "Collision detected between /World/Worker_2 and /World/Worker_1\n",
      "Collision detected between /World/Worker_2 and /World/Worker_1\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Rank-0: policy_step=331944, average_reward_env_0=117.8677646390401\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Arrive at the goal! - agent:  3  - stage:  6\n",
      "Rank-0: policy_step=331976, average_reward_env_0=248.0961486376553\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Rank-0: policy_step=332104, average_reward_env_0=199.55204900504137\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Arrive at the goal! - agent:  8  - stage:  6\n",
      "Collision detected between /World/Worker_4 and /World/Warehouse/Table_08\n",
      "Arrive at the goal! - agent:  7  - stage:  6\n",
      "Arrive at the goal! - agent:  5  - stage:  6\n",
      "Arrive at the goal! - agent:  2  - stage:  6\n",
      "Collision detected between /World/Worker_4 and /World/Worker_2\n",
      "Collision detected between /World/Worker_4 and /World/Worker_2\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Max step number - agent:  7\n",
      "Rank-0: policy_step=332528, average_reward_env_0=196.95544284012246\n",
      "Rank-0: policy_step=332528, average_reward_env_1=155.7583736501241\n",
      "Arrive at the goal! - agent:  1  - stage:  6\n",
      "Collision detected between /World/Worker_4 and /World/Warehouse/Warehouse/SM_WallA_6M18/SM_WallA_6M/SM_WallA_6M/Section0\n",
      "Collision detected between /World/Worker_4 and /World/Warehouse/Warehouse/SM_WallA_6M18/SM_WallA_6M/SM_WallA_6M/Section1\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Rank-0: policy_step=332680, average_reward_env_0=132.21730248772846\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Rank-0: policy_step=332744, average_reward_env_0=229.4451177545566\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Rank-0: policy_step=332880, average_reward_env_0=190.47135676918936\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Arrive at the goal! - agent:  3  - stage:  6\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collision detected between /World/Worker_2 and /World/Worker_1\n",
      "Collision detected between /World/Worker_2 and /World/Worker_1\n",
      "Collision detected between /World/Worker_2 and /World/Worker_1\n",
      "Collision detected between /World/Worker_2 and /World/Worker_1\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Rank-0: policy_step=333288, average_reward_env_0=131.19872893498805\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Arrive at the goal! - agent:  5  - stage:  6\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Rank-0: policy_step=333448, average_reward_env_0=254.57755120676384\n",
      "Max step number - agent:  2\n",
      "Rank-0: policy_step=333456, average_reward_env_0=163.81209500563557\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Rank-0: policy_step=333584, average_reward_env_0=222.26488721048986\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Arrive at the goal! - agent:  8  - stage:  6\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Rank-0: policy_step=333680, average_reward_env_0=245.70427265789206\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Arrive at the goal! - agent:  7  - stage:  6\n",
      "Rank-0: policy_step=333696, average_reward_env_0=156.1647585214728\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Arrive at the goal! - agent:  3  - stage:  6\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Rank-0: policy_step=333952, average_reward_env_0=130.9364403514927\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Arrive at the goal! - agent:  1  - stage:  6\n",
      "Arrive at the goal! - agent:  5  - stage:  6\n",
      "Max step number - agent:  3\n",
      "Rank-0: policy_step=334272, average_reward_env_0=170.77868706848724\n",
      "Arrive at the goal! - agent:  4  - stage:  6\n",
      "Arrive at the goal! - agent:  2  - stage:  6\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Rank-0: policy_step=334488, average_reward_env_0=250.78561405476543\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Rank-0: policy_step=334560, average_reward_env_0=107.50261700562822\n",
      "Arrive at the goal! - agent:  6  - stage:  6\n",
      "Arrive at the goal! - agent:  7  - stage:  6\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Arrive at the goal! - agent:  4  - stage:  6\n",
      "Rank-0: policy_step=334744, average_reward_env_0=224.38788356186447\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Rank-0: policy_step=334752, average_reward_env_0=225.0343524561814\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Arrive at the goal! - agent:  4  - stage:  6\n",
      "Collision detected between /World/Worker_4 and /World/Worker_2\n",
      "Collision detected between /World/Worker_4 and /World/Worker_2\n",
      "Arrive at the goal! - agent:  7  - stage:  6\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Arrive at the goal! - agent:  8  - stage:  6\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Rank-0: policy_step=335016, average_reward_env_0=225.1143095731668\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Arrive at the goal! - agent:  6  - stage:  6\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Rank-0: policy_step=335192, average_reward_env_0=153.7055576014757\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Rank-0: policy_step=335224, average_reward_env_0=206.31438544162557\n",
      "Arrive at the goal! - agent:  7  - stage:  6\n",
      "Collision detected between /World/Worker_4 and /World/Worker_2\n",
      "Collision detected between /World/Worker_4 and /World/Worker_2\n",
      "Collision detected between /World/Worker_4 and /World/Worker_2\n",
      "Collision detected between /World/Worker_4 and /World/Worker_2\n",
      "Collision detected between /World/Worker_4 and /World/Warehouse/Warehouse/SM_Rackshield_3841/SM_Rackshield_02\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Rank-0: policy_step=335416, average_reward_env_0=161.87173479746005\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Rank-0: policy_step=335496, average_reward_env_0=155.44399552641127\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Arrive at the goal! - agent:  1  - stage:  6\n",
      "Arrive at the goal! - agent:  8  - stage:  6\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Arrive at the goal! - agent:  8  - stage:  6\n",
      "Rank-0: policy_step=335672, average_reward_env_0=209.2801599947435\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Rank-0: policy_step=335696, average_reward_env_0=227.96640652347128\n",
      "Max step number - agent:  7\n",
      "Rank-0: policy_step=335728, average_reward_env_0=184.52720137372648\n",
      "Collision detected between /World/Worker_3 and /World/Warehouse/Table_08\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Rank-0: policy_step=335864, average_reward_env_0=121.41065314789033\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Rank-0: policy_step=335904, average_reward_env_0=152.67699433134203\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Arrive at the goal! - agent:  8  - stage:  6\n",
      "Arrive at the goal! - agent:  6  - stage:  6\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Rank-0: policy_step=336160, average_reward_env_0=214.45642628759532\n",
      "Arrive at the goal! - agent:  4  - stage:  6\n",
      "Arrive at the goal! - agent:  7  - stage:  6\n",
      "Arrive at the goal! - agent:  1  - stage:  6\n",
      "Arrive at the goal! - agent:  5  - stage:  6\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Rank-0: policy_step=336296, average_reward_env_0=209.27574449317825\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Rank-0: policy_step=336512, average_reward_env_0=150.89402822438015\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Rank-0: policy_step=336544, average_reward_env_0=197.60405186607852\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Rank-0: policy_step=336600, average_reward_env_0=125.35939120908773\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Arrive at the goal! - agent:  6  - stage:  6\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Rank-0: policy_step=336696, average_reward_env_0=169.58042429860726\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Collision detected between /World/Worker_3 and /World/Worker_1\n",
      "Collision detected between /World/Worker_3 and /World/Worker_1\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Rank-0: policy_step=336816, average_reward_env_0=218.19602387986643\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Arrive at the goal! - agent:  1  - stage:  6\n",
      "Arrive at the goal! - agent:  3  - stage:  6\n",
      "Collision detected between /World/Worker_1 and /World/Warehouse/Warehouse/SM_PaletteA_3962/SM_PaletteA_01\n",
      "Arrive at the goal! - agent:  6  - stage:  6\n",
      "Arrive at the goal! - agent:  4  - stage:  6\n",
      "Arrive at the goal! - agent:  8  - stage:  6\n",
      "Arrive at the goal! - agent:  2  - stage:  6\n",
      "Arrive at the goal! - agent:  1  - stage:  6\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Rank-0: policy_step=337552, average_reward_env_0=154.2004929459886\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Arrive at the goal! - agent:  3  - stage:  6\n",
      "Arrive at the goal! - agent:  8  - stage:  6\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Arrive at the goal! - agent:  7  - stage:  6\n",
      "Rank-0: policy_step=337736, average_reward_env_0=211.75805342181334\n",
      "Max step number - agent:  8\n",
      "Rank-0: policy_step=337760, average_reward_env_0=135.6779194595497\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Rank-0: policy_step=337840, average_reward_env_0=184.13110882767194\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Arrive at the goal! - agent:  7  - stage:  6\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collision detected between /World/Worker_2 and /World/Worker_1\n",
      "Collision detected between /World/Worker_2 and /World/Worker_1\n",
      "Collision detected between /World/Worker_2 and /World/Worker_1\n",
      "Collision detected between /World/Worker_2 and /World/Worker_1\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Rank-0: policy_step=338016, average_reward_env_0=124.05047337035222\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Arrive at the goal! - agent:  4  - stage:  6\n",
      "Arrive at the goal! - agent:  6  - stage:  6\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Rank-0: policy_step=338448, average_reward_env_0=197.18248488687234\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Arrive at the goal! - agent:  1  - stage:  6\n",
      "Collision detected between /World/Worker_2 and /World/Warehouse/Warehouse/SM_Rackshield_1075/SM_Rackshield_02\n",
      "Collision detected between /World/Worker_1 and /World/Warehouse/Rack_07\n",
      "Arrive at the goal! - agent:  4  - stage:  6\n",
      "Arrive at the goal! - agent:  7  - stage:  6\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Rank-0: policy_step=339000, average_reward_env_0=140.22396790674762\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Max step number - agent:  6\n",
      "Rank-0: policy_step=339064, average_reward_env_0=152.66929062864557\n",
      "Arrive at the goal! - agent:  8  - stage:  6\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Rank-0: policy_step=339184, average_reward_env_0=226.47051658754154\n",
      "Arrive at the goal! - agent:  2  - stage:  6\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Rank-0: policy_step=339384, average_reward_env_0=175.27768848151717\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Rank-0: policy_step=339416, average_reward_env_0=149.00550981453208\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Arrive at the goal! - agent:  7  - stage:  6\n",
      "Collision detected between /World/Worker_2 and /World/Warehouse/Table_09\n",
      "Arrive at the goal! - agent:  1  - stage:  6\n",
      "Max step number - agent:  1\n",
      "Rank-0: policy_step=339904, average_reward_env_0=199.9772559009005\n",
      "Collision detected between /World/Worker_2 and /World/Warehouse/Warehouse/SM_Rackshield_3841/SM_Rackshield_02\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Rank-0: policy_step=340000, average_reward_env_0=177.0703923390505\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Arrive at the goal! - agent:  3  - stage:  6\n",
      "Collision detected between /World/Worker_4 and /World/Worker_2\n",
      "Collision detected between /World/Worker_4 and /World/Worker_2\n",
      "Collision detected between /World/Worker_4 and /World/Worker_2\n",
      "Collision detected between /World/Worker_4 and /World/Worker_2\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Rank-0: policy_step=340160, average_reward_env_0=208.68261659519104\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Arrive at the goal! - agent:  4  - stage:  6\n",
      "Rank-0: policy_step=340176, average_reward_env_0=197.162612236929\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Arrive at the goal! - agent:  6  - stage:  6\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Rank-0: policy_step=340240, average_reward_env_0=228.18564111355917\n",
      "Arrive at the goal! - agent:  2  - stage:  6\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Rank-0: policy_step=340416, average_reward_env_0=147.48680980023158\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Rank-0: policy_step=340520, average_reward_env_0=177.19016270241443\n",
      "Arrive at the goal! - agent:  2  - stage:  6\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Rank-0: policy_step=340720, average_reward_env_0=146.35816193666588\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Rank-0: policy_step=340744, average_reward_env_0=215.93191255204368\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Arrive at the goal! - agent:  7  - stage:  6\n",
      "Collision detected between /World/Worker_4 and /World/Warehouse/Warehouse/SM_Rackshield_2984/SM_Rackshield_02\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Rank-0: policy_step=340992, average_reward_env_0=155.779980673519\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Arrive at the goal! - agent:  2  - stage:  6\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Rank-0: policy_step=341216, average_reward_env_0=148.30601666296977\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Arrive at the goal! - agent:  1  - stage:  6\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Rank-0: policy_step=341296, average_reward_env_0=149.00729077183252\n",
      "Arrive at the goal! - agent:  4  - stage:  6\n",
      "Arrive at the goal! - agent:  6  - stage:  6\n",
      "Arrive at the goal! - agent:  7  - stage:  6\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Arrive at the goal! - agent:  3  - stage:  6\n",
      "Arrive at the goal! - agent:  2  - stage:  6\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Rank-0: policy_step=342368, average_reward_env_0=154.7455429273268\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Rank-0: policy_step=342480, average_reward_env_0=224.96874999450915\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Max step number - agent:  6\n",
      "Rank-0: policy_step=342624, average_reward_env_0=167.80647659633374\n",
      "Arrive at the goal! - agent:  7  - stage:  6\n",
      "Arrive at the goal! - agent:  1  - stage:  6\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Rank-0: policy_step=342792, average_reward_env_0=123.95494221844703\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Arrive at the goal! - agent:  5  - stage:  6\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Rank-0: policy_step=343072, average_reward_env_0=225.204144065144\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Rank-0: policy_step=343208, average_reward_env_0=166.50450482567967\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Arrive at the goal! - agent:  7  - stage:  6\n",
      "Max step number - agent:  7\n",
      "Rank-0: policy_step=343368, average_reward_env_0=234.14610498807443\n",
      "Max step number - agent:  7\n",
      "Rank-0: policy_step=343376, average_reward_env_0=257.7939521120896\n",
      "Max step number - agent:  1\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Rank-0: policy_step=343384, average_reward_env_0=221.85288977215387\n",
      "Rank-0: policy_step=343384, average_reward_env_1=164.57313956468923\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Arrive at the goal! - agent:  3  - stage:  6\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Rank-0: policy_step=343728, average_reward_env_0=166.7736319395685\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Arrive at the goal! - agent:  2  - stage:  6\n",
      "Arrive at the goal! - agent:  4  - stage:  6\n",
      "Collision detected between /World/Worker_3 and /World/Warehouse/Warehouse/GroundPlane/CollisionPlane\n",
      "Arrive at the goal! - agent:  7  - stage:  6\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Arrive at the goal! - agent:  8  - stage:  6\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Rank-0: policy_step=344080, average_reward_env_0=151.95607251051175\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Max step number - agent:  5\n",
      "Rank-0: policy_step=344200, average_reward_env_0=173.94919873144192\n",
      "Arrive at the goal! - agent:  8  - stage:  6\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Rank-0: policy_step=344400, average_reward_env_0=163.6027014181526\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Rank-0: policy_step=344408, average_reward_env_0=162.41106047398543\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Rank-0: policy_step=344448, average_reward_env_0=267.10833484081996\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Arrive at the goal! - agent:  4  - stage:  6\n",
      "Arrive at the goal! - agent:  6  - stage:  6\n",
      "Arrive at the goal! - agent:  1  - stage:  6\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Rank-0: policy_step=345000, average_reward_env_0=142.89156524092755\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Arrive at the goal! - agent:  6  - stage:  6\n",
      "Arrive at the goal! - agent:  8  - stage:  6\n",
      "Arrive at the goal! - agent:  2  - stage:  6\n",
      "Arrive at the goal! - agent:  4  - stage:  6\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Rank-0: policy_step=345504, average_reward_env_0=232.5238293745163\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Rank-0: policy_step=345520, average_reward_env_0=149.7722605187635\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Arrive at the goal! - agent:  7  - stage:  6\n",
      "Rank-0: policy_step=345608, average_reward_env_0=246.5115990855352\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Arrive at the goal! - agent:  4  - stage:  6\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Rank-0: policy_step=345648, average_reward_env_0=157.96469088655047\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Rank-0: policy_step=345736, average_reward_env_0=161.10311491755186\n",
      "Rank-0: policy_step=345736, average_reward_env_1=157.404564345025\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Arrive at the goal! - agent:  2  - stage:  6\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Max step number - agent:  8\n",
      "Rank-0: policy_step=346000, average_reward_env_0=152.7724498175274\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Rank-0: policy_step=346168, average_reward_env_0=153.55841514888118\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Collision detected between /World/Worker_4 and /World/Warehouse/Rack_02\n",
      "Arrive at the goal! - agent:  1  - stage:  6\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Rank-0: policy_step=346296, average_reward_env_1=255.2849572880583\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Rank-0: policy_step=346440, average_reward_env_0=120.67659073321023\n",
      "Arrive at the goal! - agent:  4  - stage:  6\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Rank-0: policy_step=346536, average_reward_env_0=159.91883407806213\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Arrive at the goal! - agent:  3  - stage:  6\n",
      "Arrive at the goal! - agent:  1  - stage:  6\n",
      "Arrive at the goal! - agent:  8  - stage:  6\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Rank-0: policy_step=346784, average_reward_env_0=154.37319281272525\n",
      "Collision detected between /World/Worker_4 and /World/Worker_1\n",
      "Collision detected between /World/Worker_4 and /World/Worker_1\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Collision detected between /World/Worker_4 and /World/Worker_1\n",
      "Collision detected between /World/Worker_4 and /World/Worker_1\n",
      "Collision detected between /World/Worker_1 and /World/Warehouse/Warehouse/SM_RackFrame_3840/SM_RackFrame_03/SM_RackFrame_03/Section1\n",
      "Collision detected between /World/Worker_1 and /World/Warehouse/Warehouse/SM_Rackshield_3841/SM_Rackshield_02\n",
      "Arrive at the goal! - agent:  7  - stage:  6\n",
      "Arrive at the goal! - agent:  6  - stage:  6\n",
      "Arrive at the goal! - agent:  6  - stage:  6\n",
      "Arrive at the goal! - agent:  4  - stage:  6\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Rank-0: policy_step=346960, average_reward_env_0=153.5410731875567\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Rank-0: policy_step=347048, average_reward_env_0=258.8347564851095\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Rank-0: policy_step=347232, average_reward_env_0=163.5762927839629\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Rank-0: policy_step=347344, average_reward_env_0=112.99750922598109\n",
      "Arrive at the goal! - agent:  1  - stage:  6\n",
      "Arrive at the goal! - agent:  2  - stage:  6\n",
      "Arrive at the goal! - agent:  8  - stage:  6\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Rank-0: policy_step=347768, average_reward_env_0=163.24240123431483\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Rank-0: policy_step=347816, average_reward_env_0=238.064265430129\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Arrive at the goal! - agent:  7  - stage:  6\n",
      "Arrive at the goal! - agent:  1  - stage:  6\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Rank-0: policy_step=348024, average_reward_env_0=167.62538212699144\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Rank-0: policy_step=348064, average_reward_env_0=241.72205339165765\n",
      "Arrive at the goal! - agent:  8  - stage:  6\n",
      "Collision detected between /World/Worker_4 and /World/Worker_1\n",
      "Collision detected between /World/Worker_4 and /World/Worker_1\n",
      "Collision detected between /World/Worker_4 and /World/Worker_1\n",
      "Collision detected between /World/Worker_4 and /World/Worker_1\n",
      "Arrive at the goal! - agent:  7  - stage:  6\n",
      "Arrive at the goal! - agent:  5  - stage:  6\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Rank-0: policy_step=348480, average_reward_env_0=239.45294286013998\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Rank-0: policy_step=348664, average_reward_env_0=213.79775330275368\n",
      "Arrive at the goal! - agent:  6  - stage:  6\n",
      "Arrive at the goal! - agent:  3  - stage:  6\n",
      "Arrive at the goal! - agent:  2  - stage:  6\n",
      "Arrive at the goal! - agent:  3  - stage:  6\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collision detected between /World/Worker_1 and /World/Warehouse/Rack_01\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Rank-0: policy_step=349192, average_reward_env_0=176.4469182414656\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Rank-0: policy_step=349456, average_reward_env_0=107.60467242864053\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Rank-0: policy_step=349536, average_reward_env_0=226.87131181927228\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Arrive at the goal! - agent:  7  - stage:  6\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Rank-0: policy_step=349648, average_reward_env_0=224.80576846140372\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Rank-0: policy_step=349720, average_reward_env_0=166.1928263789448\n",
      "Arrive at the goal! - agent:  3  - stage:  6\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Max step number - agent:  2\n",
      "Rank-0: policy_step=349744, average_reward_env_0=181.05714929734552\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Rank-0: policy_step=349872, average_reward_env_0=107.59837394270406\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Max step number - agent:  3\n",
      "Rank-0: policy_step=350160, average_reward_env_0=166.50864956957815\n",
      "Arrive at the goal! - agent:  8  - stage:  6\n",
      "Max step number - agent:  7\n",
      "Rank-0: policy_step=350248, average_reward_env_0=254.47785643282936\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Rank-0: policy_step=350384, average_reward_env_0=253.0389741293206\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Arrive at the goal! - agent:  4  - stage:  6\n",
      "Arrive at the goal! - agent:  3  - stage:  6\n",
      "Arrive at the goal! - agent:  1  - stage:  6\n",
      "Arrive at the goal! - agent:  4  - stage:  6\n",
      "Arrive at the goal! - agent:  3  - stage:  6\n",
      "Arrive at the goal! - agent:  5  - stage:  6\n",
      "Arrive at the goal! - agent:  7  - stage:  6\n",
      "Arrive at the goal! - agent:  6  - stage:  6\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Rank-0: policy_step=350888, average_reward_env_0=165.24466877939184\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Arrive at the goal! - agent:  7  - stage:  6\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Rank-0: policy_step=351088, average_reward_env_0=100.35920966375087\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Rank-0: policy_step=351352, average_reward_env_0=159.31566164853365\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Arrive at the goal! - agent:  2  - stage:  6\n",
      "Arrive at the goal! - agent:  3  - stage:  6\n",
      "Arrive at the goal! - agent:  1  - stage:  6\n",
      "Arrive at the goal! - agent:  8  - stage:  6\n",
      "Arrive at the goal! - agent:  7  - stage:  6\n",
      "Collision detected between /World/Worker_1 and /World/Warehouse/Table_03\n",
      "Max step number - agent:  1\n",
      "Rank-0: policy_step=351864, average_reward_env_0=233.68388755164705\n",
      "Arrive at the goal! - agent:  7  - stage:  6\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Arrive at the goal! - agent:  4  - stage:  6\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Arrive at the goal! - agent:  5  - stage:  6\n",
      "Arrive at the goal! - agent:  6  - stage:  6\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Rank-0: policy_step=352232, average_reward_env_0=275.6065528312116\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Rank-0: policy_step=352360, average_reward_env_0=179.67189013357503\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Rank-0: policy_step=352392, average_reward_env_0=168.0034293761072\n",
      "Max step number - agent:  4\n",
      "Arrive at the goal! - agent:  6  - stage:  6\n",
      "Rank-0: policy_step=352856, average_reward_env_0=251.44654086388138\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Rank-0: policy_step=352952, average_reward_env_0=259.6900383157488\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collision detected between /World/Worker_4 and /World/Warehouse/Table_08\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Rank-0: policy_step=353024, average_reward_env_0=174.6301486259812\n",
      "Collision detected between /World/Worker_3 and /World/Warehouse/Table_09\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Rank-0: policy_step=353080, average_reward_env_0=162.66612283839217\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Rank-0: policy_step=353200, average_reward_env_0=231.41360231357257\n",
      "Arrive at the goal! - agent:  3  - stage:  6\n",
      "Arrive at the goal! - agent:  2  - stage:  6\n",
      "Max step number - agent:  3\n",
      "Rank-0: policy_step=353360, average_reward_env_1=184.4375689988141\n",
      "Arrive at the goal! - agent:  5  - stage:  6\n",
      "Collision detected between /World/Worker_3 and /World/Worker_2\n",
      "Collision detected between /World/Worker_3 and /World/Worker_2\n",
      "Collision detected between /World/Worker_3 and /World/Worker_2\n",
      "Collision detected between /World/Worker_3 and /World/Worker_2\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Rank-0: policy_step=353736, average_reward_env_0=182.06608903905712\n",
      "Arrive at the goal! - agent:  1  - stage:  6\n",
      "Arrive at the goal! - agent:  7  - stage:  6\n",
      "Arrive at the goal! - agent:  8  - stage:  6\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Arrive at the goal! - agent:  6  - stage:  6\n",
      "Arrive at the goal! - agent:  7  - stage:  6\n",
      "Arrive at the goal! - agent:  1  - stage:  6\n",
      "Max step number - agent:  5\n",
      "Rank-0: policy_step=354296, average_reward_env_0=115.10127589066174\n",
      "Collision detected between /World/Worker_4 and /World/Worker_3\n",
      "Collision detected between /World/Worker_4 and /World/Worker_3\n",
      "Arrive at the goal! - agent:  4  - stage:  6\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Rank-0: policy_step=354520, average_reward_env_0=156.27749515810143\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Arrive at the goal! - agent:  1  - stage:  6\n",
      "Arrive at the goal! - agent:  7  - stage:  6\n",
      "Arrive at the goal! - agent:  8  - stage:  6\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Arrive at the goal! - agent:  4  - stage:  6\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Arrive at the goal! - agent:  3  - stage:  6\n",
      "Arrive at the goal! - agent:  5  - stage:  6\n",
      "Arrive at the goal! - agent:  1  - stage:  6\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Rank-0: policy_step=355288, average_reward_env_0=254.729904922075\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Rank-0: policy_step=355392, average_reward_env_0=145.5755105595468\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Arrive at the goal! - agent:  6  - stage:  6\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Rank-0: policy_step=355576, average_reward_env_0=224.57619063603073\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Rank-0: policy_step=355664, average_reward_env_0=193.14183960084733\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Arrive at the goal! - agent:  5  - stage:  6\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Arrive at the goal! - agent:  6  - stage:  6\n",
      "Rank-0: policy_step=355760, average_reward_env_0=227.516436631105\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Rank-0: policy_step=355920, average_reward_env_0=200.69390586508868\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Rank-0: policy_step=356104, average_reward_env_0=165.2425701499691\n",
      "Max step number - agent:  7\n",
      "Rank-0: policy_step=356152, average_reward_env_0=284.38189766328736\n",
      "Arrive at the goal! - agent:  5  - stage:  6\n",
      "Arrive at the goal! - agent:  1  - stage:  6\n",
      "Max step number - agent:  6\n",
      "Rank-0: policy_step=356280, average_reward_env_0=163.78092006682192\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Rank-0: policy_step=356376, average_reward_env_0=284.76238264870295\n",
      "Max step number - agent:  1\n",
      "Rank-0: policy_step=356400, average_reward_env_0=250.2464978062411\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Rank-0: policy_step=356480, average_reward_env_0=164.6626039326757\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Arrive at the goal! - agent:  6  - stage:  6\n",
      "Collision detected between /World/Worker_2 and /World/Warehouse/Table_08\n",
      "Arrive at the goal! - agent:  2  - stage:  6\n",
      "Collision detected between /World/Worker_4 and /World/Warehouse/Table_03\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Arrive at the goal! - agent:  5  - stage:  6\n",
      "Arrive at the goal! - agent:  6  - stage:  6\n",
      "Arrive at the goal! - agent:  4  - stage:  6\n",
      "Max step number - agent:  5\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Rank-0: policy_step=357496, average_reward_env_0=124.88304411069825\n",
      "Rank-0: policy_step=357496, average_reward_env_1=191.82005688681318\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Arrive at the goal! - agent:  3  - stage:  6\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Rank-0: policy_step=357560, average_reward_env_0=183.58029715045058\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Rank-0: policy_step=357632, average_reward_env_0=173.33799618882247\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Arrive at the goal! - agent:  1  - stage:  6\n",
      "Arrive at the goal! - agent:  7  - stage:  6\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Rank-0: policy_step=357792, average_reward_env_0=167.28955248714985\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Collision detected between /World/Worker_4 and /World/Warehouse/Warehouse/SM_Rackshield_3841/SM_Rackshield_02\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Arrive at the goal! - agent:  7  - stage:  6\n",
      "Arrive at the goal! - agent:  1  - stage:  6\n",
      "Arrive at the goal! - agent:  4  - stage:  6\n",
      "Arrive at the goal! - agent:  2  - stage:  6\n",
      "Arrive at the goal! - agent:  2  - stage:  6\n",
      "Collision detected between /World/Worker_4 and /World/Worker_1\n",
      "Collision detected between /World/Worker_4 and /World/Worker_1\n",
      "Collision detected between /World/Worker_4 and /World/Worker_1\n",
      "Collision detected between /World/Worker_4 and /World/Worker_1\n",
      "Arrive at the goal! - agent:  4  - stage:  6\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Rank-0: policy_step=358504, average_reward_env_0=120.95702010552344\n",
      "Arrive at the goal! - agent:  6  - stage:  6\n",
      "Max step number - agent:  2\n",
      "Rank-0: policy_step=358600, average_reward_env_0=170.89738688059907\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Rank-0: policy_step=358680, average_reward_env_0=182.1304751662422\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Rank-0: policy_step=358704, average_reward_env_0=285.57352727024426\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Arrive at the goal! - agent:  3  - stage:  6\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Rank-0: policy_step=358920, average_reward_env_0=213.41873084774414\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Rank-0: policy_step=359144, average_reward_env_0=240.56240625906008\n",
      "Arrive at the goal! - agent:  6  - stage:  6\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Rank-0: policy_step=359256, average_reward_env_0=233.97185852952745\n",
      "Arrive at the goal! - agent:  7  - stage:  6\n",
      "Max step number - agent:  6\n",
      "Rank-0: policy_step=359480, average_reward_env_0=177.43699853306535\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Arrive at the goal! - agent:  5  - stage:  6\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Arrive at the goal! - agent:  3  - stage:  6\n",
      "Collision detected between /World/Worker_4 and /World/Worker_1\n",
      "Collision detected between /World/Worker_4 and /World/Worker_1\n",
      "Collision detected between /World/Worker_4 and /World/Worker_1\n",
      "Collision detected between /World/Worker_4 and /World/Worker_1\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Rank-0: policy_step=359680, average_reward_env_0=185.60473535852165\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Arrive at the goal! - agent:  2  - stage:  6\n",
      "Rank-0: policy_step=359928, average_reward_env_0=219.73761547019862\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collision detected between /World/Worker_3 and /World/Warehouse/Warehouse/SM_FireExtinguisher_1925/SM_FireExtinguisher_02\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Rank-0: policy_step=360120, average_reward_env_0=174.8093565517307\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Rank-0: policy_step=360208, average_reward_env_0=191.07176829518008\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Rank-0: policy_step=360240, average_reward_env_0=129.6886193496992\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Rank-0: policy_step=360248, average_reward_env_0=184.34789562824062\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Collision detected between /World/Worker_2 and /World/Warehouse/Rack_01\n",
      "Arrive at the goal! - agent:  7  - stage:  6\n",
      "Arrive at the goal! - agent:  6  - stage:  6\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Rank-0: policy_step=360600, average_reward_env_0=186.69010260740237\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Rank-0: policy_step=360672, average_reward_env_0=186.80646334343228\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Collision detected between /World/Worker_1 and /World/Warehouse/Warehouse/SM_FloorDecal_StripeFull_4m102/SM_FloorDecal_StripeFull_4m\n",
      "Arrive at the goal! - agent:  1  - stage:  6\n",
      "Arrive at the goal! - agent:  4  - stage:  6\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Arrive at the goal! - agent:  2  - stage:  6\n",
      "Arrive at the goal! - agent:  3  - stage:  6\n",
      "Arrive at the goal! - agent:  1  - stage:  6\n",
      "Arrive at the goal! - agent:  6  - stage:  6\n",
      "Arrive at the goal! - agent:  7  - stage:  6\n",
      "Arrive at the goal! - agent:  3  - stage:  6\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Rank-0: policy_step=361472, average_reward_env_0=189.37158421804688\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Rank-0: policy_step=361736, average_reward_env_0=191.93760359225246\n",
      "Arrive at the goal! - agent:  7  - stage:  6\n",
      "Max step number - agent:  7\n",
      "Rank-0: policy_step=361912, average_reward_env_0=315.34259619086725\n",
      "Max step number - agent:  7\n",
      "Rank-0: policy_step=361920, average_reward_env_0=347.6171429996279\n",
      "Arrive at the goal! - agent:  2  - stage:  6\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Arrive at the goal! - agent:  1  - stage:  6\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Rank-0: policy_step=362104, average_reward_env_0=340.08742883894485\n",
      "Arrive at the goal! - agent:  5  - stage:  6\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Rank-0: policy_step=362392, average_reward_env_0=200.95273216029716\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Rank-0: policy_step=362432, average_reward_env_0=194.21113803851577\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Arrive at the goal! - agent:  1  - stage:  6\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Arrive at the goal! - agent:  4  - stage:  6\n",
      "Arrive at the goal! - agent:  2  - stage:  6\n",
      "Arrive at the goal! - agent:  6  - stage:  6\n",
      "Arrive at the goal! - agent:  1  - stage:  6\n",
      "Max step number - agent:  1\n",
      "Rank-0: policy_step=363136, average_reward_env_0=256.1039750621619\n",
      "Max step number - agent:  1\n",
      "Rank-0: policy_step=363144, average_reward_env_0=281.6592594987991\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Rank-0: policy_step=363192, average_reward_env_0=312.21849100812864\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Rank-0: policy_step=363288, average_reward_env_0=200.64865725100043\n",
      "Max step number - agent:  2\n",
      "Rank-0: policy_step=363328, average_reward_env_0=192.12408377018738\n",
      "Max step number - agent:  5\n",
      "Rank-0: policy_step=363440, average_reward_env_0=137.6371454050178\n",
      "Arrive at the goal! - agent:  7  - stage:  6\n",
      "Arrive at the goal! - agent:  4  - stage:  6\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Rank-0: policy_step=363672, average_reward_env_0=201.57975580378326\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Arrive at the goal! - agent:  4  - stage:  6\n",
      "Arrive at the goal! - agent:  8  - stage:  6\n",
      "Rank-0: policy_step=363800, average_reward_env_0=180.35068408647683\n",
      "Arrive at the goal! - agent:  2  - stage:  6\n",
      "Arrive at the goal! - agent:  3  - stage:  6\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Rank-0: policy_step=363856, average_reward_env_0=176.23924414177858\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Rank-0: policy_step=364040, average_reward_env_0=197.7605481610113\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Rank-0: policy_step=364096, average_reward_env_0=120.72212131705281\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Rank-0: policy_step=364280, average_reward_env_0=187.40559480298552\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Rank-0: policy_step=364376, average_reward_env_0=114.88514825198105\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Arrive at the goal! - agent:  6  - stage:  6\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Rank-0: policy_step=364512, average_reward_env_0=319.27716352951916\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Arrive at the goal! - agent:  3  - stage:  6\n",
      "Arrive at the goal! - agent:  7  - stage:  6\n",
      "Arrive at the goal! - agent:  1  - stage:  6\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Rank-0: policy_step=364984, average_reward_env_0=108.46477690358684\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Rank-0: policy_step=365008, average_reward_env_0=170.12221121102215\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Rank-0: policy_step=365048, average_reward_env_0=219.4559104833635\n",
      "Arrive at the goal! - agent:  2  - stage:  6\n",
      "Arrive at the goal! - agent:  8  - stage:  6\n",
      "Arrive at the goal! - agent:  6  - stage:  6\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Rank-0: policy_step=365752, average_reward_env_0=110.0106598702416\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Rank-0: policy_step=365952, average_reward_env_0=107.22316309973964\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Arrive at the goal! - agent:  3  - stage:  6\n",
      "Arrive at the goal! - agent:  1  - stage:  6\n",
      "Arrive at the goal! - agent:  4  - stage:  6\n",
      "Max step number - agent:  1\n",
      "Rank-0: policy_step=366344, average_reward_env_0=286.96802621630064\n",
      "Arrive at the goal! - agent:  6  - stage:  6\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Rank-0: policy_step=366512, average_reward_env_0=326.395868706729\n",
      "Arrive at the goal! - agent:  2  - stage:  6\n",
      "Arrive at the goal! - agent:  8  - stage:  6\n",
      "Arrive at the goal! - agent:  5  - stage:  6\n",
      "Max step number - agent:  6\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Rank-0: policy_step=366880, average_reward_env_0=214.8713574220585\n",
      "Rank-0: policy_step=366880, average_reward_env_1=204.2675474770935\n",
      "Arrive at the goal! - agent:  4  - stage:  6\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Rank-0: policy_step=366976, average_reward_env_0=193.67402479247937\n",
      "Arrive at the goal! - agent:  1  - stage:  6\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Rank-0: policy_step=367064, average_reward_env_0=182.19421109654633\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Rank-0: policy_step=367192, average_reward_env_0=212.56463445990522\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Arrive at the goal! - agent:  8  - stage:  6\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Rank-0: policy_step=367488, average_reward_env_0=209.37534519472493\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Rank-0: policy_step=367568, average_reward_env_0=195.66627861204108\n",
      "Arrive at the goal! - agent:  7  - stage:  6\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Rank-0: policy_step=367600, average_reward_env_0=296.6718486410297\n",
      "Arrive at the goal! - agent:  1  - stage:  6\n",
      "Arrive at the goal! - agent:  2  - stage:  6\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Rank-0: policy_step=367760, average_reward_env_2=233.21107034895078\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Collision detected between /World/Worker_4 and /World/Warehouse/Warehouse/GroundPlane/CollisionPlane\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Arrive at the goal! - agent:  1  - stage:  6\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Rank-0: policy_step=368072, average_reward_env_0=117.40358458454055\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Arrive at the goal! - agent:  1  - stage:  6\n",
      "Arrive at the goal! - agent:  3  - stage:  6\n",
      "Arrive at the goal! - agent:  4  - stage:  6\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Rank-0: policy_step=368360, average_reward_env_0=212.38120284229336\n",
      "Arrive at the goal! - agent:  2  - stage:  6\n",
      "Arrive at the goal! - agent:  7  - stage:  6\n",
      "Collision detected between /World/Worker_1 and /World/Warehouse/Warehouse/GroundPlane/CollisionPlane\n",
      "Arrive at the goal! - agent:  4  - stage:  6\n",
      "Arrive at the goal! - agent:  3  - stage:  6\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Rank-0: policy_step=368744, average_reward_env_0=193.89362628116666\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Arrive at the goal! - agent:  8  - stage:  6\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Arrive at the goal! - agent:  6  - stage:  6\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Rank-0: policy_step=369192, average_reward_env_0=114.82674267456404\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Arrive at the goal! - agent:  4  - stage:  6\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Rank-0: policy_step=369416, average_reward_env_0=281.0731852242782\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Arrive at the goal! - agent:  3  - stage:  6\n",
      "Arrive at the goal! - agent:  8  - stage:  6\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Rank-0: policy_step=369664, average_reward_env_0=210.1593632662529\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Max step number - agent:  7\n",
      "Rank-0: policy_step=369712, average_reward_env_0=339.1738055096077\n",
      "Collision detected between /World/Worker_1 and /World/Warehouse/Warehouse/SM_Rackshield_3841/SM_Rackshield_02\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Rank-0: policy_step=369920, average_reward_env_0=198.81694877260168\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collision detected between /World/Worker_1 and /World/Warehouse/Warehouse/SM_RackFrame_145/SM_RackFrame_03/SM_RackFrame_03/Section1\n",
      "Collision detected between /World/Worker_1 and /World/Warehouse/Warehouse/SM_FireExtinguisher_1925/SM_FireExtinguisher_02\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Rank-0: policy_step=369984, average_reward_env_0=206.56820209973918\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Arrive at the goal! - agent:  6  - stage:  6\n",
      "Collision detected between /World/Worker_2 and /World/Warehouse/Warehouse/SM_Rackshield_2984/SM_Rackshield_02\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Rank-0: policy_step=370152, average_reward_env_0=338.74655389833225\n",
      "Arrive at the goal! - agent:  4  - stage:  6\n",
      "Arrive at the goal! - agent:  3  - stage:  6\n",
      "Arrive at the goal! - agent:  8  - stage:  6\n",
      "Arrive at the goal! - agent:  5  - stage:  6\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Rank-0: policy_step=370712, average_reward_env_0=189.10249773757747\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Arrive at the goal! - agent:  1  - stage:  6\n",
      "Arrive at the goal! - agent:  4  - stage:  6\n",
      "Arrive at the goal! - agent:  2  - stage:  6\n",
      "Arrive at the goal! - agent:  5  - stage:  6\n",
      "Max step number - agent:  3\n",
      "Rank-0: policy_step=370968, average_reward_env_0=260.63295315878486\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collision detected between /World/Worker_1 and /World/Warehouse/Warehouse/SM_Rackshield_1075/SM_Rackshield_02\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Arrive at the goal! - agent:  4  - stage:  6\n",
      "Rank-0: policy_step=371024, average_reward_env_0=193.49653660775272\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Rank-0: policy_step=371064, average_reward_env_0=127.05066071670808\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Rank-0: policy_step=371128, average_reward_env_0=183.51276892354178\n",
      "Arrive at the goal! - agent:  6  - stage:  6\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Rank-0: policy_step=371240, average_reward_env_0=215.36666462831664\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Rank-0: policy_step=371264, average_reward_env_0=341.18986972651516\n",
      "Collision detected between /World/Worker_3 and /World/Warehouse/Table_12\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Rank-0: policy_step=371320, average_reward_env_0=314.67856065882467\n",
      "Arrive at the goal! - agent:  3  - stage:  6\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Rank-0: policy_step=371440, average_reward_env_0=127.90306837752114\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Arrive at the goal! - agent:  8  - stage:  6\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Rank-0: policy_step=371512, average_reward_env_0=122.1874096438555\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Arrive at the goal! - agent:  1  - stage:  6\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Rank-0: policy_step=371696, average_reward_env_0=206.56248994731087\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Rank-0: policy_step=371848, average_reward_env_0=257.8527991836077\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Max step number - agent:  6\n",
      "Rank-0: policy_step=371952, average_reward_env_0=225.93920872519556\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Rank-0: policy_step=372080, average_reward_env_0=287.924400383345\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Rank-0: policy_step=372160, average_reward_env_0=195.65570094800762\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Arrive at the goal! - agent:  2  - stage:  6\n",
      "Arrive at the goal! - agent:  1  - stage:  6\n",
      "Arrive at the goal! - agent:  2  - stage:  6\n",
      "Collision detected between /World/Worker_1 and /World/Warehouse/Table_09\n",
      "Arrive at the goal! - agent:  4  - stage:  6\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Rank-0: policy_step=372512, average_reward_env_0=230.9298585274189\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Max step number - agent:  1\n",
      "Rank-0: policy_step=372624, average_reward_env_0=299.6318155590406\n",
      "Arrive at the goal! - agent:  6  - stage:  6\n",
      "Collision detected between /World/Worker_1 and /World/Warehouse/Warehouse/SM_Rackshield_2984/SM_Rackshield_02\n",
      "Arrive at the goal! - agent:  1  - stage:  6\n",
      "Arrive at the goal! - agent:  8  - stage:  6\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Arrive at the goal! - agent:  5  - stage:  6\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Rank-0: policy_step=373072, average_reward_env_0=304.97934067126795\n",
      "Max step number - agent:  4\n",
      "Rank-0: policy_step=373192, average_reward_env_0=237.56949722576366\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Rank-0: policy_step=373240, average_reward_env_0=209.0236732655564\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Arrive at the goal! - agent:  7  - stage:  6\n",
      "Arrive at the goal! - agent:  5  - stage:  6\n",
      "Arrive at the goal! - agent:  6  - stage:  6\n",
      "Arrive at the goal! - agent:  2  - stage:  6\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Rank-0: policy_step=373648, average_reward_env_0=299.73309001243075\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Arrive at the goal! - agent:  7  - stage:  6\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Rank-0: policy_step=373832, average_reward_env_0=217.76330377745728\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Arrive at the goal! - agent:  5  - stage:  6\n",
      "Max step number - agent:  2\n",
      "Rank-0: policy_step=374328, average_reward_env_0=211.53110962839932\n",
      "Collision detected between /World/Worker_3 and /World/Warehouse/Table_08\n",
      "Collision detected between /World/Worker_3 and /World/Worker_1\n",
      "Collision detected between /World/Worker_3 and /World/Worker_1\n",
      "Collision detected between /World/Worker_3 and /World/Worker_1\n",
      "Collision detected between /World/Worker_3 and /World/Worker_1\n",
      "Arrive at the goal! - agent:  7  - stage:  6\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Rank-0: policy_step=374448, average_reward_env_0=210.5687788404023\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Arrive at the goal! - agent:  6  - stage:  6\n",
      "Collision detected between /World/Worker_4 and /World/Warehouse/Table_12\n",
      "Arrive at the goal! - agent:  8  - stage:  6\n",
      "Max step number - agent:  5\n",
      "Rank-0: policy_step=374720, average_reward_env_0=145.36391029826515\n",
      "Arrive at the goal! - agent:  7  - stage:  6\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Arrive at the goal! - agent:  4  - stage:  6\n",
      "Arrive at the goal! - agent:  2  - stage:  6\n",
      "Collision detected between /World/Worker_1 and /World/Warehouse/Rack_07\n",
      "Max step number - agent:  7\n",
      "Rank-0: policy_step=375280, average_reward_env_0=296.13786654917794\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Rank-0: policy_step=375360, average_reward_env_0=195.85983578156007\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Rank-0: policy_step=375408, average_reward_env_0=239.34122659498297\n",
      "Arrive at the goal! - agent:  8  - stage:  6\n",
      "Arrive at the goal! - agent:  5  - stage:  6\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Rank-0: policy_step=375640, average_reward_env_0=307.5249953657138\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Arrive at the goal! - agent:  1  - stage:  6\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Rank-0: policy_step=376072, average_reward_env_0=221.1394795002582\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Arrive at the goal! - agent:  5  - stage:  6\n",
      "Collision detected between /World/Worker_4 and /World/Worker_2\n",
      "Collision detected between /World/Worker_4 and /World/Worker_2\n",
      "Collision detected between /World/Worker_4 and /World/Worker_2\n",
      "Collision detected between /World/Worker_4 and /World/Worker_2\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Rank-0: policy_step=376352, average_reward_env_0=155.858174038567\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Rank-0: policy_step=376400, average_reward_env_0=286.145297336863\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Max step number - agent:  8\n",
      "Rank-0: policy_step=376448, average_reward_env_0=218.83093766384067\n",
      "Arrive at the goal! - agent:  3  - stage:  6\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Rank-0: policy_step=376768, average_reward_env_0=222.08919100596316\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Arrive at the goal! - agent:  4  - stage:  6\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Arrive at the goal! - agent:  1  - stage:  6\n",
      "Arrive at the goal! - agent:  6  - stage:  6\n",
      "Collision detected between /World/Worker_4 and /World/Worker_3\n",
      "Collision detected between /World/Worker_4 and /World/Worker_3\n",
      "Collision detected between /World/Worker_4 and /World/Worker_3\n",
      "Collision detected between /World/Worker_4 and /World/Worker_3\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Rank-0: policy_step=377112, average_reward_env_0=222.22648918720145\n",
      "Arrive at the goal! - agent:  2  - stage:  6\n",
      "Arrive at the goal! - agent:  1  - stage:  6\n",
      "Arrive at the goal! - agent:  7  - stage:  6\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Rank-0: policy_step=377664, average_reward_env_0=219.22200814477569\n",
      "Arrive at the goal! - agent:  2  - stage:  6\n",
      "Arrive at the goal! - agent:  8  - stage:  6\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collision detected between /World/Worker_3 and /World/Warehouse/Warehouse/SM_FloorDecal_StripeFull_4m102/SM_FloorDecal_StripeFull_4m\n",
      "Arrive at the goal! - agent:  3  - stage:  6\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Rank-0: policy_step=378064, average_reward_env_1=266.14833748620026\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Collision detected between /World/Worker_3 and /World/Worker_1\n",
      "Collision detected between /World/Worker_3 and /World/Worker_1\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Rank-0: policy_step=378168, average_reward_env_0=235.7590015426373\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Arrive at the goal! - agent:  6  - stage:  6\n",
      "Arrive at the goal! - agent:  5  - stage:  6\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Rank-0: policy_step=378432, average_reward_env_0=227.15796137251\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Rank-0: policy_step=378464, average_reward_env_0=228.08439606439003\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Max step number - agent:  6\n",
      "Rank-0: policy_step=378568, average_reward_env_0=215.87778467599205\n",
      "Arrive at the goal! - agent:  2  - stage:  6\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Rank-0: policy_step=378808, average_reward_env_1=214.60200832500294\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Max step number - agent:  1\n",
      "Rank-0: policy_step=378848, average_reward_env_0=329.26682071699906\n",
      "Collision detected between /World/Worker_3 and /World/Warehouse/Warehouse/SM_Rackshield_3841/SM_Rackshield_02\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Rank-0: policy_step=379120, average_reward_env_0=322.4155210315479\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Arrive at the goal! - agent:  4  - stage:  6\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Rank-0: policy_step=379192, average_reward_env_0=168.8303264626859\n",
      "Max step number - agent:  2\n",
      "Rank-0: policy_step=379280, average_reward_env_0=248.29702658681435\n",
      "Arrive at the goal! - agent:  7  - stage:  6\n",
      "Arrive at the goal! - agent:  6  - stage:  6\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Rank-0: policy_step=379344, average_reward_env_0=219.6233689324554\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Rank-0: policy_step=379448, average_reward_env_0=210.40986478063968\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Arrive at the goal! - agent:  8  - stage:  6\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Rank-0: policy_step=379920, average_reward_env_0=210.24585669471392\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Arrive at the goal! - agent:  6  - stage:  6\n",
      "Rank-0: policy_step=380000, average_reward_env_0=210.37287255099028\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Rank-0: policy_step=380096, average_reward_env_0=327.6445572443306\n",
      "Arrive at the goal! - agent:  2  - stage:  6\n",
      "Arrive at the goal! - agent:  6  - stage:  6\n",
      "Collision detected between /World/Worker_3 and /World/Worker_1\n",
      "Collision detected between /World/Worker_3 and /World/Worker_1\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Collision detected between /World/Worker_3 and /World/Worker_1\n",
      "Collision detected between /World/Worker_3 and /World/Worker_1\n",
      "Rank-0: policy_step=380320, average_reward_env_0=155.17329254521772\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Rank-0: policy_step=380384, average_reward_env_0=255.67124065703294\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Rank-0: policy_step=380432, average_reward_env_0=199.88507372552962\n",
      "Arrive at the goal! - agent:  1  - stage:  6\n",
      "Arrive at the goal! - agent:  4  - stage:  6\n",
      "Arrive at the goal! - agent:  2  - stage:  6\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Rank-0: policy_step=380760, average_reward_env_0=235.9985095299725\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Rank-0: policy_step=380792, average_reward_env_0=302.29569940253356\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Arrive at the goal! - agent:  7  - stage:  6\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Rank-0: policy_step=381000, average_reward_env_0=233.1361949745681\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Rank-0: policy_step=381040, average_reward_env_0=184.1228830228417\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Arrive at the goal! - agent:  3  - stage:  6\n",
      "Arrive at the goal! - agent:  7  - stage:  6\n",
      "Arrive at the goal! - agent:  1  - stage:  6\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Rank-0: policy_step=381296, average_reward_env_0=253.1390556392407\n",
      "Arrive at the goal! - agent:  5  - stage:  6\n",
      "Arrive at the goal! - agent:  8  - stage:  6\n",
      "Max step number - agent:  8\n",
      "Rank-0: policy_step=381632, average_reward_env_0=209.40287497450635\n",
      "Max step number - agent:  7\n",
      "Rank-0: policy_step=381672, average_reward_env_0=241.81412382044826\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Rank-0: policy_step=381880, average_reward_env_0=211.74044478725375\n",
      "Rank-0: policy_step=381880, average_reward_env_1=235.65794957752976\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Arrive at the goal! - agent:  8  - stage:  6\n",
      "Arrive at the goal! - agent:  3  - stage:  6\n",
      "Arrive at the goal! - agent:  6  - stage:  6\n",
      "Arrive at the goal! - agent:  5  - stage:  6\n",
      "Arrive at the goal! - agent:  2  - stage:  6\n",
      "Arrive at the goal! - agent:  2  - stage:  6\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Rank-0: policy_step=382520, average_reward_env_0=211.50643025004402\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Rank-0: policy_step=382632, average_reward_env_0=168.69397599129502\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Arrive at the goal! - agent:  1  - stage:  6\n",
      "Arrive at the goal! - agent:  7  - stage:  6\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Rank-0: policy_step=382864, average_reward_env_0=231.2203243076248\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Arrive at the goal! - agent:  2  - stage:  6\n",
      "Arrive at the goal! - agent:  8  - stage:  6\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Rank-0: policy_step=383376, average_reward_env_0=164.42460200709502\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Arrive at the goal! - agent:  3  - stage:  6\n",
      "Arrive at the goal! - agent:  3  - stage:  6\n",
      "Arrive at the goal! - agent:  7  - stage:  6\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Max step number - agent:  1\n",
      "Rank-0: policy_step=384000, average_reward_env_0=323.95915077038336\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Arrive at the goal! - agent:  8  - stage:  6\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Rank-0: policy_step=384096, average_reward_env_0=195.82874131314185\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Rank-0: policy_step=384120, average_reward_env_0=166.10712378940204\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Collision detected between /World/Worker_2 and /World/Worker_1\n",
      "Collision detected between /World/Worker_2 and /World/Worker_1\n",
      "Arrive at the goal! - agent:  7  - stage:  6\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Rank-0: policy_step=384208, average_reward_env_0=237.4121313513162\n",
      "Max step number - agent:  3\n",
      "Rank-0: policy_step=384248, average_reward_env_0=213.47569027102585\n",
      "Arrive at the goal! - agent:  4  - stage:  6\n",
      "Max step number - agent:  2\n",
      "Rank-0: policy_step=384496, average_reward_env_0=258.16582596510415\n",
      "Collision detected between /World/Worker_4 and /World/Warehouse/Table_08\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Rank-0: policy_step=384664, average_reward_env_0=254.77350880376775\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Arrive at the goal! - agent:  2  - stage:  6\n",
      "Max step number - agent:  8\n",
      "Rank-0: policy_step=384832, average_reward_env_0=225.4947313417153\n",
      "Arrive at the goal! - agent:  7  - stage:  6\n",
      "Arrive at the goal! - agent:  4  - stage:  6\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Rank-0: policy_step=384928, average_reward_env_0=184.98114085637752\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collision detected between /World/Worker_4 and /World/Worker_3\n",
      "Collision detected between /World/Worker_4 and /World/Worker_3\n",
      "Collision detected between /World/Worker_4 and /World/Worker_3\n",
      "Collision detected between /World/Worker_4 and /World/Worker_3\n",
      "Collision detected between /World/Worker_3 and /World/Warehouse/Warehouse/Box_43762/SM_CardBoxC_01/SM_CardBoxC_01\n",
      "Collision detected between /World/Worker_3 and /World/Warehouse/Warehouse/Box_43762/SM_CardBoxC_01/SM_CardBoxC_01\n",
      "Max step number - agent:  7\n",
      "Rank-0: policy_step=385080, average_reward_env_0=239.68374423121034\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Rank-0: policy_step=385144, average_reward_env_0=218.61629878632917\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Arrive at the goal! - agent:  1  - stage:  6\n",
      "Arrive at the goal! - agent:  5  - stage:  6\n",
      "Arrive at the goal! - agent:  6  - stage:  6\n",
      "Arrive at the goal! - agent:  3  - stage:  6\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Rank-0: policy_step=385704, average_reward_env_0=232.65649615495408\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Arrive at the goal! - agent:  4  - stage:  6\n",
      "Arrive at the goal! - agent:  2  - stage:  6\n",
      "Arrive at the goal! - agent:  1  - stage:  6\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Arrive at the goal! - agent:  8  - stage:  6\n",
      "Arrive at the goal! - agent:  2  - stage:  6\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Rank-0: policy_step=386064, average_reward_env_1=166.13872354111663\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Rank-0: policy_step=386208, average_reward_env_0=165.34344649532417\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Rank-0: policy_step=386272, average_reward_env_0=233.09947708198968\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Arrive at the goal! - agent:  4  - stage:  6\n",
      "Arrive at the goal! - agent:  7  - stage:  6\n",
      "Collision detected between /World/Worker_2 and /World/Warehouse/Table_09\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Rank-0: policy_step=386424, average_reward_env_0=220.78567083971436\n",
      "Arrive at the goal! - agent:  5  - stage:  6\n",
      "Arrive at the goal! - agent:  8  - stage:  6\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Rank-0: policy_step=386584, average_reward_env_0=248.14292772358462\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Rank-0: policy_step=386656, average_reward_env_0=265.5900009322637\n",
      "Rank-0: policy_step=386656, average_reward_env_1=223.32914159286418\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Arrive at the goal! - agent:  8  - stage:  6\n",
      "Collision detected between /World/Worker_4 and /World/Worker_1\n",
      "Collision detected between /World/Worker_4 and /World/Worker_1\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Collision detected between /World/Worker_4 and /World/Worker_1\n",
      "Collision detected between /World/Worker_4 and /World/Worker_1\n",
      "Rank-0: policy_step=386808, average_reward_env_0=213.94775301355787\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Arrive at the goal! - agent:  6  - stage:  6\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Rank-0: policy_step=387064, average_reward_env_0=256.24134342163944\n",
      "Arrive at the goal! - agent:  7  - stage:  6\n",
      "Max step number - agent:  1\n",
      "Rank-0: policy_step=387200, average_reward_env_0=324.093465698979\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Rank-0: policy_step=387208, average_reward_env_0=221.6869772961896\n",
      "Max step number - agent:  4\n",
      "Rank-0: policy_step=387296, average_reward_env_0=207.3260669129643\n",
      "Collision detected between /World/Worker_3 and /World/Warehouse/Warehouse/SM_Rackshield_2984/SM_Rackshield_02\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Rank-0: policy_step=387384, average_reward_env_0=207.67083905522705\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Rank-0: policy_step=387432, average_reward_env_0=213.35685469275575\n",
      "Arrive at the goal! - agent:  5  - stage:  6\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Rank-0: policy_step=387736, average_reward_env_0=172.08999803413232\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Collision detected between /World/Worker_4 and /World/Warehouse/Table_08\n",
      "Arrive at the goal! - agent:  3  - stage:  6\n",
      "Arrive at the goal! - agent:  3  - stage:  6\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collision detected between /World/Worker_3 and /World/Warehouse/Rack_02\n",
      "Arrive at the goal! - agent:  6  - stage:  6\n",
      "Arrive at the goal! - agent:  8  - stage:  6\n",
      "Arrive at the goal! - agent:  1  - stage:  6\n",
      "Arrive at the goal! - agent:  2  - stage:  6\n",
      "Arrive at the goal! - agent:  7  - stage:  6\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Rank-0: policy_step=388592, average_reward_env_0=229.53203716687017\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Collision detected between /World/Worker_3 and /World/Warehouse/Rack_02\n",
      "Arrive at the goal! - agent:  7  - stage:  6\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Rank-0: policy_step=388808, average_reward_env_0=228.41935497796987\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Arrive at the goal! - agent:  3  - stage:  6\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Arrive at the goal! - agent:  2  - stage:  6\n",
      "Arrive at the goal! - agent:  8  - stage:  6\n",
      "Arrive at the goal! - agent:  1  - stage:  6\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Rank-0: policy_step=389184, average_reward_env_0=206.4763985811595\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Arrive at the goal! - agent:  5  - stage:  6\n",
      "Arrive at the goal! - agent:  4  - stage:  6\n",
      "Arrive at the goal! - agent:  6  - stage:  6\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Rank-0: policy_step=389296, average_reward_env_0=246.13252603738027\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Rank-0: policy_step=389752, average_reward_env_0=244.93512953515778\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Rank-0: policy_step=389848, average_reward_env_0=330.7745234645596\n",
      "Arrive at the goal! - agent:  5  - stage:  6\n",
      "Arrive at the goal! - agent:  3  - stage:  6\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Arrive at the goal! - agent:  2  - stage:  6\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Rank-0: policy_step=390112, average_reward_env_0=183.87406936487042\n",
      "Max step number - agent:  2\n",
      "Rank-0: policy_step=390264, average_reward_env_0=274.70959028333067\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Rank-0: policy_step=390328, average_reward_env_0=182.54760091343636\n",
      "Rank-0: policy_step=390328, average_reward_env_1=226.51171915202576\n",
      "Arrive at the goal! - agent:  8  - stage:  6\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Rank-0: policy_step=390408, average_reward_env_0=313.16024425855426\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Rank-0: policy_step=390536, average_reward_env_0=220.55853360327453\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Max step number - agent:  4\n",
      "Rank-0: policy_step=390592, average_reward_env_0=198.9883692339696\n",
      "Max step number - agent:  8\n",
      "Rank-0: policy_step=390632, average_reward_env_0=237.64827306228952\n",
      "Arrive at the goal! - agent:  4  - stage:  6\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Rank-0: policy_step=390784, average_reward_env_0=170.3814498263638\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Max step number - agent:  5\n",
      "Rank-0: policy_step=390944, average_reward_env_0=172.4946335075595\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Rank-0: policy_step=390976, average_reward_env_0=141.92969377206532\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Arrive at the goal! - agent:  2  - stage:  6\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Rank-0: policy_step=391232, average_reward_env_0=172.2304702478092\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Arrive at the goal! - agent:  4  - stage:  6\n",
      "Arrive at the goal! - agent:  5  - stage:  6\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Rank-0: policy_step=391600, average_reward_env_0=225.53258448163288\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Rank-0: policy_step=391608, average_reward_env_0=310.99129803806653\n",
      "Arrive at the goal! - agent:  6  - stage:  6\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Rank-0: policy_step=391696, average_reward_env_0=276.6745230524168\n",
      "Arrive at the goal! - agent:  7  - stage:  6\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Arrive at the goal! - agent:  5  - stage:  6\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Rank-0: policy_step=392176, average_reward_env_0=312.6052934379562\n",
      "Arrive at the goal! - agent:  6  - stage:  6\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Rank-0: policy_step=392480, average_reward_env_0=168.72091174046264\n",
      "Arrive at the goal! - agent:  2  - stage:  6\n",
      "Arrive at the goal! - agent:  6  - stage:  6\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Rank-0: policy_step=392616, average_reward_env_0=223.64816385159196\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Rank-0: policy_step=392728, average_reward_env_0=191.8433335010278\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Arrive at the goal! - agent:  5  - stage:  6\n",
      "Arrive at the goal! - agent:  4  - stage:  6\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Rank-0: policy_step=392992, average_reward_env_0=263.6955346963249\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Arrive at the goal! - agent:  8  - stage:  6\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Rank-0: policy_step=393256, average_reward_env_0=236.21387356359403\n",
      "Arrive at the goal! - agent:  4  - stage:  6\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Rank-0: policy_step=393288, average_reward_env_0=292.8007035830606\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Max step number - agent:  4\n",
      "Rank-0: policy_step=393792, average_reward_env_0=209.66675112432057\n",
      "Arrive at the goal! - agent:  3  - stage:  6\n",
      "Arrive at the goal! - agent:  6  - stage:  6\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Arrive at the goal! - agent:  4  - stage:  6\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Rank-0: policy_step=394040, average_reward_env_0=253.95539222567228\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Max step number - agent:  5\n",
      "Rank-0: policy_step=394176, average_reward_env_0=165.3409857301374\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Rank-0: policy_step=394320, average_reward_env_0=180.329478813003\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Arrive at the goal! - agent:  8  - stage:  6\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Rank-0: policy_step=394520, average_reward_env_0=156.29511100656157\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Collision detected between /World/Worker_1 and /World/Warehouse/Warehouse/SM_Rackshield_2984/SM_Rackshield_02\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Rank-0: policy_step=394680, average_reward_env_0=293.23976791212004\n",
      "Collision detected between /World/Worker_1 and /World/Warehouse/Warehouse/SM_RackFrame_2983/SM_RackFrame_03/SM_RackFrame_03/Section1\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Collision detected between /World/Worker_1 and /World/Warehouse/Warehouse/SM_Rackshield_2984/SM_Rackshield_02\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Rank-0: policy_step=394696, average_reward_env_0=165.2820448322054\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Arrive at the goal! - agent:  5  - stage:  6\n",
      "Arrive at the goal! - agent:  7  - stage:  6\n",
      "Arrive at the goal! - agent:  8  - stage:  6\n",
      "Collision detected between /World/Worker_2 and /World/Warehouse/Table_09\n",
      "Max step number - agent:  8\n",
      "Rank-0: policy_step=394808, average_reward_env_0=244.62681793730712\n",
      "Arrive at the goal! - agent:  4  - stage:  6\n",
      "Arrive at the goal! - agent:  8  - stage:  6\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Rank-0: policy_step=394928, average_reward_env_0=185.91499082615945\n",
      "Arrive at the goal! - agent:  2  - stage:  6\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Rank-0: policy_step=394976, average_reward_env_0=149.16141720893913\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Rank-0: policy_step=395104, average_reward_env_0=231.65913048053255\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Rank-0: policy_step=395216, average_reward_env_0=179.91010090193788\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Rank-0: policy_step=395368, average_reward_env_0=260.5996252185813\n",
      "Rank-0: policy_step=395368, average_reward_env_1=229.57145885794432\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Rank-0: policy_step=395440, average_reward_env_0=145.55707191746149\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Arrive at the goal! - agent:  2  - stage:  6\n",
      "Arrive at the goal! - agent:  3  - stage:  6\n",
      "Arrive at the goal! - agent:  8  - stage:  6\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Rank-0: policy_step=395864, average_reward_env_0=138.13928783411015\n",
      "Arrive at the goal! - agent:  7  - stage:  6\n",
      "Arrive at the goal! - agent:  6  - stage:  6\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Arrive at the goal! - agent:  7  - stage:  6\n",
      "Rank-0: policy_step=396056, average_reward_env_0=244.58372383882582\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Rank-0: policy_step=396184, average_reward_env_0=226.28812165834628\n",
      "Collision detected between /World/Worker_2 and /World/Warehouse/Table_08\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Rank-0: policy_step=396216, average_reward_env_0=227.47929235555787\n",
      "Collision detected between /World/Worker_2 and /World/Warehouse/Warehouse/SM_WallA_6M18/SM_WallA_6M/SM_WallA_6M/Section0\n",
      "Collision detected between /World/Worker_2 and /World/Warehouse/Warehouse/SM_WallA_6M18/SM_WallA_6M/SM_WallA_6M/Section0\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Rank-0: policy_step=396280, average_reward_env_0=209.9220161483398\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Arrive at the goal! - agent:  5  - stage:  6\n",
      "Arrive at the goal! - agent:  1  - stage:  6\n",
      "Arrive at the goal! - agent:  4  - stage:  6\n",
      "Arrive at the goal! - agent:  2  - stage:  6\n",
      "Arrive at the goal! - agent:  6  - stage:  6\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Rank-0: policy_step=397016, average_reward_env_0=127.60761298507805\n",
      "Arrive at the goal! - agent:  4  - stage:  6\n",
      "Collision detected between /World/Worker_4 and /World/Worker_2\n",
      "Collision detected between /World/Worker_4 and /World/Worker_2\n",
      "Collision detected between /World/Worker_4 and /World/Worker_2\n",
      "Collision detected between /World/Worker_4 and /World/Worker_2\n",
      "Max step number - agent:  2\n",
      "Rank-0: policy_step=397256, average_reward_env_0=265.8256233795166\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Rank-0: policy_step=397296, average_reward_env_0=153.88451767803582\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Arrive at the goal! - agent:  8  - stage:  6\n",
      "Arrive at the goal! - agent:  7  - stage:  6\n",
      "Arrive at the goal! - agent:  5  - stage:  6\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Rank-0: policy_step=397640, average_reward_env_0=252.2231924068575\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Collision detected between /World/Worker_3 and /World/Warehouse/Table_09\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Rank-0: policy_step=397776, average_reward_env_0=186.8863252777085\n",
      "Arrive at the goal! - agent:  1  - stage:  6\n",
      "Arrive at the goal! - agent:  5  - stage:  6\n",
      "Collision detected between /World/Worker_4 and /World/Worker_1\n",
      "Collision detected between /World/Worker_4 and /World/Worker_1\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Rank-0: policy_step=397880, average_reward_env_0=265.25551460230656\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Rank-0: policy_step=397928, average_reward_env_0=240.34326297578792\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Arrive at the goal! - agent:  2  - stage:  6\n",
      "Arrive at the goal! - agent:  3  - stage:  6\n",
      "Arrive at the goal! - agent:  6  - stage:  6\n",
      "Max step number - agent:  6\n",
      "Rank-0: policy_step=398424, average_reward_env_0=204.41876108552776\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Rank-0: policy_step=398504, average_reward_env_0=256.68577661995334\n",
      "Max step number - agent:  8\n",
      "Rank-0: policy_step=398576, average_reward_env_0=250.04726554693568\n",
      "Arrive at the goal! - agent:  7  - stage:  6\n",
      "Arrive at the goal! - agent:  2  - stage:  6\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Rank-0: policy_step=398776, average_reward_env_0=166.91409022068518\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Rank-0: policy_step=398808, average_reward_env_0=177.593912650531\n",
      "Arrive at the goal! - agent:  3  - stage:  6\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Rank-0: policy_step=399040, average_reward_env_0=232.39500525773525\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Rank-0: policy_step=399224, average_reward_env_0=165.95664574709855\n",
      "Arrive at the goal! - agent:  4  - stage:  6\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Rank-0: policy_step=399320, average_reward_env_0=222.99427574195568\n",
      "Collision detected between /World/Worker_3 and /World/Worker_1\n",
      "Collision detected between /World/Worker_3 and /World/Worker_1\n",
      "Arrive at the goal! - agent:  6  - stage:  6\n",
      "Collision detected between /World/Worker_3 and /World/Worker_1\n",
      "Collision detected between /World/Worker_3 and /World/Worker_1\n",
      "Collision detected between /World/Worker_4 and /World/Warehouse/Table_03\n",
      "Arrive at the goal! - agent:  2  - stage:  6\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Rank-0: policy_step=399896, average_reward_env_0=188.5537458345082\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Arrive at the goal! - agent:  7  - stage:  6\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Rank-0: policy_step=400088, average_reward_env_0=231.6010921078033\n",
      "Max step number - agent:  3\n",
      "Rank-0: policy_step=400216, average_reward_env_0=122.73165063210732\n",
      "Arrive at the goal! - agent:  4  - stage:  6\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Rank-0: policy_step=400400, average_reward_env_0=259.9692889335827\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Arrive at the goal! - agent:  5  - stage:  6\n",
      "Arrive at the goal! - agent:  6  - stage:  6\n",
      "Arrive at the goal! - agent:  2  - stage:  6\n",
      "Arrive at the goal! - agent:  4  - stage:  6\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Rank-0: policy_step=400920, average_reward_env_1=234.73988887589178\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Max step number - agent:  7\n",
      "Rank-0: policy_step=400976, average_reward_env_0=209.9193422891781\n",
      "Arrive at the goal! - agent:  1  - stage:  6\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Arrive at the goal! - agent:  2  - stage:  6\n",
      "Collision detected between /World/Worker_4 and /World/Worker_1\n",
      "Collision detected between /World/Worker_4 and /World/Worker_1\n",
      "Collision detected between /World/Worker_4 and /World/Worker_1\n",
      "Collision detected between /World/Worker_4 and /World/Worker_1\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Rank-0: policy_step=401096, average_reward_env_0=193.91411626329904\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Rank-0: policy_step=401104, average_reward_env_0=174.0553847946048\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Max step number - agent:  4\n",
      "Rank-0: policy_step=401128, average_reward_env_0=244.08989476295363\n",
      "Arrive at the goal! - agent:  3  - stage:  6\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Rank-0: policy_step=401440, average_reward_env_0=263.32450310075916\n",
      "Arrive at the goal! - agent:  5  - stage:  6\n",
      "Arrive at the goal! - agent:  6  - stage:  6\n",
      "Arrive at the goal! - agent:  1  - stage:  6\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Rank-0: policy_step=401896, average_reward_env_0=200.99386949434512\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Rank-0: policy_step=402040, average_reward_env_0=240.74310634570745\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Arrive at the goal! - agent:  4  - stage:  6\n",
      "Arrive at the goal! - agent:  7  - stage:  6\n",
      "Arrive at the goal! - agent:  1  - stage:  6\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Rank-0: policy_step=402136, average_reward_env_0=181.4900823793684\n",
      "Arrive at the goal! - agent:  6  - stage:  6\n",
      "Arrive at the goal! - agent:  2  - stage:  6\n",
      "Arrive at the goal! - agent:  8  - stage:  6\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Rank-0: policy_step=402448, average_reward_env_0=249.4348442000286\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Rank-0: policy_step=402472, average_reward_env_0=127.9175636446277\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Rank-0: policy_step=402520, average_reward_env_0=170.62771737232254\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Arrive at the goal! - agent:  1  - stage:  6\n",
      "Arrive at the goal! - agent:  8  - stage:  6\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Rank-0: policy_step=402632, average_reward_env_0=126.8038900677909\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Collision detected between /World/Worker_2 and /World/Warehouse/Table_08\n",
      "Arrive at the goal! - agent:  8  - stage:  6\n",
      "Arrive at the goal! - agent:  8  - stage:  6\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Rank-0: policy_step=402720, average_reward_env_0=167.00478768837937\n",
      "Rank-0: policy_step=402720, average_reward_env_1=216.36536710030822\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Rank-0: policy_step=402768, average_reward_env_0=207.9723466067126\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Rank-0: policy_step=402856, average_reward_env_0=246.3262246885757\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Arrive at the goal! - agent:  2  - stage:  6\n",
      "Arrive at the goal! - agent:  4  - stage:  6\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Rank-0: policy_step=402944, average_reward_env_0=209.29199298302575\n",
      "Arrive at the goal! - agent:  6  - stage:  6\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Rank-0: policy_step=403080, average_reward_env_0=153.95414972389284\n",
      "Max step number - agent:  1\n",
      "Rank-0: policy_step=403104, average_reward_env_0=198.60238748263663\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Rank-0: policy_step=403192, average_reward_env_0=189.79932028148883\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Rank-0: policy_step=403248, average_reward_env_0=199.95549290967313\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Rank-0: policy_step=403424, average_reward_env_0=175.46081792732912\n",
      "Arrive at the goal! - agent:  3  - stage:  6\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Arrive at the goal! - agent:  4  - stage:  6\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Rank-0: policy_step=403696, average_reward_env_0=124.23280238095235\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Rank-0: policy_step=403896, average_reward_env_0=113.86523270613445\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Rank-0: policy_step=403912, average_reward_env_0=250.56436902027403\n",
      "Rank-0: policy_step=403912, average_reward_env_1=174.98741927354433\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Arrive at the goal! - agent:  6  - stage:  6\n",
      "Collision detected between /World/Worker_3 and /World/Warehouse/Rack_02\n",
      "Max step number - agent:  8\n",
      "Rank-0: policy_step=404128, average_reward_env_0=265.291568588422\n",
      "Arrive at the goal! - agent:  5  - stage:  6\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Rank-0: policy_step=404368, average_reward_env_0=263.4249511840454\n",
      "Arrive at the goal! - agent:  1  - stage:  6\n",
      "Arrive at the goal! - agent:  3  - stage:  6\n",
      "Arrive at the goal! - agent:  3  - stage:  6\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Rank-0: policy_step=404728, average_reward_env_0=172.98655914861172\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Rank-0: policy_step=404832, average_reward_env_0=164.2746990685624\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Rank-0: policy_step=404840, average_reward_env_0=122.80513477787794\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Arrive at the goal! - agent:  2  - stage:  6\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Rank-0: policy_step=404880, average_reward_env_1=197.73212349638106\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Arrive at the goal! - agent:  1  - stage:  6\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Rank-0: policy_step=405112, average_reward_env_0=114.7478608640604\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Rank-0: policy_step=405312, average_reward_env_0=259.39518617261956\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Rank-0: policy_step=405328, average_reward_env_0=265.5696879621587\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Arrive at the goal! - agent:  4  - stage:  6\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Rank-0: policy_step=405504, average_reward_env_0=173.2228083643343\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Arrive at the goal! - agent:  6  - stage:  6\n",
      "Rank-0: policy_step=405816, average_reward_env_0=240.73617538540614\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Rank-0: policy_step=405856, average_reward_env_0=187.8151276316509\n",
      "Arrive at the goal! - agent:  8  - stage:  6\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Rank-0: policy_step=405960, average_reward_env_0=169.7662398324745\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Arrive at the goal! - agent:  3  - stage:  6\n",
      "Arrive at the goal! - agent:  7  - stage:  6\n",
      "Arrive at the goal! - agent:  6  - stage:  6\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Rank-0: policy_step=406624, average_reward_env_0=185.02688916402494\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Arrive at the goal! - agent:  7  - stage:  6\n",
      "Arrive at the goal! - agent:  2  - stage:  6\n",
      "Collision detected between /World/Worker_4 and /World/Worker_2\n",
      "Collision detected between /World/Worker_4 and /World/Worker_2\n",
      "Arrive at the goal! - agent:  1  - stage:  6\n",
      "Collision detected between /World/Worker_4 and /World/Worker_2\n",
      "Collision detected between /World/Worker_4 and /World/Worker_2\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Rank-0: policy_step=406824, average_reward_env_0=119.25195006015637\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Arrive at the goal! - agent:  2  - stage:  6\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Arrive at the goal! - agent:  8  - stage:  6\n",
      "Arrive at the goal! - agent:  4  - stage:  6\n",
      "Arrive at the goal! - agent:  5  - stage:  6\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Rank-0: policy_step=407104, average_reward_env_0=186.70122429830016\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Arrive at the goal! - agent:  2  - stage:  6\n",
      "Arrive at the goal! - agent:  3  - stage:  6\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Rank-0: policy_step=407424, average_reward_env_0=241.47419925327583\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Rank-0: policy_step=407456, average_reward_env_0=278.64921076302704\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Rank-0: policy_step=407528, average_reward_env_0=256.7609696051775\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Arrive at the goal! - agent:  6  - stage:  6\n",
      "Rank-0: policy_step=407560, average_reward_env_0=237.23694305209628\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Rank-0: policy_step=407696, average_reward_env_0=157.94770412247865\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Rank-0: policy_step=408016, average_reward_env_0=257.89136574125763\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Rank-0: policy_step=408064, average_reward_env_0=277.2267209382229\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Rank-0: policy_step=408104, average_reward_env_0=157.54803220252478\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Rank-0: policy_step=408168, average_reward_env_0=264.39646738586583\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Rank-0: policy_step=408176, average_reward_env_0=127.82059793357327\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Collision detected between /World/Worker_4 and /World/Worker_1\n",
      "Collision detected between /World/Worker_4 and /World/Worker_1\n",
      "Collision detected between /World/Worker_4 and /World/Worker_1\n",
      "Collision detected between /World/Worker_4 and /World/Worker_1\n",
      "Collision detected between /World/Worker_1 and /World/Warehouse/Warehouse/Box_10634/SM_CardBoxB_01/SM_CardBoxB_01\n",
      "Arrive at the goal! - agent:  1  - stage:  6\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Rank-0: policy_step=408352, average_reward_env_0=126.77723757449166\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Arrive at the goal! - agent:  7  - stage:  6\n",
      "Arrive at the goal! - agent:  6  - stage:  6\n",
      "Max step number - agent:  6\n",
      "Rank-0: policy_step=408704, average_reward_env_0=201.4392593527753\n",
      "Collision detected between /World/Worker_2 and /World/Warehouse/Table_08\n",
      "Arrive at the goal! - agent:  2  - stage:  6\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Max step number - agent:  1\n",
      "Rank-0: policy_step=409056, average_reward_env_0=181.23480408833544\n",
      "Arrive at the goal! - agent:  5  - stage:  6\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Rank-0: policy_step=409128, average_reward_env_0=98.12730865667638\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Arrive at the goal! - agent:  8  - stage:  6\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Rank-0: policy_step=409208, average_reward_env_0=154.2942402841415\n",
      "Arrive at the goal! - agent:  8  - stage:  6\n",
      "Arrive at the goal! - agent:  8  - stage:  6\n",
      "Arrive at the goal! - agent:  4  - stage:  6\n",
      "Arrive at the goal! - agent:  5  - stage:  6\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Rank-0: policy_step=409424, average_reward_env_0=144.15270009171815\n",
      "Arrive at the goal! - agent:  7  - stage:  6\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Rank-0: policy_step=409496, average_reward_env_0=243.3544391093917\n",
      "Arrive at the goal! - agent:  6  - stage:  6\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Rank-0: policy_step=409816, average_reward_env_0=178.18099401381014\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Rank-0: policy_step=409968, average_reward_env_0=140.52915907843558\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Rank-0: policy_step=410088, average_reward_env_0=267.0142538138026\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Rank-0: policy_step=410120, average_reward_env_0=89.79876931891113\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Rank-0: policy_step=410216, average_reward_env_0=125.46231366414929\n",
      "Arrive at the goal! - agent:  7  - stage:  6\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Arrive at the goal! - agent:  6  - stage:  6\n",
      "Rank-0: policy_step=410288, average_reward_env_0=177.94111172775297\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Rank-0: policy_step=410304, average_reward_env_0=125.71681728929732\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Rank-0: policy_step=410384, average_reward_env_0=245.8467399961538\n",
      "Arrive at the goal! - agent:  2  - stage:  6\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Rank-0: policy_step=410440, average_reward_env_0=261.7650625556609\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Rank-0: policy_step=410600, average_reward_env_0=197.93068037695917\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Rank-0: policy_step=410736, average_reward_env_0=260.0977885382638\n",
      "Max step number - agent:  7\n",
      "Rank-0: policy_step=410896, average_reward_env_0=179.9976710373837\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Arrive at the goal! - agent:  4  - stage:  6\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Rank-0: policy_step=411000, average_reward_env_0=125.57280888560445\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Collision detected between /World/Worker_4 and /World/Warehouse/Table_09\n",
      "Arrive at the goal! - agent:  2  - stage:  6\n",
      "Max step number - agent:  4\n",
      "Rank-0: policy_step=411216, average_reward_env_0=267.0504637032302\n",
      "Arrive at the goal! - agent:  3  - stage:  6\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Rank-0: policy_step=411408, average_reward_env_0=228.34494393843968\n",
      "Rank-0: policy_step=411408, average_reward_env_1=80.71408202593818\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Rank-0: policy_step=411416, average_reward_env_0=173.02858029800146\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Rank-0: policy_step=411424, average_reward_env_0=193.66759571573127\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Rank-0: policy_step=411488, average_reward_env_0=249.7939487747026\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Arrive at the goal! - agent:  7  - stage:  6\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Rank-0: policy_step=411680, average_reward_env_0=218.5311037689254\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Collision detected between /World/Worker_4 and /World/Warehouse/Warehouse/SM_Rackshield_3841/SM_Rackshield_02\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Rank-0: policy_step=411832, average_reward_env_0=146.6668520759869\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Rank-0: policy_step=411928, average_reward_env_0=117.42447496325981\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Arrive at the goal! - agent:  7  - stage:  6\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Rank-0: policy_step=412056, average_reward_env_0=146.06554558442838\n",
      "Arrive at the goal! - agent:  4  - stage:  6\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Rank-0: policy_step=412168, average_reward_env_0=174.1340424005891\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Rank-0: policy_step=412224, average_reward_env_0=192.98568902093396\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Arrive at the goal! - agent:  5  - stage:  6\n",
      "Rank-0: policy_step=412312, average_reward_env_0=142.2255070496713\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Rank-0: policy_step=412400, average_reward_env_0=85.10459080439452\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Rank-0: policy_step=412424, average_reward_env_0=239.6222841287417\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Rank-0: policy_step=412552, average_reward_env_0=154.84085594167811\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Arrive at the goal! - agent:  5  - stage:  6\n",
      "Arrive at the goal! - agent:  1  - stage:  6\n",
      "Arrive at the goal! - agent:  2  - stage:  6\n",
      "Arrive at the goal! - agent:  5  - stage:  6\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Arrive at the goal! - agent:  1  - stage:  6\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Arrive at the goal! - agent:  7  - stage:  6\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Rank-0: policy_step=413192, average_reward_env_0=254.0415454325555\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Arrive at the goal! - agent:  2  - stage:  6\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Arrive at the goal! - agent:  3  - stage:  6\n",
      "Rank-0: policy_step=413320, average_reward_env_0=147.40259864041317\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Rank-0: policy_step=413440, average_reward_env_0=197.7308041334107\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Rank-0: policy_step=413528, average_reward_env_0=235.65277314346332\n",
      "Arrive at the goal! - agent:  5  - stage:  6\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Arrive at the goal! - agent:  1  - stage:  6\n",
      "Collision detected between /World/Worker_4 and /World/Warehouse/Warehouse/SM_Rackshield_3841/SM_Rackshield_02\n",
      "Arrive at the goal! - agent:  3  - stage:  6\n",
      "Arrive at the goal! - agent:  3  - stage:  6\n",
      "Arrive at the goal! - agent:  8  - stage:  6\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Rank-0: policy_step=413952, average_reward_env_0=141.40203762270642\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Arrive at the goal! - agent:  6  - stage:  6\n",
      "Max step number - agent:  7\n",
      "Rank-0: policy_step=414096, average_reward_env_0=194.91242889827768\n",
      "Arrive at the goal! - agent:  3  - stage:  6\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Rank-0: policy_step=414200, average_reward_env_0=193.99547627982736\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Rank-0: policy_step=414232, average_reward_env_0=195.34581378514576\n",
      "Arrive at the goal! - agent:  4  - stage:  6\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Rank-0: policy_step=414408, average_reward_env_0=217.33146027900216\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Rank-0: policy_step=414440, average_reward_env_0=167.60200865645228\n",
      "Arrive at the goal! - agent:  1  - stage:  6\n",
      "Arrive at the goal! - agent:  6  - stage:  6\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Rank-0: policy_step=414936, average_reward_env_0=131.5857031666814\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Arrive at the goal! - agent:  1  - stage:  6\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Rank-0: policy_step=415248, average_reward_env_0=233.7360012360253\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Arrive at the goal! - agent:  7  - stage:  6\n",
      "Arrive at the goal! - agent:  6  - stage:  6\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Rank-0: policy_step=415432, average_reward_env_0=108.10044240549033\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Arrive at the goal! - agent:  8  - stage:  6\n",
      "Arrive at the goal! - agent:  2  - stage:  6\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Rank-0: policy_step=415536, average_reward_env_0=100.52047387657909\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Rank-0: policy_step=415688, average_reward_env_0=112.37644560472212\n",
      "Max step number - agent:  6\n",
      "Rank-0: policy_step=415768, average_reward_env_0=182.02137522058945\n",
      "Arrive at the goal! - agent:  4  - stage:  6\n",
      "Arrive at the goal! - agent:  8  - stage:  6\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Rank-0: policy_step=416032, average_reward_env_0=238.66438500298372\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Rank-0: policy_step=416040, average_reward_env_0=221.35537238573042\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Arrive at the goal! - agent:  6  - stage:  6\n",
      "Collision detected between /World/Worker_3 and /World/Warehouse/Warehouse/GroundPlane/CollisionPlane\n",
      "Arrive at the goal! - agent:  2  - stage:  6\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Rank-0: policy_step=416136, average_reward_env_0=181.30744751430814\n",
      "Arrive at the goal! - agent:  1  - stage:  6\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Rank-0: policy_step=416200, average_reward_env_0=103.11820630322667\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Rank-0: policy_step=416280, average_reward_env_0=101.95121875799414\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Max step number - agent:  1\n",
      "Rank-0: policy_step=416528, average_reward_env_0=157.26810410454826\n",
      "Arrive at the goal! - agent:  3  - stage:  6\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Rank-0: policy_step=416592, average_reward_env_0=180.0395414788102\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Arrive at the goal! - agent:  5  - stage:  6\n",
      "Arrive at the goal! - agent:  6  - stage:  6\n",
      "Arrive at the goal! - agent:  4  - stage:  6\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Arrive at the goal! - agent:  8  - stage:  6\n",
      "Arrive at the goal! - agent:  7  - stage:  6\n",
      "Arrive at the goal! - agent:  4  - stage:  6\n",
      "Arrive at the goal! - agent:  6  - stage:  6\n",
      "Arrive at the goal! - agent:  6  - stage:  6\n",
      "Arrive at the goal! - agent:  2  - stage:  6\n",
      "Max step number - agent:  7\n",
      "Rank-0: policy_step=417432, average_reward_env_0=217.923009585165\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Rank-0: policy_step=417456, average_reward_env_0=239.5256743042471\n",
      "Arrive at the goal! - agent:  3  - stage:  6\n",
      "Max step number - agent:  2\n",
      "Rank-0: policy_step=417640, average_reward_env_0=186.68356295951912\n",
      "Arrive at the goal! - agent:  6  - stage:  6\n",
      "Arrive at the goal! - agent:  1  - stage:  6\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Arrive at the goal! - agent:  7  - stage:  6\n",
      "Collision detected between /World/Worker_2 and /World/Warehouse/Warehouse/SM_FloorDecal_StripeFull_4m78/SM_FloorDecal_StripeFull_4m\n",
      "Arrive at the goal! - agent:  1  - stage:  6\n",
      "Arrive at the goal! - agent:  8  - stage:  6\n",
      "Arrive at the goal! - agent:  3  - stage:  6\n",
      "Arrive at the goal! - agent:  4  - stage:  6\n",
      "Arrive at the goal! - agent:  5  - stage:  6\n",
      "Arrive at the goal! - agent:  6  - stage:  6\n",
      "Collision detected between /World/Worker_3 and /World/Worker_2\n",
      "Collision detected between /World/Worker_3 and /World/Worker_2\n",
      "Collision detected between /World/Worker_3 and /World/Worker_2\n",
      "Collision detected between /World/Worker_3 and /World/Worker_2\n",
      "Arrive at the goal! - agent:  2  - stage:  6\n",
      "Max step number - agent:  5\n",
      "Rank-0: policy_step=418888, average_reward_env_0=135.74325126106476\n",
      "Arrive at the goal! - agent:  1  - stage:  6\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Rank-0: policy_step=419032, average_reward_env_0=155.95139159553193\n",
      "Arrive at the goal! - agent:  8  - stage:  6\n",
      "Max step number - agent:  8\n",
      "Rank-0: policy_step=419248, average_reward_env_0=227.89182028138578\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Rank-0: policy_step=419256, average_reward_env_0=125.18678292646902\n",
      "Collision detected between /World/Worker_3 and /World/Warehouse/Table_08\n",
      "Collision detected between /World/Worker_3 and /World/Warehouse/Rack_04\n",
      "Arrive at the goal! - agent:  4  - stage:  6\n",
      "Arrive at the goal! - agent:  6  - stage:  6\n",
      "Arrive at the goal! - agent:  8  - stage:  6\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Rank-0: policy_step=419584, average_reward_env_0=121.65427529510896\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Arrive at the goal! - agent:  8  - stage:  6\n",
      "Max step number - agent:  6\n",
      "Rank-0: policy_step=419816, average_reward_env_0=214.82042079024532\n",
      "Arrive at the goal! - agent:  5  - stage:  6\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Rank-0: policy_step=420040, average_reward_env_0=111.36426082751613\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Arrive at the goal! - agent:  2  - stage:  6\n",
      "Arrive at the goal! - agent:  7  - stage:  6\n",
      "Collision detected between /World/Worker_3 and /World/Worker_1\n",
      "Collision detected between /World/Worker_3 and /World/Worker_1\n",
      "Collision detected between /World/Worker_3 and /World/Worker_1\n",
      "Collision detected between /World/Worker_3 and /World/Worker_1\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Rank-0: policy_step=420288, average_reward_env_0=212.51242247362154\n",
      "Arrive at the goal! - agent:  4  - stage:  6\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Rank-0: policy_step=420552, average_reward_env_0=141.7296957623231\n",
      "Arrive at the goal! - agent:  5  - stage:  6\n",
      "Max step number - agent:  7\n",
      "Rank-0: policy_step=420632, average_reward_env_0=214.81857892261607\n",
      "Max step number - agent:  4\n",
      "Rank-0: policy_step=420656, average_reward_env_0=249.15143937572626\n",
      "Max step number - agent:  2\n",
      "Rank-0: policy_step=420840, average_reward_env_0=209.92521356381687\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Rank-0: policy_step=421176, average_reward_env_0=113.61987982118097\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Rank-0: policy_step=421256, average_reward_env_0=211.08656324193998\n",
      "Arrive at the goal! - agent:  7  - stage:  6\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Rank-0: policy_step=421304, average_reward_env_0=112.9136937498931\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Arrive at the goal! - agent:  8  - stage:  6\n",
      "Arrive at the goal! - agent:  2  - stage:  6\n",
      "Arrive at the goal! - agent:  4  - stage:  6\n",
      "Arrive at the goal! - agent:  6  - stage:  6\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Rank-0: policy_step=421816, average_reward_env_0=107.32466849119365\n",
      "Arrive at the goal! - agent:  5  - stage:  6\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Max step number - agent:  5\n",
      "Rank-0: policy_step=422088, average_reward_env_0=256.8896523726578\n",
      "Rank-0: policy_step=422088, average_reward_env_1=138.66826602467196\n",
      "Arrive at the goal! - agent:  6  - stage:  6\n",
      "Arrive at the goal! - agent:  7  - stage:  6\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Rank-0: policy_step=422360, average_reward_env_0=256.0880290882949\n",
      "Arrive at the goal! - agent:  1  - stage:  6\n",
      "Collision detected between /World/Worker_4 and /World/Warehouse/Table_08\n",
      "Arrive at the goal! - agent:  8  - stage:  6\n",
      "Arrive at the goal! - agent:  2  - stage:  6\n",
      "Arrive at the goal! - agent:  7  - stage:  6\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Rank-0: policy_step=422632, average_reward_env_0=234.49528488948158\n",
      "Collision detected between /World/Worker_2 and /World/Warehouse/Rack_04\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Rank-0: policy_step=422712, average_reward_env_0=107.62002841292113\n",
      "Rank-0: policy_step=422712, average_reward_env_1=203.59241800043534\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Arrive at the goal! - agent:  6  - stage:  6\n",
      "Arrive at the goal! - agent:  3  - stage:  6\n",
      "Arrive at the goal! - agent:  3  - stage:  6\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Arrive at the goal! - agent:  1  - stage:  6\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collision detected between /World/Worker_2 and /World/Warehouse/Table_09\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Rank-0: policy_step=423216, average_reward_env_0=142.271246455814\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Rank-0: policy_step=423272, average_reward_env_0=197.13546451541578\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Arrive at the goal! - agent:  1  - stage:  6\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Rank-0: policy_step=423424, average_reward_env_0=94.41333035297244\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Rank-0: policy_step=423448, average_reward_env_0=248.35685225040882\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Max step number - agent:  8\n",
      "Rank-0: policy_step=423496, average_reward_env_0=233.66272775158723\n",
      "Arrive at the goal! - agent:  5  - stage:  6\n",
      "Collision detected between /World/Worker_2 and /World/Worker_1\n",
      "Collision detected between /World/Worker_2 and /World/Worker_1\n",
      "Max step number - agent:  1\n",
      "Arrive at the goal! - agent:  2  - stage:  6\n",
      "Rank-0: policy_step=423752, average_reward_env_0=169.31444962321547\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Max step number - agent:  2\n",
      "Rank-0: policy_step=424040, average_reward_env_0=207.94270761596036\n",
      "Arrive at the goal! - agent:  6  - stage:  6\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Rank-0: policy_step=424376, average_reward_env_0=237.2399453783026\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Collision detected between /World/Worker_4 and /World/Warehouse/Warehouse/SM_Rackshield_3841/SM_Rackshield_02\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Rank-0: policy_step=424464, average_reward_env_0=249.32298894516512\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Rank-0: policy_step=424632, average_reward_env_0=179.2710793872644\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Rank-0: policy_step=424720, average_reward_env_0=236.960332813491\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Rank-0: policy_step=424880, average_reward_env_0=88.49823349228701\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Rank-0: policy_step=424888, average_reward_env_0=169.22488584314902\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Arrive at the goal! - agent:  2  - stage:  6\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Rank-0: policy_step=425096, average_reward_env_0=178.31476289654537\n",
      "Arrive at the goal! - agent:  6  - stage:  6\n",
      "Arrive at the goal! - agent:  1  - stage:  6\n",
      "Arrive at the goal! - agent:  2  - stage:  6\n",
      "Arrive at the goal! - agent:  3  - stage:  6\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Rank-0: policy_step=425592, average_reward_env_0=145.995919347411\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Arrive at the goal! - agent:  8  - stage:  6\n",
      "Collision detected between /World/Worker_1 and /World/Warehouse/Table_09\n",
      "Arrive at the goal! - agent:  5  - stage:  6\n",
      "Arrive at the goal! - agent:  1  - stage:  6\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collision detected between /World/Worker_3 and /World/Worker_1\n",
      "Collision detected between /World/Worker_3 and /World/Worker_1\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Rank-0: policy_step=425976, average_reward_env_0=179.58155640518495\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collision detected between /World/Worker_1 and /World/Warehouse/Warehouse/SM_RackFrame_3840/SM_RackFrame_03/SM_RackFrame_03/Section1\n",
      "Collision detected between /World/Worker_1 and /World/Warehouse/Warehouse/SM_Rackshield_3841/SM_Rackshield_02\n",
      "Arrive at the goal! - agent:  3  - stage:  6\n",
      "Arrive at the goal! - agent:  4  - stage:  6\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Rank-0: policy_step=426200, average_reward_env_0=138.5399130111995\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Arrive at the goal! - agent:  7  - stage:  6\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Rank-0: policy_step=426640, average_reward_env_0=248.98268335097643\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Max step number - agent:  8\n",
      "Rank-0: policy_step=426696, average_reward_env_0=230.63737990975287\n",
      "Arrive at the goal! - agent:  4  - stage:  6\n",
      "Arrive at the goal! - agent:  3  - stage:  6\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Rank-0: policy_step=426936, average_reward_env_0=125.38029098297308\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Rank-0: policy_step=427096, average_reward_env_0=184.7450943261623\n",
      "Arrive at the goal! - agent:  6  - stage:  6\n",
      "Arrive at the goal! - agent:  1  - stage:  6\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Rank-0: policy_step=427120, average_reward_env_1=123.87665590851994\n",
      "Max step number - agent:  2\n",
      "Rank-0: policy_step=427240, average_reward_env_0=221.89511818132297\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Rank-0: policy_step=427280, average_reward_env_0=249.41036117632567\n",
      "Arrive at the goal! - agent:  5  - stage:  6\n",
      "Collision detected between /World/Worker_2 and /World/Warehouse/Table_12\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Rank-0: policy_step=427496, average_reward_env_0=214.57428417953793\n",
      "Max step number - agent:  4\n",
      "Rank-0: policy_step=427664, average_reward_env_0=267.14199468242714\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Rank-0: policy_step=427704, average_reward_env_0=211.22788895932902\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Arrive at the goal! - agent:  8  - stage:  6\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Rank-0: policy_step=428064, average_reward_env_0=119.71971617735925\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Max step number - agent:  3\n",
      "Rank-0: policy_step=428080, average_reward_env_0=113.11372836856428\n",
      "Arrive at the goal! - agent:  1  - stage:  6\n",
      "Collision detected between /World/Worker_2 and /World/Warehouse/Rack_01\n",
      "Collision detected between /World/Worker_2 and /World/Warehouse/Rack_01\n",
      "Arrive at the goal! - agent:  7  - stage:  6\n",
      "Collision detected between /World/Worker_2 and /World/Warehouse/Forklift_B01_PR_V_NVD_01\n",
      "Collision detected between /World/Worker_2 and /World/Warehouse/Forklift_B01_PR_V_NVD_01\n",
      "Arrive at the goal! - agent:  2  - stage:  6\n",
      "Arrive at the goal! - agent:  8  - stage:  6\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Rank-0: policy_step=428640, average_reward_env_0=107.16332247715549\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Arrive at the goal! - agent:  8  - stage:  6\n",
      "Collision detected between /World/Worker_1 and /World/Warehouse/Warehouse/SM_Rackshield_43/SM_Rackshield_02\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Rank-0: policy_step=428880, average_reward_env_0=194.78144979180914\n",
      "Collision detected between /World/Worker_1 and /World/Warehouse/Rack_01\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Rank-0: policy_step=428976, average_reward_env_0=179.68412101092343\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Arrive at the goal! - agent:  6  - stage:  6\n",
      "Arrive at the goal! - agent:  2  - stage:  6\n",
      "Arrive at the goal! - agent:  5  - stage:  6\n",
      "Arrive at the goal! - agent:  8  - stage:  6\n",
      "Collision detected between /World/Worker_3 and /World/Warehouse/Warehouse/SM_Rackshield_2984/SM_Rackshield_02\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Rank-0: policy_step=429504, average_reward_env_0=121.54765293446766\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Collision detected between /World/Worker_2 and /World/Warehouse/Table_09\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Rank-0: policy_step=429560, average_reward_env_0=259.74919318167025\n",
      "Arrive at the goal! - agent:  4  - stage:  6\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Collision detected between /World/Worker_4 and /World/Worker_1\n",
      "Collision detected between /World/Worker_4 and /World/Worker_1\n",
      "Collision detected between /World/Worker_4 and /World/Warehouse/Rack_01\n",
      "Max step number - agent:  8\n",
      "Rank-0: policy_step=429896, average_reward_env_0=236.52724829578452\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Arrive at the goal! - agent:  3  - stage:  6\n",
      "Arrive at the goal! - agent:  6  - stage:  6\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Rank-0: policy_step=430216, average_reward_env_0=115.07892456160515\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Rank-0: policy_step=430296, average_reward_env_0=123.03370808697399\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Rank-0: policy_step=430320, average_reward_env_0=246.2219709923812\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Arrive at the goal! - agent:  1  - stage:  6\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Rank-0: policy_step=430600, average_reward_env_0=225.57537715459702\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Arrive at the goal! - agent:  8  - stage:  6\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Rank-0: policy_step=430728, average_reward_env_0=177.3708021161113\n",
      "Arrive at the goal! - agent:  8  - stage:  6\n",
      "Arrive at the goal! - agent:  1  - stage:  6\n",
      "Arrive at the goal! - agent:  6  - stage:  6\n",
      "Max step number - agent:  2\n",
      "Rank-0: policy_step=430912, average_reward_env_0=207.224423417907\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Rank-0: policy_step=430976, average_reward_env_0=204.76057550626493\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Arrive at the goal! - agent:  5  - stage:  6\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Rank-0: policy_step=431272, average_reward_env_0=232.236265594906\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Arrive at the goal! - agent:  8  - stage:  6\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Rank-0: policy_step=431344, average_reward_env_0=185.75822048860803\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Rank-0: policy_step=431608, average_reward_env_0=170.07796936600673\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Arrive at the goal! - agent:  3  - stage:  6\n",
      "Arrive at the goal! - agent:  1  - stage:  6\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Rank-0: policy_step=431848, average_reward_env_0=114.07510714354886\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Rank-0: policy_step=431904, average_reward_env_0=237.8921728155109\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Arrive at the goal! - agent:  2  - stage:  6\n",
      "Max step number - agent:  1\n",
      "Rank-0: policy_step=432080, average_reward_env_0=217.7843569093732\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Rank-0: policy_step=432160, average_reward_env_0=225.30822462859544\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Arrive at the goal! - agent:  5  - stage:  6\n",
      "Collision detected between /World/Worker_1 and /World/Warehouse/Warehouse/Box_3581/SM_CardBoxB_01/SM_CardBoxB_01\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Rank-0: policy_step=432376, average_reward_env_0=239.5958519715007\n",
      "Arrive at the goal! - agent:  1  - stage:  6\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Rank-0: policy_step=432528, average_reward_env_0=183.71034085368285\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Arrive at the goal! - agent:  7  - stage:  6\n",
      "Arrive at the goal! - agent:  3  - stage:  6\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Rank-0: policy_step=432688, average_reward_env_0=181.273036973485\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Rank-0: policy_step=432728, average_reward_env_0=226.6860545980799\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Rank-0: policy_step=432808, average_reward_env_0=255.83698040149338\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Rank-0: policy_step=432848, average_reward_env_0=174.0879160350083\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Arrive at the goal! - agent:  1  - stage:  6\n",
      "Arrive at the goal! - agent:  3  - stage:  6\n",
      "Arrive at the goal! - agent:  3  - stage:  6\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Rank-0: policy_step=433320, average_reward_env_0=229.42752932316537\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Arrive at the goal! - agent:  7  - stage:  6\n",
      "Max step number - agent:  5\n",
      "Rank-0: policy_step=433504, average_reward_env_0=145.30567664889938\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Rank-0: policy_step=433616, average_reward_env_0=169.06777786760713\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Arrive at the goal! - agent:  8  - stage:  6\n",
      "Arrive at the goal! - agent:  1  - stage:  6\n",
      "Arrive at the goal! - agent:  7  - stage:  6\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Rank-0: policy_step=433960, average_reward_env_0=146.03038630992\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Rank-0: policy_step=434136, average_reward_env_0=205.89888988623596\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Arrive at the goal! - agent:  1  - stage:  6\n",
      "Arrive at the goal! - agent:  3  - stage:  6\n",
      "Arrive at the goal! - agent:  4  - stage:  6\n",
      "Arrive at the goal! - agent:  2  - stage:  6\n",
      "Arrive at the goal! - agent:  6  - stage:  6\n",
      "Arrive at the goal! - agent:  5  - stage:  6\n",
      "Collision detected between /World/Worker_2 and /World/Warehouse/Table_08\n",
      "Arrive at the goal! - agent:  3  - stage:  6\n",
      "Arrive at the goal! - agent:  7  - stage:  6\n",
      "Arrive at the goal! - agent:  1  - stage:  6\n",
      "Arrive at the goal! - agent:  2  - stage:  6\n",
      "Arrive at the goal! - agent:  2  - stage:  6\n",
      "Collision detected between /World/Worker_1 and /World/Warehouse/Table_03\n",
      "Arrive at the goal! - agent:  2  - stage:  6\n",
      "Max step number - agent:  7\n",
      "Rank-0: policy_step=434816, average_reward_env_0=191.18025315946224\n",
      "Arrive at the goal! - agent:  2  - stage:  6\n",
      "Arrive at the goal! - agent:  4  - stage:  6\n",
      "Arrive at the goal! - agent:  8  - stage:  6\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Max step number - agent:  3\n",
      "Rank-0: policy_step=435056, average_reward_env_0=150.07452545660465\n",
      "Arrive at the goal! - agent:  5  - stage:  6\n",
      "Arrive at the goal! - agent:  6  - stage:  6\n",
      "Arrive at the goal! - agent:  5  - stage:  6\n",
      "Arrive at the goal! - agent:  2  - stage:  6\n",
      "Max step number - agent:  1\n",
      "Rank-0: policy_step=435280, average_reward_env_0=250.05804035945394\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Rank-0: policy_step=435328, average_reward_env_0=219.96856380849758\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Arrive at the goal! - agent:  4  - stage:  6\n",
      "Arrive at the goal! - agent:  5  - stage:  6\n",
      "Arrive at the goal! - agent:  1  - stage:  6\n",
      "Arrive at the goal! - agent:  2  - stage:  6\n",
      "Max step number - agent:  4\n",
      "Rank-0: policy_step=435576, average_reward_env_0=248.20566140994154\n",
      "Arrive at the goal! - agent:  4  - stage:  6\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Rank-0: policy_step=435688, average_reward_env_0=252.39103635415574\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Rank-0: policy_step=435704, average_reward_env_0=157.41161676497597\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Arrive at the goal! - agent:  3  - stage:  6\n",
      "Arrive at the goal! - agent:  8  - stage:  6\n",
      "Arrive at the goal! - agent:  7  - stage:  6\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Rank-0: policy_step=435992, average_reward_env_0=245.07596888737663\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Rank-0: policy_step=436064, average_reward_env_0=199.23295026248968\n",
      "Arrive at the goal! - agent:  1  - stage:  6\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Rank-0: policy_step=436112, average_reward_env_0=153.77621102825367\n",
      "Collision detected between /World/Worker_3 and /World/Warehouse/Warehouse/SM_FloorDecal_StripeFull_4m102/SM_FloorDecal_StripeFull_4m\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Rank-0: policy_step=436208, average_reward_env_0=246.5521822361121\n",
      "Collision detected between /World/Worker_4 and /World/Warehouse/Warehouse/GroundPlane/CollisionPlane\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Rank-0: policy_step=436576, average_reward_env_0=225.25979665472553\n",
      "Arrive at the goal! - agent:  2  - stage:  6\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Rank-0: policy_step=436656, average_reward_env_0=150.82464175694082\n",
      "Collision detected between /World/Worker_4 and /World/Worker_3\n",
      "Collision detected between /World/Worker_4 and /World/Worker_3\n",
      "Collision detected between /World/Worker_4 and /World/Worker_3\n",
      "Collision detected between /World/Worker_4 and /World/Worker_3\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Rank-0: policy_step=436816, average_reward_env_0=142.45077415282947\n",
      "Max step number - agent:  2\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Rank-0: policy_step=436824, average_reward_env_0=205.02424717576343\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Arrive at the goal! - agent:  1  - stage:  6\n",
      "Arrive at the goal! - agent:  2  - stage:  6\n",
      "Arrive at the goal! - agent:  7  - stage:  6\n",
      "Collision detected between /World/Worker_4 and /World/Worker_1\n",
      "Collision detected between /World/Worker_4 and /World/Worker_1\n",
      "Arrive at the goal! - agent:  4  - stage:  6\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Rank-0: policy_step=437352, average_reward_env_0=143.88799669611615\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Rank-0: policy_step=437384, average_reward_env_0=145.6275511842168\n",
      "Arrive at the goal! - agent:  2  - stage:  6\n",
      "Arrive at the goal! - agent:  8  - stage:  6\n",
      "Arrive at the goal! - agent:  7  - stage:  6\n",
      "Arrive at the goal! - agent:  6  - stage:  6\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Max step number - agent:  7\n",
      "Rank-0: policy_step=438016, average_reward_env_0=195.50228768368595\n",
      "Arrive at the goal! - agent:  1  - stage:  6\n",
      "Arrive at the goal! - agent:  2  - stage:  6\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Rank-0: policy_step=438320, average_reward_env_0=185.64543457838204\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Collision detected between /World/Worker_4 and /World/Warehouse/Warehouse/SM_FloorDecal_StripeFull_4m102/SM_FloorDecal_StripeFull_4m\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Rank-0: policy_step=438448, average_reward_env_0=202.64200960098822\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Rank-0: policy_step=438456, average_reward_env_0=259.11445344744584\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Rank-0: policy_step=438576, average_reward_env_0=194.4553689437911\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Arrive at the goal! - agent:  4  - stage:  6\n",
      "Max step number - agent:  4\n",
      "Rank-0: policy_step=438776, average_reward_env_0=246.83612374055875\n",
      "Arrive at the goal! - agent:  3  - stage:  6\n",
      "Collision detected between /World/Worker_4 and /World/Worker_1\n",
      "Collision detected between /World/Worker_4 and /World/Worker_1\n",
      "Collision detected between /World/Worker_4 and /World/Worker_1\n",
      "Collision detected between /World/Worker_4 and /World/Worker_1\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Rank-0: policy_step=438920, average_reward_env_0=235.38094677161942\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Rank-0: policy_step=438968, average_reward_env_0=232.96323053128808\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Rank-0: policy_step=438976, average_reward_env_0=181.17108408389956\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Rank-0: policy_step=438992, average_reward_env_0=231.02900044764144\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collision detected between /World/Worker_2 and /World/Warehouse/Warehouse/GroundPlane/CollisionPlane\n",
      "Arrive at the goal! - agent:  8  - stage:  6\n",
      "Arrive at the goal! - agent:  5  - stage:  6\n",
      "Arrive at the goal! - agent:  1  - stage:  6\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Rank-0: policy_step=439720, average_reward_env_0=236.94022993911108\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Collision detected between /World/Worker_4 and /World/Warehouse/Warehouse/SM_FloorDecal_StripeFull_4m102/SM_FloorDecal_StripeFull_4m\n",
      "Arrive at the goal! - agent:  2  - stage:  6\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Max step number - agent:  2\n",
      "Rank-0: policy_step=440024, average_reward_env_0=217.062407828009\n",
      "Arrive at the goal! - agent:  4  - stage:  6\n",
      "Arrive at the goal! - agent:  8  - stage:  6\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Rank-0: policy_step=440112, average_reward_env_0=191.81206321741965\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Arrive at the goal! - agent:  3  - stage:  6\n",
      "Arrive at the goal! - agent:  5  - stage:  6\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Rank-0: policy_step=440320, average_reward_env_0=210.7342839094465\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Rank-0: policy_step=440360, average_reward_env_0=182.1890523997868\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Arrive at the goal! - agent:  2  - stage:  6\n",
      "Arrive at the goal! - agent:  2  - stage:  6\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Rank-0: policy_step=440512, average_reward_env_0=153.39842892608706\n",
      "Arrive at the goal! - agent:  1  - stage:  6\n",
      "Max step number - agent:  3\n",
      "Rank-0: policy_step=440560, average_reward_env_0=163.60112192989212\n",
      "Max step number - agent:  5\n",
      "Rank-0: policy_step=440584, average_reward_env_0=159.73034426727293\n",
      "Collision detected between /World/Worker_1 and /World/Warehouse/Table_08\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Rank-0: policy_step=440680, average_reward_env_0=240.92521505604978\n",
      "Rank-0: policy_step=440680, average_reward_env_1=155.25018835443134\n",
      "Arrive at the goal! - agent:  2  - stage:  6\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Rank-0: policy_step=440960, average_reward_env_0=208.79529167151514\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Rank-0: policy_step=441008, average_reward_env_0=154.49399254370093\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Arrive at the goal! - agent:  6  - stage:  6\n",
      "Arrive at the goal! - agent:  2  - stage:  6\n",
      "Arrive at the goal! - agent:  4  - stage:  6\n",
      "Collision detected between /World/Worker_4 and /World/Warehouse/Table_03\n",
      "Arrive at the goal! - agent:  4  - stage:  6\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Rank-0: policy_step=441448, average_reward_env_0=164.14725332477528\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Rank-0: policy_step=441488, average_reward_env_0=273.0331243656759\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Rank-0: policy_step=441616, average_reward_env_0=271.3578369168033\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Rank-0: policy_step=441696, average_reward_env_0=239.0639076078426\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Rank-0: policy_step=441760, average_reward_env_0=197.03800284956145\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Rank-0: policy_step=441808, average_reward_env_0=239.0357438855683\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Arrive at the goal! - agent:  6  - stage:  6\n",
      "Arrive at the goal! - agent:  2  - stage:  6\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Rank-0: policy_step=442168, average_reward_env_0=239.79803803398866\n",
      "Rank-0: policy_step=442168, average_reward_env_1=157.2812341102463\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Rank-0: policy_step=442416, average_reward_env_0=199.17042940086904\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Arrive at the goal! - agent:  8  - stage:  6\n",
      "Arrive at the goal! - agent:  5  - stage:  6\n",
      "Arrive at the goal! - agent:  2  - stage:  6\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Rank-0: policy_step=442704, average_reward_env_0=242.7759916681829\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Arrive at the goal! - agent:  2  - stage:  6\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Rank-0: policy_step=443136, average_reward_env_0=166.73734620173937\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Rank-0: policy_step=443192, average_reward_env_0=157.73720402876532\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Rank-0: policy_step=443256, average_reward_env_0=217.98374872514916\n",
      "Arrive at the goal! - agent:  7  - stage:  6\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Rank-0: policy_step=443296, average_reward_env_0=129.73421791616136\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Rank-0: policy_step=443368, average_reward_env_0=160.4670156635726\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Rank-0: policy_step=443440, average_reward_env_0=129.97242612959587\n",
      "Arrive at the goal! - agent:  2  - stage:  6\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Rank-0: policy_step=443640, average_reward_env_0=205.26231946552306\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Collision detected between /World/Worker_4 and /World/Warehouse/Table_08\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Rank-0: policy_step=443664, average_reward_env_0=250.71501088104\n",
      "Arrive at the goal! - agent:  1  - stage:  6\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Rank-0: policy_step=443680, average_reward_env_0=157.83355264594357\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Rank-0: policy_step=443888, average_reward_env_0=204.2440720474642\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Arrive at the goal! - agent:  2  - stage:  6\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Rank-0: policy_step=444248, average_reward_env_0=234.77727609398883\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Rank-0: policy_step=444264, average_reward_env_0=212.3000444411073\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Arrive at the goal! - agent:  6  - stage:  6\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Rank-0: policy_step=444376, average_reward_env_0=209.69754251040067\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Arrive at the goal! - agent:  5  - stage:  6\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Rank-0: policy_step=444512, average_reward_env_0=132.1483237457781\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Rank-0: policy_step=444536, average_reward_env_0=213.99097446183796\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Rank-0: policy_step=444760, average_reward_env_0=177.4298714777391\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Arrive at the goal! - agent:  7  - stage:  6\n",
      "Arrive at the goal! - agent:  2  - stage:  6\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Rank-0: policy_step=445064, average_reward_env_0=208.87527101484207\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Rank-0: policy_step=445136, average_reward_env_0=261.64875338622\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Rank-0: policy_step=445176, average_reward_env_0=164.73178235082003\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Rank-0: policy_step=445264, average_reward_env_0=213.72559243198083\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Rank-0: policy_step=445424, average_reward_env_0=190.18052129751467\n",
      "Arrive at the goal! - agent:  7  - stage:  6\n",
      "Arrive at the goal! - agent:  3  - stage:  6\n",
      "Collision detected between /World/Worker_3 and /World/Warehouse/Warehouse/GroundPlane/CollisionPlane\n",
      "Arrive at the goal! - agent:  5  - stage:  6\n",
      "Arrive at the goal! - agent:  6  - stage:  6\n",
      "Arrive at the goal! - agent:  1  - stage:  6\n",
      "Arrive at the goal! - agent:  8  - stage:  6\n",
      "Arrive at the goal! - agent:  7  - stage:  6\n",
      "Arrive at the goal! - agent:  2  - stage:  6\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Arrive at the goal! - agent:  4  - stage:  6\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Rank-0: policy_step=446152, average_reward_env_0=219.57844749019816\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Rank-0: policy_step=446192, average_reward_env_0=142.0498570713688\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Collision detected between /World/Worker_3 and /World/Warehouse/Table_09\n",
      "Collision detected between /World/Worker_3 and /World/Warehouse/Warehouse/SM_RackShelf_141/SM_RackShelf_01/SM_RackShelf_01/Section2\n",
      "Collision detected between /World/Worker_3 and /World/Warehouse/Warehouse/SM_RackShelf_138/SM_RackShelf_01/SM_RackShelf_01/Section2\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Rank-0: policy_step=446384, average_reward_env_0=210.0291414335545\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Rank-0: policy_step=446392, average_reward_env_0=117.66043183378932\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Rank-0: policy_step=446440, average_reward_env_0=225.97185440884104\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Rank-0: policy_step=446456, average_reward_env_0=197.0572963049301\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Arrive at the goal! - agent:  5  - stage:  6\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Rank-0: policy_step=446584, average_reward_env_0=213.7200657000047\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Rank-0: policy_step=446680, average_reward_env_0=171.87515922509905\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collision detected between /World/Worker_2 and /World/Warehouse/Table_08\n",
      "Collision detected between /World/Worker_4 and /World/Worker_3\n",
      "Collision detected between /World/Worker_4 and /World/Worker_3\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Arrive at the goal! - agent:  4  - stage:  6\n",
      "Rank-0: policy_step=447168, average_reward_env_0=119.95437801560824\n",
      "Arrive at the goal! - agent:  4  - stage:  6\n",
      "Arrive at the goal! - agent:  3  - stage:  6\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Rank-0: policy_step=447616, average_reward_env_0=264.32624122067614\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Rank-0: policy_step=447624, average_reward_env_0=193.8232687112593\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Collision detected between /World/Worker_3 and /World/Warehouse/Warehouse/SM_FloorDecal_StripeFull_4m102/SM_FloorDecal_StripeFull_4m\n",
      "Collision detected between /World/Worker_3 and /World/Warehouse/Warehouse/SM_SignCVer_3946/SM_SignCVer_10\n",
      "Arrive at the goal! - agent:  4  - stage:  6\n",
      "Arrive at the goal! - agent:  6  - stage:  6\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Rank-0: policy_step=447696, average_reward_env_0=221.36370812836725\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Rank-0: policy_step=447744, average_reward_env_0=212.42766872614058\n",
      "Arrive at the goal! - agent:  8  - stage:  6\n",
      "Arrive at the goal! - agent:  7  - stage:  6\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Arrive at the goal! - agent:  3  - stage:  6\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Arrive at the goal! - agent:  2  - stage:  6\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Rank-0: policy_step=448192, average_reward_env_0=205.2864614931849\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Arrive at the goal! - agent:  7  - stage:  6\n",
      "Arrive at the goal! - agent:  6  - stage:  6\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Rank-0: policy_step=448560, average_reward_env_0=222.5915151795463\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Rank-0: policy_step=448616, average_reward_env_0=132.36514390002597\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Rank-0: policy_step=448736, average_reward_env_0=222.34936946918722\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Arrive at the goal! - agent:  4  - stage:  6\n",
      "Collision detected between /World/Worker_3 and /World/Worker_2\n",
      "Collision detected between /World/Worker_3 and /World/Worker_2\n",
      "Collision detected between /World/Worker_3 and /World/Worker_2\n",
      "Collision detected between /World/Worker_3 and /World/Worker_2\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Arrive at the goal! - agent:  5  - stage:  6\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Rank-0: policy_step=449120, average_reward_env_0=222.5801746871194\n",
      "Arrive at the goal! - agent:  2  - stage:  6\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Rank-0: policy_step=449216, average_reward_env_0=234.2967884243699\n",
      "Arrive at the goal! - agent:  7  - stage:  6\n",
      "Arrive at the goal! - agent:  8  - stage:  6\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Rank-0: policy_step=449400, average_reward_env_0=211.96351452933362\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Collision detected between /World/Worker_3 and /World/Warehouse/Warehouse/SM_WallA_6M18/SM_WallA_6M/SM_WallA_6M/Section0\n",
      "Collision detected between /World/Worker_3 and /World/Warehouse/Warehouse/SM_WallA_6M18/SM_WallA_6M/SM_WallA_6M/Section1\n",
      "Arrive at the goal! - agent:  8  - stage:  6\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Rank-0: policy_step=449696, average_reward_env_0=164.58712579686588\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Arrive at the goal! - agent:  3  - stage:  6\n",
      "Arrive at the goal! - agent:  5  - stage:  6\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Arrive at the goal! - agent:  1  - stage:  6\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Rank-0: policy_step=450040, average_reward_env_0=204.05347696049262\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Rank-0: policy_step=450096, average_reward_env_0=217.99814679547785\n",
      "Arrive at the goal! - agent:  4  - stage:  6\n",
      "Arrive at the goal! - agent:  2  - stage:  6\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Rank-0: policy_step=450472, average_reward_env_0=159.8288224313615\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Rank-0: policy_step=450488, average_reward_env_0=286.306045502718\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Collision detected between /World/Worker_3 and /World/Worker_2\n",
      "Collision detected between /World/Worker_3 and /World/Worker_2\n",
      "Collision detected between /World/Worker_3 and /World/Worker_2\n",
      "Collision detected between /World/Worker_3 and /World/Worker_2\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Rank-0: policy_step=450576, average_reward_env_0=284.9394783731629\n",
      "Arrive at the goal! - agent:  6  - stage:  6\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Rank-0: policy_step=450840, average_reward_env_0=141.78024514463607\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Arrive at the goal! - agent:  4  - stage:  6\n",
      "Max step number - agent:  7\n",
      "Rank-0: policy_step=450944, average_reward_env_1=238.40797433327162\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Rank-0: policy_step=450976, average_reward_env_0=211.49423705569484\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Rank-0: policy_step=451008, average_reward_env_0=274.6186223047451\n",
      "Arrive at the goal! - agent:  3  - stage:  6\n",
      "Arrive at the goal! - agent:  5  - stage:  6\n",
      "Arrive at the goal! - agent:  8  - stage:  6\n",
      "Arrive at the goal! - agent:  1  - stage:  6\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Rank-0: policy_step=451512, average_reward_env_0=126.5784706664306\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Rank-0: policy_step=451632, average_reward_env_0=223.2006651522927\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Rank-0: policy_step=451696, average_reward_env_0=234.75197397193665\n",
      "Arrive at the goal! - agent:  1  - stage:  6\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Rank-0: policy_step=452136, average_reward_env_0=234.21242958282124\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Rank-0: policy_step=452200, average_reward_env_0=204.95750601397128\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Collision detected between /World/Worker_1 and /World/Warehouse/Warehouse/GroundPlane/CollisionPlane\n",
      "Collision detected between /World/Worker_1 and /World/Warehouse/Warehouse/Box_29168/SM_CardBoxD_04/SM_CardBoxD_04\n",
      "Collision detected between /World/Worker_1 and /World/Warehouse/Warehouse/SM_PaletteA_4518/SM_PaletteA_01\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Rank-0: policy_step=452416, average_reward_env_0=149.63767013916004\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Arrive at the goal! - agent:  7  - stage:  6\n",
      "Arrive at the goal! - agent:  2  - stage:  6\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Rank-0: policy_step=452600, average_reward_env_0=125.02504090910959\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Arrive at the goal! - agent:  5  - stage:  6\n",
      "Arrive at the goal! - agent:  3  - stage:  6\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Arrive at the goal! - agent:  8  - stage:  6\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Rank-0: policy_step=453096, average_reward_env_0=153.02148291621992\n",
      "Max step number - agent:  5\n",
      "Rank-0: policy_step=453248, average_reward_env_0=202.37892386900603\n",
      "Arrive at the goal! - agent:  4  - stage:  6\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Rank-0: policy_step=453296, average_reward_env_1=228.137504715128\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Rank-0: policy_step=453392, average_reward_env_0=187.47806363363551\n",
      "Rank-0: policy_step=453392, average_reward_env_1=99.61946526993093\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Rank-0: policy_step=453472, average_reward_env_0=98.09027805998245\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Rank-0: policy_step=453536, average_reward_env_0=182.7727639146251\n",
      "Collision detected between /World/Worker_2 and /World/Warehouse/Warehouse/SM_Rackshield_3841/SM_Rackshield_02\n",
      "Arrive at the goal! - agent:  7  - stage:  6\n",
      "Arrive at the goal! - agent:  8  - stage:  6\n",
      "Arrive at the goal! - agent:  4  - stage:  6\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Rank-0: policy_step=453992, average_reward_env_0=153.5423144051241\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Arrive at the goal! - agent:  5  - stage:  6\n",
      "Max step number - agent:  7\n",
      "Rank-0: policy_step=454184, average_reward_env_0=211.28278166240977\n",
      "Max step number - agent:  2\n",
      "Rank-0: policy_step=454208, average_reward_env_0=288.8557670310464\n",
      "Arrive at the goal! - agent:  3  - stage:  6\n",
      "Arrive at the goal! - agent:  2  - stage:  6\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Rank-0: policy_step=454520, average_reward_env_0=229.90896477048648\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Arrive at the goal! - agent:  1  - stage:  6\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Rank-0: policy_step=454584, average_reward_env_0=90.84477841312946\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Rank-0: policy_step=454680, average_reward_env_0=212.12522076167542\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Arrive at the goal! - agent:  8  - stage:  6\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Rank-0: policy_step=454808, average_reward_env_0=85.44817952458625\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Arrive at the goal! - agent:  7  - stage:  6\n",
      "Arrive at the goal! - agent:  2  - stage:  6\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Rank-0: policy_step=454960, average_reward_env_0=217.94996107643453\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collision detected between /World/Worker_3 and /World/Warehouse/Warehouse/SM_Rackshield_2984/SM_Rackshield_02\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Rank-0: policy_step=455136, average_reward_env_0=238.6548135684551\n",
      "Arrive at the goal! - agent:  5  - stage:  6\n",
      "Arrive at the goal! - agent:  4  - stage:  6\n",
      "Arrive at the goal! - agent:  8  - stage:  6\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Rank-0: policy_step=455264, average_reward_env_0=227.83668277782755\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Arrive at the goal! - agent:  3  - stage:  6\n",
      "Arrive at the goal! - agent:  4  - stage:  6\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Rank-0: policy_step=455616, average_reward_env_0=167.04634585819923\n",
      "Arrive at the goal! - agent:  4  - stage:  6\n",
      "Arrive at the goal! - agent:  6  - stage:  6\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Arrive at the goal! - agent:  5  - stage:  6\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Rank-0: policy_step=456040, average_reward_env_0=197.84909462312112\n",
      "Arrive at the goal! - agent:  5  - stage:  6\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Rank-0: policy_step=456152, average_reward_env_0=158.71324860002912\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Rank-0: policy_step=456280, average_reward_env_0=152.9876262510022\n",
      "Rank-0: policy_step=456280, average_reward_env_1=78.02962801905345\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Arrive at the goal! - agent:  2  - stage:  6\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Rank-0: policy_step=456344, average_reward_env_0=178.09616594140948\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Rank-0: policy_step=456392, average_reward_env_0=77.78939652328422\n",
      "Rank-0: policy_step=456392, average_reward_env_1=203.83025639526815\n",
      "Max step number - agent:  5\n",
      "Rank-0: policy_step=456448, average_reward_env_0=230.06099385721262\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Rank-0: policy_step=456480, average_reward_env_0=158.69080901803983\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Arrive at the goal! - agent:  4  - stage:  6\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Arrive at the goal! - agent:  4  - stage:  6\n",
      "Rank-0: policy_step=456592, average_reward_env_0=128.44979497559797\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Arrive at the goal! - agent:  4  - stage:  6\n",
      "Collision detected between /World/Worker_4 and /World/Warehouse/Warehouse/SM_Rackshield_29/SM_Rackshield_02\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Rank-0: policy_step=457160, average_reward_env_0=167.7032374490799\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Collision detected between /World/Worker_4 and /World/Worker_3\n",
      "Collision detected between /World/Worker_4 and /World/Worker_3\n",
      "Collision detected between /World/Worker_4 and /World/Worker_3\n",
      "Collision detected between /World/Worker_4 and /World/Worker_3\n",
      "Max step number - agent:  2\n",
      "Rank-0: policy_step=457408, average_reward_env_0=313.7572879918461\n",
      "Arrive at the goal! - agent:  8  - stage:  6\n",
      "Arrive at the goal! - agent:  6  - stage:  6\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Rank-0: policy_step=457936, average_reward_env_0=133.4062162302455\n",
      "Arrive at the goal! - agent:  4  - stage:  6\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Arrive at the goal! - agent:  5  - stage:  6\n",
      "Collision detected between /World/Worker_3 and /World/Warehouse/Rack_01\n",
      "Max step number - agent:  4\n",
      "Rank-0: policy_step=458160, average_reward_env_0=249.75833261155796\n",
      "Arrive at the goal! - agent:  2  - stage:  6\n",
      "Collision detected between /World/Worker_3 and /World/Warehouse/Rack_01\n",
      "Collision detected between /World/Worker_3 and /World/Warehouse/Rack_01\n",
      "Arrive at the goal! - agent:  2  - stage:  6\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Rank-0: policy_step=458264, average_reward_env_1=83.82052227957033\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Rank-0: policy_step=458272, average_reward_env_0=161.09518552044125\n",
      "Arrive at the goal! - agent:  7  - stage:  6\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Rank-0: policy_step=458480, average_reward_env_0=311.21955496165725\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Rank-0: policy_step=458504, average_reward_env_0=80.55654337930318\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Rank-0: policy_step=458832, average_reward_env_0=239.6397883485311\n",
      "Arrive at the goal! - agent:  3  - stage:  6\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collision detected between /World/Worker_2 and /World/Worker_1\n",
      "Collision detected between /World/Worker_2 and /World/Worker_1\n",
      "Collision detected between /World/Worker_2 and /World/Worker_1\n",
      "Collision detected between /World/Worker_2 and /World/Worker_1\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Rank-0: policy_step=459064, average_reward_env_0=217.6502362548944\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Rank-0: policy_step=459264, average_reward_env_0=207.3546114491293\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Rank-0: policy_step=459296, average_reward_env_0=289.38203194212673\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Rank-0: policy_step=459344, average_reward_env_0=173.60747202708774\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Rank-0: policy_step=459584, average_reward_env_0=83.42390720365294\n",
      "Arrive at the goal! - agent:  8  - stage:  6\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Rank-0: policy_step=459848, average_reward_env_0=78.98807725935367\n",
      "Arrive at the goal! - agent:  1  - stage:  6\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Arrive at the goal! - agent:  3  - stage:  6\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Rank-0: policy_step=459984, average_reward_env_0=192.06411322428556\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Arrive at the goal! - agent:  2  - stage:  6\n",
      "Arrive at the goal! - agent:  4  - stage:  6\n",
      "Arrive at the goal! - agent:  4  - stage:  6\n",
      "Arrive at the goal! - agent:  7  - stage:  6\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Rank-0: policy_step=460512, average_reward_env_0=193.3098496469416\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Arrive at the goal! - agent:  4  - stage:  6\n",
      "Arrive at the goal! - agent:  8  - stage:  6\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Arrive at the goal! - agent:  3  - stage:  6\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Arrive at the goal! - agent:  6  - stage:  6\n",
      "Arrive at the goal! - agent:  7  - stage:  6\n",
      "Max step number - agent:  3\n",
      "Rank-0: policy_step=461136, average_reward_env_0=152.31774903871153\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Rank-0: policy_step=461152, average_reward_env_0=159.69928833042167\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Rank-0: policy_step=461208, average_reward_env_0=181.43933533720912\n",
      "Arrive at the goal! - agent:  5  - stage:  6\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Rank-0: policy_step=461272, average_reward_env_0=74.43349726467028\n",
      "Arrive at the goal! - agent:  3  - stage:  6\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Rank-0: policy_step=461328, average_reward_env_0=151.19830549431302\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Max step number - agent:  4\n",
      "Rank-0: policy_step=461360, average_reward_env_0=263.7309821966299\n",
      "Arrive at the goal! - agent:  2  - stage:  6\n",
      "Collision detected between /World/Worker_4 and /World/Worker_1\n",
      "Collision detected between /World/Worker_4 and /World/Worker_1\n",
      "Collision detected between /World/Worker_4 and /World/Worker_1\n",
      "Collision detected between /World/Worker_4 and /World/Worker_1\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Rank-0: policy_step=461512, average_reward_env_0=280.7449293640282\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Rank-0: policy_step=461528, average_reward_env_0=148.50513593104168\n",
      "Arrive at the goal! - agent:  2  - stage:  6\n",
      "Arrive at the goal! - agent:  4  - stage:  6\n",
      "Arrive at the goal! - agent:  7  - stage:  6\n",
      "Collision detected between /World/Worker_3 and /World/Warehouse/Table_08\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Rank-0: policy_step=462080, average_reward_env_0=81.25151630655623\n",
      "Arrive at the goal! - agent:  3  - stage:  6\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Rank-0: policy_step=462232, average_reward_env_0=135.08377121286884\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Rank-0: policy_step=462312, average_reward_env_0=258.18580285557533\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Rank-0: policy_step=462352, average_reward_env_0=117.12177903859973\n",
      "Rank-0: policy_step=462352, average_reward_env_1=195.80569407978143\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Rank-0: policy_step=462360, average_reward_env_0=133.07750180590457\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Arrive at the goal! - agent:  5  - stage:  6\n",
      "Arrive at the goal! - agent:  2  - stage:  6\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Rank-0: policy_step=462440, average_reward_env_0=72.2261500299715\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Rank-0: policy_step=462584, average_reward_env_0=205.6228907343089\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Rank-0: policy_step=462848, average_reward_env_0=108.71658852244576\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Rank-0: policy_step=462880, average_reward_env_0=268.4995063090311\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Rank-0: policy_step=462920, average_reward_env_0=132.94820765519032\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Arrive at the goal! - agent:  7  - stage:  6\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Rank-0: policy_step=463008, average_reward_env_0=232.03921613875363\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Arrive at the goal! - agent:  2  - stage:  6\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Rank-0: policy_step=463176, average_reward_env_0=109.54331213030788\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Rank-0: policy_step=463272, average_reward_env_0=250.236131540081\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Arrive at the goal! - agent:  4  - stage:  6\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Rank-0: policy_step=463432, average_reward_env_0=200.67416957702994\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Arrive at the goal! - agent:  6  - stage:  6\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Rank-0: policy_step=463592, average_reward_env_0=109.8509654565118\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Rank-0: policy_step=463608, average_reward_env_0=104.8094732573898\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Arrive at the goal! - agent:  3  - stage:  6\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Rank-0: policy_step=463824, average_reward_env_0=200.73418392556957\n",
      "Collision detected between /World/Worker_1 and /World/Warehouse/Warehouse/GroundPlane/CollisionPlane\n",
      "Arrive at the goal! - agent:  5  - stage:  6\n",
      "Arrive at the goal! - agent:  8  - stage:  6\n",
      "Arrive at the goal! - agent:  4  - stage:  6\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Rank-0: policy_step=463960, average_reward_env_1=82.23860211993278\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Arrive at the goal! - agent:  7  - stage:  6\n",
      "Arrive at the goal! - agent:  6  - stage:  6\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Rank-0: policy_step=464256, average_reward_env_0=237.81022844935106\n",
      "Arrive at the goal! - agent:  2  - stage:  6\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Arrive at the goal! - agent:  3  - stage:  6\n",
      "Arrive at the goal! - agent:  1  - stage:  6\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Rank-0: policy_step=464816, average_reward_env_0=207.85507337767064\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Arrive at the goal! - agent:  7  - stage:  6\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collision detected between /World/Worker_1 and /World/Warehouse/Table_09\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Arrive at the goal! - agent:  1  - stage:  6\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Rank-0: policy_step=465112, average_reward_env_0=88.50715440726512\n",
      "Arrive at the goal! - agent:  7  - stage:  6\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Rank-0: policy_step=465360, average_reward_env_0=261.55616361553547\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Arrive at the goal! - agent:  3  - stage:  6\n",
      "Arrive at the goal! - agent:  8  - stage:  6\n",
      "Arrive at the goal! - agent:  7  - stage:  6\n",
      "Arrive at the goal! - agent:  2  - stage:  6\n",
      "Arrive at the goal! - agent:  1  - stage:  6\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Arrive at the goal! - agent:  4  - stage:  6\n",
      "Max step number - agent:  8\n",
      "Rank-0: policy_step=466128, average_reward_env_0=131.73781814057986\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Rank-0: policy_step=466152, average_reward_env_0=267.0670898196377\n",
      "Arrive at the goal! - agent:  5  - stage:  6\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Rank-0: policy_step=466400, average_reward_env_0=245.95553143496406\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Rank-0: policy_step=466456, average_reward_env_0=86.66626905471003\n",
      "Max step number - agent:  3\n",
      "Rank-0: policy_step=466800, average_reward_env_0=114.92564773496547\n",
      "Max step number - agent:  1\n",
      "Rank-0: policy_step=466816, average_reward_env_0=126.5529458905464\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Rank-0: policy_step=466872, average_reward_env_0=132.6676674284207\n",
      "Arrive at the goal! - agent:  5  - stage:  6\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Rank-0: policy_step=466968, average_reward_env_0=217.80973273794422\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collision detected between /World/Worker_4 and /World/Worker_2\n",
      "Collision detected between /World/Worker_4 and /World/Worker_2\n",
      "Collision detected between /World/Worker_4 and /World/Worker_2\n",
      "Collision detected between /World/Worker_4 and /World/Worker_2\n",
      "Collision detected between /World/Worker_4 and /World/Worker_3\n",
      "Collision detected between /World/Worker_4 and /World/Worker_3\n",
      "Collision detected between /World/Worker_4 and /World/Worker_3\n",
      "Collision detected between /World/Worker_4 and /World/Worker_3\n",
      "Max step number - agent:  4\n",
      "Rank-0: policy_step=467464, average_reward_env_0=257.5312418946749\n",
      "Arrive at the goal! - agent:  7  - stage:  6\n",
      "Collision detected between /World/Worker_3 and /World/Worker_2\n",
      "Collision detected between /World/Worker_3 and /World/Worker_2\n",
      "Collision detected between /World/Worker_3 and /World/Worker_2\n",
      "Collision detected between /World/Worker_3 and /World/Worker_2\n",
      "Arrive at the goal! - agent:  3  - stage:  6\n",
      "Arrive at the goal! - agent:  1  - stage:  6\n",
      "Collision detected between /World/Worker_4 and /World/Worker_3\n",
      "Collision detected between /World/Worker_4 and /World/Worker_3\n",
      "Collision detected between /World/Worker_4 and /World/Worker_3\n",
      "Collision detected between /World/Worker_4 and /World/Worker_3\n",
      "Arrive at the goal! - agent:  6  - stage:  6\n",
      "Arrive at the goal! - agent:  8  - stage:  6\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Max step number - agent:  5\n",
      "Rank-0: policy_step=468024, average_reward_env_0=226.71838083668246\n",
      "Arrive at the goal! - agent:  6  - stage:  6\n",
      "Arrive at the goal! - agent:  5  - stage:  6\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Rank-0: policy_step=468512, average_reward_env_0=135.64695752468592\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Rank-0: policy_step=468528, average_reward_env_0=131.80727157842176\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Rank-0: policy_step=468544, average_reward_env_0=102.89737399188847\n",
      "Arrive at the goal! - agent:  2  - stage:  6\n",
      "Arrive at the goal! - agent:  3  - stage:  6\n",
      "Arrive at the goal! - agent:  4  - stage:  6\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collision detected between /World/Worker_3 and /World/Worker_2\n",
      "Collision detected between /World/Worker_3 and /World/Worker_2\n",
      "Arrive at the goal! - agent:  1  - stage:  6\n",
      "Arrive at the goal! - agent:  7  - stage:  6\n",
      "Arrive at the goal! - agent:  1  - stage:  6\n",
      "Arrive at the goal! - agent:  6  - stage:  6\n",
      "Max step number - agent:  2\n",
      "Rank-0: policy_step=469600, average_reward_env_0=263.29119211254255\n",
      "Collision detected between /World/Worker_3 and /World/Worker_1\n",
      "Collision detected between /World/Worker_3 and /World/Worker_1\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Rank-0: policy_step=469864, average_reward_env_0=262.16631187711033\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Rank-0: policy_step=469872, average_reward_env_0=136.66777573565102\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Arrive at the goal! - agent:  3  - stage:  6\n",
      "Arrive at the goal! - agent:  1  - stage:  6\n",
      "Arrive at the goal! - agent:  5  - stage:  6\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Max step number - agent:  3\n",
      "Rank-0: policy_step=470000, average_reward_env_0=137.03614469447447\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Arrive at the goal! - agent:  6  - stage:  6\n",
      "Max step number - agent:  7\n",
      "Rank-0: policy_step=470168, average_reward_env_0=230.196402783013\n",
      "Arrive at the goal! - agent:  4  - stage:  6\n",
      "Arrive at the goal! - agent:  7  - stage:  6\n",
      "Arrive at the goal! - agent:  6  - stage:  6\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Rank-0: policy_step=470592, average_reward_env_0=136.10384667858537\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Rank-0: policy_step=470648, average_reward_env_0=104.00916027677715\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Max step number - agent:  4\n",
      "Rank-0: policy_step=470664, average_reward_env_0=275.28992598829\n",
      "Arrive at the goal! - agent:  8  - stage:  6\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Rank-0: policy_step=470848, average_reward_env_0=273.01874268342755\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Rank-0: policy_step=470912, average_reward_env_0=228.03473066252783\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Arrive at the goal! - agent:  4  - stage:  6\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Rank-0: policy_step=470952, average_reward_env_0=136.46715784696815\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Rank-0: policy_step=471048, average_reward_env_0=248.22081481953902\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Arrive at the goal! - agent:  7  - stage:  6\n",
      "Arrive at the goal! - agent:  8  - stage:  6\n",
      "Arrive at the goal! - agent:  3  - stage:  6\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Rank-0: policy_step=471280, average_reward_env_0=229.74763679329828\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Arrive at the goal! - agent:  2  - stage:  6\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Rank-0: policy_step=471368, average_reward_env_0=254.94953601986484\n",
      "Arrive at the goal! - agent:  6  - stage:  6\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Rank-0: policy_step=471800, average_reward_env_0=121.4531101551204\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Arrive at the goal! - agent:  8  - stage:  6\n",
      "Rank-0: policy_step=471832, average_reward_env_0=120.90956258744535\n",
      "Arrive at the goal! - agent:  6  - stage:  6\n",
      "Arrive at the goal! - agent:  4  - stage:  6\n",
      "Arrive at the goal! - agent:  7  - stage:  6\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Arrive at the goal! - agent:  8  - stage:  6\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Rank-0: policy_step=472152, average_reward_env_0=121.20695756826652\n",
      "Collision detected between /World/Worker_3 and /World/Worker_1\n",
      "Collision detected between /World/Worker_3 and /World/Worker_1\n",
      "Collision detected between /World/Worker_3 and /World/Worker_1\n",
      "Collision detected between /World/Worker_3 and /World/Worker_1\n",
      "Arrive at the goal! - agent:  5  - stage:  6\n",
      "Collision detected between /World/Worker_4 and /World/Worker_1\n",
      "Collision detected between /World/Worker_4 and /World/Worker_1\n",
      "Collision detected between /World/Worker_4 and /World/Worker_1\n",
      "Collision detected between /World/Worker_4 and /World/Worker_1\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Rank-0: policy_step=472504, average_reward_env_0=118.76711291591646\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Rank-0: policy_step=472616, average_reward_env_0=261.22801431334653\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Arrive at the goal! - agent:  2  - stage:  6\n",
      "Arrive at the goal! - agent:  6  - stage:  6\n",
      "Arrive at the goal! - agent:  6  - stage:  6\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collision detected between /World/Worker_3 and /World/Warehouse/Warehouse/SM_Rackshield_2984/SM_Rackshield_02\n",
      "Max step number - agent:  2\n",
      "Rank-0: policy_step=473072, average_reward_env_0=275.2859741806582\n",
      "Max step number - agent:  8\n",
      "Rank-0: policy_step=473080, average_reward_env_0=161.64738912560404\n",
      "Collision detected between /World/Worker_3 and /World/Warehouse/Warehouse/Box_43764/SM_CardBoxC_01/SM_CardBoxC_01\n",
      "Collision detected between /World/Worker_3 and /World/Warehouse/Warehouse/Box_43712/SM_CardBoxD_04/SM_CardBoxD_04\n",
      "Arrive at the goal! - agent:  7  - stage:  6\n",
      "Arrive at the goal! - agent:  3  - stage:  6\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Rank-0: policy_step=473224, average_reward_env_0=212.4191959016636\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Arrive at the goal! - agent:  1  - stage:  6\n",
      "Collision detected between /World/Worker_3 and /World/Warehouse/Warehouse/SM_Rackshield_3844/SM_Rackshield_02\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Rank-0: policy_step=473656, average_reward_env_0=277.2478241554785\n",
      "Rank-0: policy_step=473656, average_reward_env_1=261.6263756652239\n",
      "Arrive at the goal! - agent:  2  - stage:  6\n",
      "Arrive at the goal! - agent:  5  - stage:  6\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Rank-0: policy_step=473760, average_reward_env_0=114.69995525485143\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Max step number - agent:  3\n",
      "Rank-0: policy_step=473792, average_reward_env_0=154.5761384586858\n",
      "Max step number - agent:  6\n",
      "Rank-0: policy_step=473856, average_reward_env_0=132.32927484440066\n",
      "Arrive at the goal! - agent:  7  - stage:  6\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Rank-0: policy_step=474032, average_reward_env_0=267.3575365713989\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Arrive at the goal! - agent:  5  - stage:  6\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Rank-0: policy_step=474344, average_reward_env_0=114.66706113348944\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Arrive at the goal! - agent:  8  - stage:  6\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Rank-0: policy_step=474424, average_reward_env_0=124.62209599431982\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Rank-0: policy_step=474520, average_reward_env_0=194.01410017486353\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Rank-0: policy_step=474576, average_reward_env_0=263.91160004009265\n",
      "Arrive at the goal! - agent:  3  - stage:  6\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Rank-0: policy_step=474688, average_reward_env_0=113.47011844891249\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Arrive at the goal! - agent:  2  - stage:  6\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Rank-0: policy_step=474768, average_reward_env_0=193.43804616575238\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Rank-0: policy_step=474800, average_reward_env_0=236.01430562510828\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Rank-0: policy_step=474832, average_reward_env_0=117.83191783359928\n",
      "Arrive at the goal! - agent:  6  - stage:  6\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Rank-0: policy_step=474944, average_reward_env_0=290.8836720439965\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Rank-0: policy_step=475040, average_reward_env_0=168.7608175853486\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Rank-0: policy_step=475096, average_reward_env_0=287.952567982178\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Arrive at the goal! - agent:  1  - stage:  6\n",
      "Arrive at the goal! - agent:  4  - stage:  6\n",
      "Arrive at the goal! - agent:  7  - stage:  6\n",
      "Arrive at the goal! - agent:  7  - stage:  6\n",
      "Arrive at the goal! - agent:  5  - stage:  6\n",
      "Arrive at the goal! - agent:  2  - stage:  6\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Rank-0: policy_step=476016, average_reward_env_0=115.63926044057163\n",
      "Arrive at the goal! - agent:  6  - stage:  6\n",
      "Arrive at the goal! - agent:  8  - stage:  6\n",
      "Arrive at the goal! - agent:  2  - stage:  6\n",
      "Arrive at the goal! - agent:  4  - stage:  6\n",
      "Arrive at the goal! - agent:  6  - stage:  6\n",
      "Arrive at the goal! - agent:  3  - stage:  6\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Rank-0: policy_step=476488, average_reward_env_0=253.24795065093483\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Arrive at the goal! - agent:  7  - stage:  6\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Rank-0: policy_step=476544, average_reward_env_0=172.58065356196622\n",
      "Arrive at the goal! - agent:  3  - stage:  6\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Rank-0: policy_step=476976, average_reward_env_0=224.25144115957193\n",
      "Arrive at the goal! - agent:  5  - stage:  6\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Rank-0: policy_step=477096, average_reward_env_0=177.97696296940398\n",
      "Arrive at the goal! - agent:  2  - stage:  6\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Rank-0: policy_step=477168, average_reward_env_0=164.45121878386988\n",
      "Max step number - agent:  5\n",
      "Rank-0: policy_step=477240, average_reward_env_0=288.3279914599563\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Rank-0: policy_step=477272, average_reward_env_0=117.30553849499341\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Rank-0: policy_step=477528, average_reward_env_0=211.8932297525589\n",
      "Collision detected between /World/Worker_4 and /World/Warehouse/Rack_02\n",
      "Arrive at the goal! - agent:  6  - stage:  6\n",
      "Arrive at the goal! - agent:  2  - stage:  6\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Rank-0: policy_step=477912, average_reward_env_0=281.7690616273069\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Arrive at the goal! - agent:  1  - stage:  6\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Arrive at the goal! - agent:  3  - stage:  6\n",
      "Collision detected between /World/Worker_3 and /World/Warehouse/Table_12\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Rank-0: policy_step=478192, average_reward_env_0=237.86286965726933\n",
      "Rank-0: policy_step=478192, average_reward_env_1=168.05966595146933\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Arrive at the goal! - agent:  8  - stage:  6\n",
      "Rank-0: policy_step=478424, average_reward_env_0=121.83267615030847\n",
      "Arrive at the goal! - agent:  5  - stage:  6\n",
      "Arrive at the goal! - agent:  7  - stage:  6\n",
      "Collision detected between /World/Worker_4 and /World/Worker_3\n",
      "Collision detected between /World/Worker_4 and /World/Worker_3\n",
      "Collision detected between /World/Worker_4 and /World/Worker_3\n",
      "Collision detected between /World/Worker_4 and /World/Worker_3\n",
      "Arrive at the goal! - agent:  4  - stage:  6\n",
      "Arrive at the goal! - agent:  5  - stage:  6\n",
      "Arrive at the goal! - agent:  1  - stage:  6\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Rank-0: policy_step=478848, average_reward_env_0=188.5527459780355\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Rank-0: policy_step=478928, average_reward_env_0=235.4487614402304\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Arrive at the goal! - agent:  3  - stage:  6\n",
      "Arrive at the goal! - agent:  2  - stage:  6\n",
      "Arrive at the goal! - agent:  2  - stage:  6\n",
      "Max step number - agent:  1\n",
      "Rank-0: policy_step=479216, average_reward_env_0=132.1263692100965\n",
      "Collision detected between /World/Worker_1 and /World/Warehouse/Rack_02\n",
      "Arrive at the goal! - agent:  7  - stage:  6\n",
      "Arrive at the goal! - agent:  8  - stage:  6\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Rank-0: policy_step=479616, average_reward_env_0=221.44071905128072\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Rank-0: policy_step=479632, average_reward_env_0=293.5187690520725\n",
      "Arrive at the goal! - agent:  3  - stage:  6\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Rank-0: policy_step=479712, average_reward_env_0=230.24074422207894\n",
      "Collision detected between /World/Worker_3 and /World/Warehouse/Warehouse/SM_Rackshield_2984/SM_Rackshield_02\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Rank-0: policy_step=479736, average_reward_env_0=228.6384710350314\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Collision detected between /World/Worker_2 and /World/Warehouse/Warehouse/GroundPlane/CollisionPlane\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Rank-0: policy_step=479760, average_reward_env_0=118.69683723507067\n",
      "Arrive at the goal! - agent:  2  - stage:  6\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Rank-0: policy_step=479792, average_reward_env_0=114.59162549431701\n",
      "Rank-0: policy_step=479792, average_reward_env_1=219.66760681835913\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Rank-0: policy_step=479824, average_reward_env_0=132.13103036004534\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Rank-0: policy_step=479856, average_reward_env_0=178.15731557623513\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Rank-0: policy_step=479944, average_reward_env_0=112.99127850919612\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collision detected between /World/Worker_4 and /World/Worker_2\n",
      "Collision detected between /World/Worker_4 and /World/Worker_2\n",
      "Collision detected between /World/Worker_4 and /World/Worker_2\n",
      "Collision detected between /World/Worker_4 and /World/Worker_2\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Arrive at the goal! - agent:  3  - stage:  6\n",
      "Collision detected between /World/Worker_4 and /World/Worker_3\n",
      "Collision detected between /World/Worker_4 and /World/Worker_3\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Rank-0: policy_step=480336, average_reward_env_0=273.31678941412304\n",
      "Collision detected between /World/Worker_4 and /World/Warehouse/Warehouse/SM_SignCVer_148/SM_SignCVer_10\n",
      "Collision detected between /World/Worker_4 and /World/Warehouse/Warehouse/SM_RackFrame_2983/SM_RackFrame_03/SM_RackFrame_03/Section1\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Rank-0: policy_step=480400, average_reward_env_0=217.37313656160822\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Rank-0: policy_step=480440, average_reward_env_0=271.7588867973719\n",
      "Collision detected between /World/Worker_4 and /World/Warehouse/Warehouse/SM_SignCVer_1127/SM_SignCVer_10\n",
      "Arrive at the goal! - agent:  2  - stage:  6\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Rank-0: policy_step=480712, average_reward_env_0=249.8527429737104\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Rank-0: policy_step=480784, average_reward_env_0=115.42680644873136\n",
      "Arrive at the goal! - agent:  8  - stage:  6\n",
      "Collision detected between /World/Worker_3 and /World/Worker_1\n",
      "Collision detected between /World/Worker_3 and /World/Worker_1\n",
      "Collision detected between /World/Worker_3 and /World/Worker_1\n",
      "Collision detected between /World/Worker_3 and /World/Worker_1\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Rank-0: policy_step=481016, average_reward_env_0=205.5100068404177\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Rank-0: policy_step=481056, average_reward_env_0=219.69733043718392\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Rank-0: policy_step=481112, average_reward_env_0=136.18673492840986\n",
      "Arrive at the goal! - agent:  4  - stage:  6\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Rank-0: policy_step=481312, average_reward_env_0=250.99125390164562\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Rank-0: policy_step=481480, average_reward_env_0=214.45826921745112\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Rank-0: policy_step=481624, average_reward_env_0=223.79040438807934\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Rank-0: policy_step=481704, average_reward_env_0=220.4687559872377\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Rank-0: policy_step=481936, average_reward_env_0=113.06803251302104\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Rank-0: policy_step=482000, average_reward_env_0=138.33381886273722\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Arrive at the goal! - agent:  3  - stage:  6\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Rank-0: policy_step=482168, average_reward_env_0=204.15320135885906\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Rank-0: policy_step=482272, average_reward_env_0=210.21572183709387\n",
      "Arrive at the goal! - agent:  3  - stage:  6\n",
      "Arrive at the goal! - agent:  5  - stage:  6\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Rank-0: policy_step=482384, average_reward_env_1=189.78729343181539\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Rank-0: policy_step=482400, average_reward_env_0=197.5174603150252\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Collision detected between /World/Worker_3 and /World/Worker_1\n",
      "Collision detected between /World/Worker_3 and /World/Worker_1\n",
      "Collision detected between /World/Worker_3 and /World/Worker_1\n",
      "Collision detected between /World/Worker_3 and /World/Worker_1\n",
      "Collision detected between /World/Worker_3 and /World/Warehouse/Warehouse/SM_Rackshield_3841/SM_Rackshield_02\n",
      "Collision detected between /World/Worker_3 and /World/Warehouse/Warehouse/SM_Rackshield_3841/SM_Rackshield_02\n",
      "Arrive at the goal! - agent:  1  - stage:  6\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Rank-0: policy_step=482768, average_reward_env_0=183.74689472886254\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Rank-0: policy_step=482792, average_reward_env_0=115.97288040008627\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Rank-0: policy_step=482928, average_reward_env_0=176.89287555339325\n",
      "Arrive at the goal! - agent:  4  - stage:  6\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Rank-0: policy_step=482976, average_reward_env_0=201.05030715727975\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Rank-0: policy_step=483072, average_reward_env_0=145.42004673127036\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Collision detected between /World/Worker_3 and /World/Worker_1\n",
      "Collision detected between /World/Worker_3 and /World/Worker_1\n",
      "Arrive at the goal! - agent:  8  - stage:  6\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Rank-0: policy_step=483264, average_reward_env_0=107.01869967492563\n",
      "Arrive at the goal! - agent:  1  - stage:  6\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Rank-0: policy_step=483432, average_reward_env_0=106.21539129087265\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Arrive at the goal! - agent:  3  - stage:  6\n",
      "Collision detected between /World/Worker_4 and /World/Worker_1\n",
      "Collision detected between /World/Worker_4 and /World/Worker_1\n",
      "Collision detected between /World/Worker_4 and /World/Worker_1\n",
      "Collision detected between /World/Worker_4 and /World/Worker_1\n",
      "Arrive at the goal! - agent:  4  - stage:  6\n",
      "Arrive at the goal! - agent:  8  - stage:  6\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Rank-0: policy_step=483872, average_reward_env_0=222.39972921515826\n",
      "Arrive at the goal! - agent:  2  - stage:  6\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Rank-0: policy_step=483944, average_reward_env_0=204.8265860167143\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Arrive at the goal! - agent:  2  - stage:  6\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Rank-0: policy_step=484224, average_reward_env_0=261.2189185441635\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Rank-0: policy_step=484240, average_reward_env_0=201.24872727097554\n",
      "Arrive at the goal! - agent:  6  - stage:  6\n",
      "Arrive at the goal! - agent:  5  - stage:  6\n",
      "Arrive at the goal! - agent:  3  - stage:  6\n",
      "Arrive at the goal! - agent:  2  - stage:  6\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Rank-0: policy_step=484824, average_reward_env_0=197.83604554999835\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Arrive at the goal! - agent:  1  - stage:  6\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Rank-0: policy_step=484896, average_reward_env_0=174.46742764102558\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Rank-0: policy_step=485088, average_reward_env_0=227.31781072165575\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Rank-0: policy_step=485104, average_reward_env_0=132.4689025049882\n",
      "Rank-0: policy_step=485104, average_reward_env_1=191.36382462918507\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Arrive at the goal! - agent:  4  - stage:  6\n",
      "Rank-0: policy_step=485128, average_reward_env_0=223.58848638588967\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Rank-0: policy_step=485152, average_reward_env_0=104.35298322180353\n",
      "Arrive at the goal! - agent:  5  - stage:  6\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Arrive at the goal! - agent:  2  - stage:  6\n",
      "Arrive at the goal! - agent:  7  - stage:  6\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Rank-0: policy_step=485552, average_reward_env_0=252.67561292491808\n",
      "Arrive at the goal! - agent:  5  - stage:  6\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Rank-0: policy_step=485592, average_reward_env_0=187.5287676499227\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Arrive at the goal! - agent:  8  - stage:  6\n",
      "Arrive at the goal! - agent:  2  - stage:  6\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Arrive at the goal! - agent:  1  - stage:  6\n",
      "Arrive at the goal! - agent:  7  - stage:  6\n",
      "Max step number - agent:  2\n",
      "Rank-0: policy_step=486128, average_reward_env_0=209.42069793238062\n",
      "Arrive at the goal! - agent:  4  - stage:  6\n",
      "Collision detected between /World/Worker_1 and /World/Warehouse/Table_08\n",
      "Arrive at the goal! - agent:  2  - stage:  6\n",
      "Arrive at the goal! - agent:  3  - stage:  6\n",
      "Arrive at the goal! - agent:  1  - stage:  6\n",
      "Arrive at the goal! - agent:  4  - stage:  6\n",
      "Collide! - agent:  2  - stage:  6\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Rank-0: policy_step=486936, average_reward_env_0=214.38613831927523\n",
      "Rank-0: policy_step=486936, average_reward_env_1=258.6160915346385\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Arrive at the goal! - agent:  6  - stage:  6\n",
      "Arrive at the goal! - agent:  7  - stage:  6\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collision detected between /World/Worker_4 and /World/Worker_3\n",
      "Collision detected between /World/Worker_4 and /World/Worker_3\n",
      "Collision detected between /World/Worker_4 and /World/Worker_3\n",
      "Collision detected between /World/Worker_4 and /World/Worker_3\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Rank-0: policy_step=487280, average_reward_env_0=115.87570820511083\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Arrive at the goal! - agent:  3  - stage:  6\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Rank-0: policy_step=487592, average_reward_env_0=201.9579692506339\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Arrive at the goal! - agent:  8  - stage:  6\n",
      "Collision detected between /World/Worker_4 and /World/Worker_3\n",
      "Collision detected between /World/Worker_4 and /World/Worker_3\n",
      "Collision detected between /World/Worker_4 and /World/Worker_3\n",
      "Collision detected between /World/Worker_4 and /World/Worker_3\n",
      "Collision detected between /World/Worker_2 and /World/Warehouse/Rack_02\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Rank-0: policy_step=487952, average_reward_env_0=195.05607245910753\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Rank-0: policy_step=488064, average_reward_env_0=113.30676550604917\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Arrive at the goal! - agent:  7  - stage:  6\n",
      "Arrive at the goal! - agent:  2  - stage:  6\n",
      "Max step number - agent:  7\n",
      "Rank-0: policy_step=488104, average_reward_env_0=207.2734654355581\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Rank-0: policy_step=488176, average_reward_env_0=146.5946135951081\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Rank-0: policy_step=488304, average_reward_env_0=146.19714062197048\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Max step number - agent:  3\n",
      "Rank-0: policy_step=488328, average_reward_env_0=233.06809460047737\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Rank-0: policy_step=488360, average_reward_env_0=112.6634128424576\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Rank-0: policy_step=488808, average_reward_env_0=262.0573063186334\n",
      "Arrive at the goal! - agent:  6  - stage:  6\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Rank-0: policy_step=488864, average_reward_env_0=189.60588039507118\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Arrive at the goal! - agent:  7  - stage:  6\n",
      "Arrive at the goal! - agent:  2  - stage:  6\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Rank-0: policy_step=488984, average_reward_env_0=190.64375815721195\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Rank-0: policy_step=489048, average_reward_env_0=137.286282248118\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Rank-0: policy_step=489064, average_reward_env_0=190.36270920347374\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Arrive at the goal! - agent:  8  - stage:  6\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Rank-0: policy_step=489280, average_reward_env_0=132.6219533679008\n",
      "Arrive at the goal! - agent:  2  - stage:  6\n",
      "Arrive at the goal! - agent:  3  - stage:  6\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Rank-0: policy_step=489552, average_reward_env_0=190.80221423015718\n",
      "Collide! - agent:  3  - stage:  6\n",
      "Arrive at the goal! - agent:  7  - stage:  6\n",
      "Rank-0: policy_step=489560, average_reward_env_0=228.53916951187642\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Arrive at the goal! - agent:  8  - stage:  6\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Rank-0: policy_step=490024, average_reward_env_0=113.87926901297519\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Arrive at the goal! - agent:  3  - stage:  6\n",
      "Arrive at the goal! - agent:  4  - stage:  6\n",
      "Arrive at the goal! - agent:  5  - stage:  6\n",
      "Max step number - agent:  2\n",
      "Rank-0: policy_step=490136, average_reward_env_0=224.12938884502364\n",
      "Arrive at the goal! - agent:  4  - stage:  6\n",
      "Collision detected between /World/Worker_3 and /World/Worker_2\n",
      "Collision detected between /World/Worker_3 and /World/Worker_2\n",
      "Collision detected between /World/Worker_3 and /World/Warehouse/Table_02\n",
      "Collision detected between /World/Worker_3 and /World/Warehouse/Table_02\n",
      "Arrive at the goal! - agent:  1  - stage:  6\n",
      "Arrive at the goal! - agent:  2  - stage:  6\n",
      "Arrive at the goal! - agent:  3  - stage:  6\n",
      "Arrive at the goal! - agent:  2  - stage:  6\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Arrive at the goal! - agent:  7  - stage:  6\n",
      "Arrive at the goal! - agent:  2  - stage:  6\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Rank-0: policy_step=491096, average_reward_env_0=205.8713658464938\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Rank-0: policy_step=491152, average_reward_env_0=114.8497780138162\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Arrive at the goal! - agent:  8  - stage:  6\n",
      "Arrive at the goal! - agent:  8  - stage:  6\n",
      "Collision detected between /World/Worker_2 and /World/Warehouse/Warehouse/SM_RackFrame_3840/SM_RackFrame_03/SM_RackFrame_03/Section0\n",
      "Collision detected between /World/Worker_2 and /World/Warehouse/Warehouse/SM_Rackshield_3841/SM_Rackshield_02\n",
      "Collision detected between /World/Worker_3 and /World/Warehouse/Warehouse/SM_Rackshield_1075/SM_Rackshield_02\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Rank-0: policy_step=491408, average_reward_env_0=165.5929056529327\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Arrive at the goal! - agent:  7  - stage:  6\n",
      "Arrive at the goal! - agent:  3  - stage:  6\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Rank-0: policy_step=491632, average_reward_env_0=205.40398426406315\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Rank-0: policy_step=491720, average_reward_env_0=141.18364356366038\n",
      "Arrive at the goal! - agent:  3  - stage:  6\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Rank-0: policy_step=491984, average_reward_env_0=266.29937964556933\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Rank-0: policy_step=491992, average_reward_env_0=125.12486846498439\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Rank-0: policy_step=492040, average_reward_env_0=190.77856122760699\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Rank-0: policy_step=492056, average_reward_env_0=147.2171092256186\n",
      "Arrive at the goal! - agent:  6  - stage:  6\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Rank-0: policy_step=492344, average_reward_env_0=122.61814375381417\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Rank-0: policy_step=492360, average_reward_env_0=256.7819948812768\n",
      "Arrive at the goal! - agent:  2  - stage:  6\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Rank-0: policy_step=492488, average_reward_env_0=122.92114944237035\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Rank-0: policy_step=492552, average_reward_env_0=188.3463050071431\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Arrive at the goal! - agent:  3  - stage:  6\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Rank-0: policy_step=492704, average_reward_env_0=119.08693302126792\n",
      "Max step number - agent:  7\n",
      "Rank-0: policy_step=492752, average_reward_env_0=210.62075074540587\n",
      "Max step number - agent:  3\n",
      "Rank-0: policy_step=492760, average_reward_env_0=256.2103173918889\n",
      "Arrive at the goal! - agent:  2  - stage:  6\n",
      "Collide! - agent:  1  - stage:  6\n",
      "Rank-0: policy_step=492944, average_reward_env_0=93.16337775188414\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Rank-0: policy_step=493184, average_reward_env_0=143.35862459937553\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Arrive at the goal! - agent:  3  - stage:  6\n",
      "Max step number - agent:  2\n",
      "Rank-0: policy_step=493336, average_reward_env_0=232.14233300615612\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Rank-0: policy_step=493360, average_reward_env_0=240.54977111174924\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Arrive at the goal! - agent:  8  - stage:  6\n",
      "Collision detected between /World/Worker_1 and /World/Warehouse/Warehouse/GroundPlane/CollisionPlane\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Rank-0: policy_step=493872, average_reward_env_0=144.27771260499736\n",
      "Collide! - agent:  4  - stage:  6\n",
      "Collision detected between /World/Worker_2 and /World/Worker_1\n",
      "Collision detected between /World/Worker_2 and /World/Worker_1\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Rank-0: policy_step=493912, average_reward_env_0=118.67940018019004\n",
      "Collide! - agent:  6  - stage:  6\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Rank-0: policy_step=493920, average_reward_env_0=213.2178156046923\n",
      "Collide! - agent:  7  - stage:  6\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collision detected between /World/Worker_1 and /World/Warehouse/Warehouse/Box_3822/SM_CardBoxA_01/SM_CardBoxA_01\n",
      "Arrive at the goal! - agent:  2  - stage:  6\n",
      "Collision detected between /World/Worker_4 and /World/Worker_1\n",
      "Collision detected between /World/Worker_4 and /World/Worker_1\n",
      "Collision detected between /World/Worker_4 and /World/Worker_1\n",
      "Collision detected between /World/Worker_4 and /World/Worker_1\n",
      "Arrive at the goal! - agent:  2  - stage:  6\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Rank-0: policy_step=494472, average_reward_env_0=230.6406031067578\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Arrive at the goal! - agent:  8  - stage:  6\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Rank-0: policy_step=494520, average_reward_env_0=207.81301315007596\n",
      "Collide! - agent:  5  - stage:  6\n",
      "Rank-0: policy_step=494632, average_reward_env_0=176.3280665305881\n",
      "Arrive at the goal! - agent:  1  - stage:  6\n",
      "Arrive at the goal! - agent:  3  - stage:  6\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Rank-0: policy_step=494776, average_reward_env_0=201.61507575364647\n",
      "Arrive at the goal! - agent:  4  - stage:  6\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Rank-0: policy_step=494968, average_reward_env_0=186.17106178183755\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collide! - agent:  8  - stage:  6\n",
      "Judging Increasing\n",
      "Logged gif to TensorBoard at logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0\n",
      "Collision detected between /World/Worker_3 and /World/Warehouse/Warehouse/SM_FloorDecal_StripeFull_4m78/SM_FloorDecal_StripeFull_4m\n",
      "Collision detected between /World/Worker_3 and /World/Warehouse/Warehouse/SM_SignCVer_190/SM_SignCVer_10\n",
      "^C\n",
      "Fatal Python error: Segmentation fault\n",
      "\n",
      "Thread 0x00007f842901d700 (most recent call first):\n",
      "  <no Python frame>\n",
      "\n",
      "Thread 0x00007f84883ed700 (most recent call first):\n",
      "  File \"/home/jianheng/miniconda3/envs/sheeprl/lib/python3.10/threading.py\", line 324 in wait\n",
      "  File \"/home/jianheng/miniconda3/envs/sheeprl/lib/python3.10/queue.py\", line 180 in get\n",
      "  File \"/home/jianheng/miniconda3/envs/sheeprl/lib/python3.10/site-packages/tensorboard/summary/writer/event_file_writer.py\", line 227 in run\n",
      "  File \"/home/jianheng/miniconda3/envs/sheeprl/lib/python3.10/threading.py\", line 1016 in _bootstrap_inner\n",
      "  File \"/home/jianheng/miniconda3/envs/sheeprl/lib/python3.10/threading.py\", line 973 in _bootstrap\n",
      "\n",
      "Thread 0x00007f91357b7700 (most recent call first):\n",
      "  File \"/home/jianheng/miniconda3/envs/sheeprl/lib/python3.10/concurrent/futures/thread.py\", line 81 in _worker\n",
      "  File \"/home/jianheng/miniconda3/envs/sheeprl/lib/python3.10/threading.py\", line 953 in run\n",
      "  File \"/home/jianheng/miniconda3/envs/sheeprl/lib/python3.10/threading.py\", line 1016 in _bootstrap_inner\n",
      "  File \"/home/jianheng/miniconda3/envs/sheeprl/lib/python3.10/threading.py\", line 973 in _bootstrap\n",
      "\n",
      "Thread 0x00007f930e3b9700 (most recent call first):\n",
      "  File \"/home/jianheng/miniconda3/envs/sheeprl/lib/python3.10/concurrent/futures/thread.py\", line 81 in _worker\n",
      "  File \"/home/jianheng/miniconda3/envs/sheeprl/lib/python3.10/threading.py\", line 953 in run\n",
      "  File \"/home/jianheng/miniconda3/envs/sheeprl/lib/python3.10/threading.py\", line 1016 in _bootstrap_inner\n",
      "  File \"/home/jianheng/miniconda3/envs/sheeprl/lib/python3.10/threading.py\", line 973 in _bootstrap\n",
      "\n",
      "Thread 0x00007f92e8ff9700 (most recent call first):\n",
      "  File \"/home/jianheng/miniconda3/envs/sheeprl/lib/python3.10/concurrent/futures/thread.py\", line 81 in _worker\n",
      "  File \"/home/jianheng/miniconda3/envs/sheeprl/lib/python3.10/threading.py\", line 953 in run\n",
      "  File \"/home/jianheng/miniconda3/envs/sheeprl/lib/python3.10/threading.py\", line 1016 in _bootstrap_inner\n",
      "  File \"/home/jianheng/miniconda3/envs/sheeprl/lib/python3.10/threading.py\", line 973 in _bootstrap\n",
      "\n",
      "Thread 0x00007f934eac3700 (most recent call first):\n",
      "  File \"/home/jianheng/miniconda3/envs/sheeprl/lib/python3.10/concurrent/futures/thread.py\", line 81 in _worker\n",
      "  File \"/home/jianheng/miniconda3/envs/sheeprl/lib/python3.10/threading.py\", line 953 in run\n",
      "  File \"/home/jianheng/miniconda3/envs/sheeprl/lib/python3.10/threading.py\", line 1016 in _bootstrap_inner\n",
      "  File \"/home/jianheng/miniconda3/envs/sheeprl/lib/python3.10/threading.py\", line 973 in _bootstrap\n",
      "\n",
      "Thread 0x00007f9354a94700 (most recent call first):\n",
      "  File \"/home/jianheng/miniconda3/envs/sheeprl/lib/python3.10/concurrent/futures/thread.py\", line 81 in _worker\n",
      "  File \"/home/jianheng/miniconda3/envs/sheeprl/lib/python3.10/threading.py\", line 953 in run\n",
      "  File \"/home/jianheng/miniconda3/envs/sheeprl/lib/python3.10/threading.py\", line 1016 in _bootstrap_inner\n",
      "  File \"/home/jianheng/miniconda3/envs/sheeprl/lib/python3.10/threading.py\", line 973 in _bootstrap\n",
      "\n",
      "Current thread 0x00007f9661c06740 (most recent call first):\n",
      "  File \"/home/jianheng/.local/share/ov/pkg/isaac-sim-4.0.0/exts/omni.isaac.kit/omni/isaac/kit/simulation_app.py\", line 205 in signal_handler\n",
      "  File \"/home/jianheng/.local/share/ov/pkg/isaac-sim-4.0.0/exts/omni.isaac.ml_archive/pip_prebundle/torch/amp/autocast_mode.py\", line 187 in __init__\n",
      "  File \"/home/jianheng/miniconda3/envs/sheeprl/lib/python3.10/site-packages/lightning/fabric/plugins/precision/amp.py\", line 62 in forward_context\n",
      "  File \"/home/jianheng/miniconda3/envs/sheeprl/lib/python3.10/site-packages/lightning/fabric/wrappers.py\", line 140 in forward\n",
      "  File \"/home/jianheng/.local/share/ov/pkg/isaac-sim-4.0.0/exts/omni.isaac.ml_archive/pip_prebundle/torch/nn/modules/module.py\", line 1520 in _call_impl\n",
      "  File \"/home/jianheng/.local/share/ov/pkg/isaac-sim-4.0.0/exts/omni.isaac.ml_archive/pip_prebundle/torch/nn/modules/module.py\", line 1511 in _wrapped_call_impl\n",
      "  File \"/home/jianheng/sheeprl/sheeprl/algos/dreamer_v3/dreamer_v3.py\", line 686 in train\n",
      "  File \"/home/jianheng/sheeprl/sheeprl/algos/dreamer_v3/dreamer_v3.py\", line 1834 in main\n",
      "  File \"/home/jianheng/sheeprl/sheeprl/cli.py\", line 186 in wrapper\n",
      "  File \"/home/jianheng/miniconda3/envs/sheeprl/lib/python3.10/site-packages/lightning/fabric/fabric.py\", line 936 in _wrap_with_setup\n",
      "  File \"/home/jianheng/miniconda3/envs/sheeprl/lib/python3.10/site-packages/lightning/fabric/fabric.py\", line 931 in _wrap_and_launch\n",
      "  File \"/home/jianheng/miniconda3/envs/sheeprl/lib/python3.10/site-packages/lightning/fabric/fabric.py\", line 845 in launch\n",
      "  File \"/home/jianheng/sheeprl/sheeprl/cli.py\", line 190 in run_algorithm\n",
      "  File \"/home/jianheng/sheeprl/sheeprl/cli.py\", line 352 in run\n",
      "  File \"/home/jianheng/miniconda3/envs/sheeprl/lib/python3.10/site-packages/hydra/core/utils.py\", line 186 in run_job\n",
      "  File \"/home/jianheng/miniconda3/envs/sheeprl/lib/python3.10/site-packages/hydra/_internal/hydra.py\", line 119 in run\n",
      "  File \"/home/jianheng/miniconda3/envs/sheeprl/lib/python3.10/site-packages/hydra/_internal/utils.py\", line 458 in <lambda>\n",
      "  File \"/home/jianheng/miniconda3/envs/sheeprl/lib/python3.10/site-packages/hydra/_internal/utils.py\", line 219 in run_and_report\n",
      "  File \"/home/jianheng/miniconda3/envs/sheeprl/lib/python3.10/site-packages/hydra/_internal/utils.py\", line 457 in _run_app\n",
      "  File \"/home/jianheng/miniconda3/envs/sheeprl/lib/python3.10/site-packages/hydra/_internal/utils.py\", line 394 in _run_hydra\n",
      "  File \"/home/jianheng/miniconda3/envs/sheeprl/lib/python3.10/site-packages/hydra/main.py\", line 90 in decorated_main\n",
      "  File \"/home/jianheng/sheeprl/sheeprl.py\", line 6 in <module>\n",
      "\n",
      "Extension modules: psutil._psutil_linux, psutil._psutil_posix, omni.mdl.pymdlsdk._pymdlsdk, numpy.core._multiarray_umath, numpy.core._multiarray_tests, numpy.linalg._umath_linalg, numpy.fft._pocketfft_internal, numpy.random._common, numpy.random.bit_generator, numpy.random._bounded_integers, numpy.random._mt19937, numpy.random.mtrand, numpy.random._philox, numpy.random._pcg64, numpy.random._sfc64, numpy.random._generator, torch._C, torch._C._fft, torch._C._linalg, torch._C._nested, torch._C._nn, torch._C._sparse, torch._C._special, scipy._lib._ccallback_c, scipy.sparse._sparsetools, _csparsetools, scipy.sparse._csparsetools, scipy.sparse.linalg._isolve._iterative, scipy.linalg._fblas, scipy.linalg._flapack, scipy.linalg._cythonized_array_utils, scipy.linalg._flinalg, scipy.linalg._solve_toeplitz, scipy.linalg._matfuncs_sqrtm_triu, scipy.linalg.cython_lapack, scipy.linalg.cython_blas, scipy.linalg._matfuncs_expm, scipy.linalg._decomp_update, scipy.sparse.linalg._dsolve._superlu, scipy.sparse.linalg._eigen.arpack._arpack, scipy.sparse.csgraph._tools, scipy.sparse.csgraph._shortest_path, scipy.sparse.csgraph._traversal, scipy.sparse.csgraph._min_spanning_tree, scipy.sparse.csgraph._flow, scipy.sparse.csgraph._matching, scipy.sparse.csgraph._reordering, scipy.spatial._ckdtree, scipy._lib.messagestream, scipy.spatial._qhull, scipy.spatial._voronoi, scipy.spatial._distance_wrap, scipy.spatial._hausdorff, scipy.special._ufuncs_cxx, scipy.special._ufuncs, scipy.special._specfun, scipy.special._comb, scipy.special._ellip_harm_2, scipy.spatial.transform._rotation, PIL._imaging, PIL._imagingft, scipy.ndimage._nd_image, _ni_label, scipy.ndimage._ni_label, _brotli, osqp._osqp, multidict._multidict, yarl._quoting_c, aiohttp._helpers, aiohttp._http_writer, aiohttp._http_parser, aiohttp._websocket, _cffi_backend, frozenlist._frozenlist, scipy.io.matlab._mio_utils, scipy.io.matlab._streams, scipy.io.matlab._mio5_utils, yaml._yaml, cv2, gmpy2.gmpy2, matplotlib._c_internal_utils, matplotlib._path, kiwisolver._cext, matplotlib._image, google.protobuf.pyext._message, PIL._imagingmath, pygame.base, pygame.constants, pygame.rect, pygame.rwobject, pygame.surflock, pygame.bufferproxy, pygame.math, pygame.surface, pygame.display, pygame.draw, pygame.event, pygame.imageext, pygame.image, pygame.joystick, pygame.key, pygame.mouse, pygame.time, pygame.mask, pygame.pixelcopy, pygame.transform, pygame.font, pygame.mixer_music, pygame.mixer, pygame.scrap, pygame._freetype (total: 111)\n"
     ]
    }
   ],
   "source": [
    "!HYDRA_FULL_ERROR=1 python sheeprl.py exp=dreamer_v3_isaacsim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!HYDRA_FULL_ERROR=1 python sheeprl_eval.py checkpoint_path=./logs/runs/dreamer_v3/isaac_sim_environment/2024-08-23_09-18-05_dreamer_v3_isaac_sim_environment_5/version_0/checkpoint/ckpt_490000_0.ckpt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sheeprl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
